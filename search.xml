<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[[Gin教程-5]中间件与API开发-2]]></title>
    <url>%2F2019%2F03%2F07%2Fgin-tutorial-5%2F</url>
    <content type="text"><![CDATA[本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 本文写于2019/03/07，基于Go 1.11。至于其他版本的Go SDK，如有出入请自行查阅其他资料。 本文为[Gin-教程]连载系列的第五篇，更多后续请继续关注。 Overview在上一篇教程《[Gin教程-4]中间件与API开发-1》里我们实现了Auth API，可是还没有说怎么可以让它“运行起来”，还不知道怎么可以让用户调用。在本篇教程里，我们要定义后端的Router和Server，还有实现Bucket和Photo的部分API，让我们的服务真正地跑起来，可供外部调用。 如图所示，所谓的Router，就是路由器，负责把外部请求转发给对应的执行方法。例如用户调用的是www.xxx.com/add_auth，服务器接到请求后，Router就把该请求转发给我们上一篇实现的AddAuth()方法，执行里面的添加新用户的逻辑。 而Server的实现，通俗来说其实就是要写一些代码，让你的进程一直运行，正常情况下不要自行退出，并且能够监听一个端口，从该端口处能接收用户请求和用户数据。 Router &amp; Main因为上一篇教程里我们已经实现了Auth API，所以可以先简单地实现Router和Server，试试我们的API能不能正常工作。 在项目的根目录下新建routers文件夹，然后在文件夹内新建router.go代码文件： gin-photo-storage|—— conf |—— ......|—— constant |—— ......|—— models |—— ......|—— utils |—— ......|—— middleware |—— ......|—— apis |—— ......|—— routers |—— router.go Router的逻辑无非就是新建一个Router对象，然后往该Router对象里添加各种路径 -&gt; 方法的映射关系。 package routersimport ( "gin-photo-storage/apis/v1" "gin-photo-storage/middleware" "github.com/gin-gonic/gin")// a global routervar Router *gin.Engine// Init router, adding paths to it.func init() &#123; Router = gin.Default() // api group for v1 v1Group := Router.Group("/api/v1") &#123; // api group for authentication authGroup := v1Group.Group("/auth") &#123; authGroup.POST("/add", v1.AddAuth) authGroup.POST("/check", v1.CheckAuth) &#125; &#125;&#125; Note： 用Group来划分API，可以让组织结构更加清晰。例如第一版的API，我们全部把它们归到/api/v1这个group下面，当项目迭代要开发第二版时，可以另外开一个group叫/api/v2，避免冲突。当调用该group下的API时，路径都要带上该group的前缀，如www.xxx.com/api/v1/......。 Group是可以嵌套的，在v1的group内部，基于v1还可以按照业务或者Model来划分不同的group，可以有authGroup、bucketGroup、photoGroup……同理，调用的时候也要加上该group的路径前缀。 接下来在项目的根目录下直接新建main.go代码文件： gin-photo-storage|—— conf |—— ......|—— constant |—— ......|—— models |—— ......|—— utils |—— ......|—— middleware |—— ......|—— apis |—— ......|—— routers |—— ......|—— main.go 在Main里面，主要的逻辑就是起一个http server，然后配置一下该http server的handler，让它把请求转交给我们的Router去处理。 package mainimport ( "fmt" "gin-photo-storage/conf" "gin-photo-storage/routers" "gin-photo-storage/constant" "net/http")func main() &#123; // get the global router from router.go router := routers.Router // set up a http server server := http.Server&#123; Addr: fmt.Sprintf(":%s", conf.ServerCfg.Get(constant.SERVER_PORT)), Handler: router, MaxHeaderBytes: 1 &lt;&lt; 20, &#125; // run the server server.ListenAndServe()&#125; 最后，保证你的MySQL和Redis在正常运行，然后在项目根目录下执行go run main.go，http server就能正常跑起来，保持监听我们在server.conf里设定的那个端口。 既然已经跑起来了，我们可以试试调用AddAuth()，看看是不是能够成功写入数据库。此处强烈建议使用Postman工具来测试API调用。（可以在电脑上装Postman软件，也可以用它的chrome插件） 用Postman调用add_authAPI。（我设的监听端口是9088，测试时请更换为你们自己的端口） 可以看到在数据库的auth表里出现了一条新增的用户记录。 Bucket API在gin-photo-storage/apis/v1/下新建bucket.go代码文件： gin-photo-storage|—— conf |—— ......|—— constant |—— ......|—— models |—— ......|—— utils |—— ......|—— middleware |—— ......|—— apis |—— v1 |—— auth.go |—— bucket.go|—— routers |—— ......|—— main.go 然后为Bucket实现增删改查的API，因为篇幅和排版的原因，教程里只给出AddBucket()的源码。其他API的代码逻辑基本类似，没有太大区别，全部Bucket API的源码可参考github repo里的bucket.go package v1import ( "gin-photo-storage/models" "gin-photo-storage/constant" "github.com/astaxie/beego/validation" "github.com/gin-gonic/gin" "github.com/gin-gonic/gin/binding" "log" "net/http" "strconv")// Add a new bucket.func AddBucket(context *gin.Context) &#123; responseCode := constant.INVALID_PARAMS bucketToAdd := models.Bucket&#123;&#125; if err := context.ShouldBindWith(&amp;bucketToAdd, binding.Form); err != nil &#123; log.Println(err) context.AbortWithStatusJSON(http.StatusBadRequest, gin.H&#123; "code": responseCode, "data": make(map[string]string), "msg": constant.GetMessage(responseCode), &#125;) return &#125; validCheck := validation.Validation&#123;&#125; validCheck.Required(bucketToAdd.AuthID, "auth_id").Message("Must have auth id") validCheck.Required(bucketToAdd.Name, "bucket_name").Message("Must have bucket name") validCheck.MaxSize(bucketToAdd.Name, 64, "bucket_name").Message("Bucket name length can not exceed 64") if !validCheck.HasErrors() &#123; if err := models.AddBucket(&amp;bucketToAdd); err != nil &#123; if err == models.BucketExistsError &#123; responseCode = constant.BUCKET_ALREADY_EXIST &#125; else &#123; responseCode = constant.INTERNAL_SERVER_ERROR &#125; &#125; else &#123; responseCode = constant.BUCKET_ADD_SUCCESS &#125; &#125; else &#123; for _, err := range validCheck.Errors &#123; log.Println(err.Message) &#125; &#125; data := make(map[string]string) data["bucket_name"] = bucketToAdd.Name context.JSON(http.StatusOK, gin.H&#123; "code": responseCode, "data": data, "msg": constant.GetMessage(responseCode), &#125;)&#125;// Delete an existed bucket.func DeleteBucket(context *gin.Context) &#123; // ......&#125;// Update an existed bucket.func UpdateBucket(context *gin.Context) &#123; // ......&#125;// Get a bucket by bucket id.func GetBucketByID(context *gin.Context) &#123; // ......&#125;// Get buckets by auth id.func GetBucketByAuthID(context *gin.Context) &#123; // ......&#125; Note： 这里没有用context.Form()或者context.Query()来获取请求里的参数，而是直接把请求参数bind到一个Bucket对象上（context.ShouldBindWith()）。为什么可以这么做呢？原因就是我们在Bucket Model里为每个字段设定了form标签，所以只要请求参数里的参数名能严格对应字段的form标签，就能成功把参数绑定到Bucket Model对象上。 Photo API在gin-photo-storage/apis/v1/下新建photo.go代码文件： gin-photo-storage|—— conf |—— ......|—— constant |—— ......|—— models |—— ......|—— utils |—— ......|—— middleware |—— ......|—— apis |—— v1 |—— auth.go |—— bucket.go |—— photo.go|—— routers |—— ......|—— main.go 然后为Photo实现增删改查的API，因为篇幅和排版的原因，教程里只给出AddPhoto()的源码。其他API的代码逻辑基本类似，没有太大区别，全部Photo API的源码可参考github repo里的photo.go package v1import ( "gin-photo-storage/models" "gin-photo-storage/constant" "github.com/astaxie/beego/validation" "github.com/gin-gonic/gin" "github.com/gin-gonic/gin/binding" "log" "net/http" "strconv" "strings")// Add a new photofunc AddPhoto(context *gin.Context) &#123; responseCode := constant.INVALID_PARAMS photoFile, fileErr := context.FormFile("photo") if fileErr != nil &#123; log.Println(fileErr) &#125; photo := models.Photo&#123;&#125; paramErr := context.ShouldBindWith(&amp;photo, binding.Form) if fileErr != nil || paramErr != nil &#123; context.AbortWithStatusJSON(http.StatusBadRequest, gin.H&#123; "code": responseCode, "data": make(map[string]string), "msg": constant.GetMessage(responseCode), &#125;) return &#125; validCheck := validation.Validation&#123;&#125; validCheck.Required(photo.AuthID, "auth_id").Message("Must have auth id") validCheck.Required(photo.BucketID, "bucket_id").Message("Must have bucket id") validCheck.Required(photo.Name, "photo_name").Message("Must have photo name") validCheck.MaxSize(photo.Name, 255, "photo_name").Message("Photo name len must not exceed 255") data := make(map[string]interface&#123;&#125;) photoToAdd := &amp;models.Photo&#123;BucketID: photo.BucketID, AuthID: photo.AuthID, Name: photo.Name, Description: photo.Description, Tag:strings.Join(photo.Tags, ";")&#125; if !validCheck.HasErrors() &#123; if photoToAdd, uploadID, err := models.AddPhoto(photoToAdd, photoFile); err != nil &#123; if err == models.PhotoExistsError &#123; responseCode = constant.PHOTO_ALREADY_EXIST &#125; else &#123; responseCode = constant.INTERNAL_SERVER_ERROR &#125; &#125; else &#123; responseCode = constant.PHOTO_ADD_IN_PROCESS data["photo"] = *photoToAdd data["photo_upload_id"] = uploadID &#125; &#125; else &#123; for _, err := range validCheck.Errors &#123; log.Println(err.Message) &#125; &#125; context.JSON(http.StatusOK, gin.H&#123; "code": responseCode, "data": data, "msg": constant.GetMessage(responseCode), &#125;)&#125;// Delete an existed photo.func DeletePhoto(context *gin.Context) &#123; // ......&#125;// Update an existed photo.func UpdatePhoto(context *gin.Context) &#123; // ......&#125;// Get a photo by photo id.func GetPhotoByID(context *gin.Context) &#123; // ......&#125;// Get photos by bucket id.func GetPhotoByBucketID(context *gin.Context) &#123; // ......&#125;// Get the upload status of a photo by upload id.func GetPhotoUploadStatus(context *gin.Context) &#123; // ......&#125; 同理，除了用context.ShouldBindWith()来绑定参数之外，我们还用context.FormFile()来获取用户通过API上传的图片文件。 设置Router转发规则到这里，我们已经实现了： 中间件 pagination中间件 auth中间件 refresh中间件 API Auth API Bucket API Photo API 此时应该把所有API的routing规则都设置到Router里，让请求真正都能被转发到对应的执行方法处。以下为最终的router.go： package routersimport ( "gin-photo-storage/apis/v1" "gin-photo-storage/middleware" "github.com/gin-gonic/gin")// a global routervar Router *gin.Engine// Init router, adding paths to it.func init() &#123; Router = gin.Default() checkAuthMdw := middleware.GetAuthMiddleware() // middleware for authentication refreshMdw := middleware.GetRefreshMiddleware() // middleware for refresh auth token paginationMdw := middleware.GetPaginationMiddleware() // middleware for pagination // api group for v1 v1Group := Router.Group("/api/v1") &#123; // api group for authentication authGroup := v1Group.Group("/auth") &#123; authGroup.POST("/add", v1.AddAuth) authGroup.POST("/check", v1.CheckAuth) &#125; // api group for bucket bucketGroup := v1Group.Group("/bucket") &#123; // must check auth &amp; refresh auth token before any operation bucketGroup.POST("/add", checkAuthMdw, refreshMdw, v1.AddBucket) bucketGroup.DELETE("/delete", checkAuthMdw, refreshMdw, v1.DeleteBucket) bucketGroup.PUT("/update", checkAuthMdw, refreshMdw, v1.UpdateBucket) bucketGroup.GET("/get_by_id", checkAuthMdw, refreshMdw, v1.GetBucketByID) bucketGroup.GET("/get_by_auth_id", checkAuthMdw, refreshMdw, paginationMdw, v1.GetBucketByAuthID) &#125; // api group for photo photoGroup := v1Group.Group("/photo") &#123; // must check auth &amp; refresh auth token before any operation photoGroup.POST("/add", checkAuthMdw, refreshMdw, v1.AddPhoto) photoGroup.GET("/upload_status", checkAuthMdw, refreshMdw, v1.GetPhotoUploadStatus) photoGroup.DELETE("/delete", checkAuthMdw, refreshMdw, v1.DeletePhoto) photoGroup.PUT("/update", checkAuthMdw, refreshMdw, v1.UpdatePhoto) photoGroup.GET("/get_by_id", checkAuthMdw, refreshMdw, v1.GetPhotoByID) photoGroup.GET("/get_by_bucket_id", checkAuthMdw, refreshMdw, paginationMdw, v1.GetPhotoByBucketID) &#125; &#125;&#125; 可以看到，在bucket group和photo group里我们为每个API都设定了调用路径，并且为其配置了auth中间件、pagination中间件和refresh中间件。 总结 Router定义里，不同的方法会对应GET、POST、PUT、DELETE多个不同的动词，它们有什么区别？除了这4个还有什么动词可以直接拿来用？ 关键词：http verb 在教程里，Server、Router、APIs、Models全都是一个进程里的，在单台服务器上把程序run起来就可以工作了。但如果程序里某一个地方出bug导致崩溃了，意味着从Server到Models都不能工作了，有更好的架构解决类似的问题吗？ 关键词：微服务、容灾 我们在main里面起了一个http server，什么是http？可以用http server来做即时聊天工具的服务器吗？ 关键词：http协议、tcp协议、网络协议栈 在AddAuth()的逻辑里，我们会先把用户密码用md5加一层壳再写MySQL，但是在调API的时候是直接明文传输的，这样是不是很不安全？有什么解决手段吗？ 关键词：SSL/TLS、https]]></content>
      <categories>
        <category>gin</category>
      </categories>
      <tags>
        <tag>gin</tag>
        <tag>教程</tag>
        <tag>连载</tag>
        <tag>后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Gin教程-4]中间件与API开发-1]]></title>
    <url>%2F2019%2F03%2F04%2Fgin-tutorial-4%2F</url>
    <content type="text"><![CDATA[本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 本文写于2019/03/04，基于Go 1.11。至于其他版本的Go SDK，如有出入请自行查阅其他资料。 本文为[Gin-教程]连载系列的第四篇，更多后续请继续关注。 Overview在上一篇教程《[Gin教程-3]引入Redis》里，我们引入了Redis，实现了Redis utils且结合腾讯云COS开发了图片上传的功能。 在本篇教程中，我们要开始开发API层，接收客户端的请求，解析请求里的参数，然后对各个Model执行增/删/改/查操作。 层次结构API层包含了很多API，每个API的职责就是： 解析用户请求的参数（表单、url参数……） 验证参数有效性 调用对应Model的增/删/改/查方法，得到执行结果 对执行结果进行封装，返回给用户 但是在真正调用API前，很多时候我们需要做一些额外的工作。拿新增图片这个操作为例，我们不能允许用户在未登录状态下新增图片，所以调用AddPhoto()前我们还得额外地“鉴权”，去查查该用户是不是已经登录了，所以我们需要写一个“鉴权的方法”；再例如查询一个bucket下的所有图片时，我们需要对数据分页，总不能一次请求返回几万张图片数据吧？所以我们需要写一个“分页的方法”。 所以，实际上API层是由中间件 + API组成的，每一个中间件其实可以看成一个特殊的API，它也要解析参数，也要执行相应的逻辑，也要返回结果给下一级，称其为中间件是因为它不是执行链的最后一个，而是在执行链的中间位置。 如上图所示，在请求到达最终执行业务逻辑的API前，很可能要先经过一系列的中间件，就像是一条执行链。 分页中间件首先来看看分页中间件，凡是返回多项数据的API，都必须先经过分页。例如GetPhotoByBucketID()、GetBucketByAuthID()，我们要把请求中的page参数先转化为offset = page * PAGE_SIZE，然后才能从数据库中直接利用offset得到我们想要的数据。 在项目根目录下新建middleware文件夹，然后新建pagination.go代码文件。 gin-photo-storage|—— conf |—— ......|—— constant |—— ......|—— models |—— ......|—— utils |—— ......|—— middleware |—— pagination.go 接着可以实现分页中间件： package middlewareimport ( "gin-photo-storage/constant" "github.com/gin-gonic/gin" "github.com/pkg/errors" "net/http" "strconv")var InvalidPageNoError = errors.New("page no can not be negative")// A wrapper function which returns the pagination middleware.func GetPaginationMiddleware() func(*gin.Context) &#123; return func(context *gin.Context) &#123; responseCode := constant.PAGINATION_SUCCESS pageNo := context.Query("page") if pageNo == "" &#123; responseCode = constant.INVALID_PARAMS &#125; else &#123; pageOffset, err := GetPagination(pageNo) if err != nil &#123; responseCode = constant.INVALID_PARAMS &#125; else &#123; context.Set("offset", pageOffset) &#125; &#125; if responseCode == constant.INVALID_PARAMS &#123; data := make(map[string]string) data["page"] = pageNo context.JSON(http.StatusBadRequest, gin.H&#123; "code": constant.INVALID_PARAMS, "data": data, "msg": constant.GetMessage(constant.INVALID_PARAMS), &#125;) context.Abort() &#125; context.Next() &#125;&#125;// Pagination function which calculates the offset given the page number.func GetPagination(pageNo string) (int, error) &#123; pageNoInt, err := strconv.Atoi(pageNo) if err != nil &#123; return 0, err &#125; if pageNoInt &lt; 0 &#123; return 0, InvalidPageNoError &#125; return pageNoInt * constant.PAGE_SIZE, nil&#125; Note：为什么我们要绕一圈，定义一个wrapper方法？用wrapper方法返回一个func(context *gin.Context)，而不是直接把分页中间件定义成一个func(context *gin.Context)？因为用wrapper方法的话，我们可以在return真正的中间件方法前做一些其他操作，例如： func GetXXXMiddleware() func(*gin.Context) &#123; // ...... // 可以在这连接数据库 // 可以在这读某个磁盘文件 // ...... return func(context *gin.Context) &#123; // ...... // 真正的中间件逻辑 // ...... &#125;&#125; 把一些耗时的操作，或者那些只需要在最开始做一次的操作放在return中间件的前面，就不需要把它们写在中间件的内部，“污染”中间件真正的逻辑。 Auth中间件至于Auth中间件，我们要实现的逻辑是： 尝试从请求的cookie里获取JWT字符串，如果不存在JWT串说明该用户从未登录，或者N久前登录导致cookie都消失了。 调用utils里的解析方法，解析该JWT串，得到UserClaim。 从UserClaim里获取用户名，检查Redis里存不存在该用户名，如果不存在说明该用户登录状态已过期。 若前几步都验证成功，可以把用户名加到本次请求的context里，完成本中间件的所有任务，去做下一步。 我们在gin-photo-storage/middleware/下新建auth.go。 gin-photo-storage|—— conf |—— ......|—— constant |—— ......|—— models |—— ......|—— utils |—— ......|—— middleware |—— pagination.go |—— auth.go 然后可以实现auth.go： package middlewareimport ( "gin-photo-storage/constant" "gin-photo-storage/utils" "github.com/gin-gonic/gin" "log" "net/http")// A wrapper function which returns the auth middleware.func GetAuthMiddleware() gin.HandlerFunc &#123; return func(context *gin.Context) &#123; jwtString, err := context.Cookie(constant.JWT) if err != nil &#123; log.Println(err) context.JSON(http.StatusBadRequest, gin.H&#123; "code": constant.JWT_MISSING_ERROR, "data": make(map[string]string), "msg": constant.GetMessage(constant.JWT_MISSING_ERROR), &#125;) context.Abort() return &#125; claim, err := utils.ParseJWT(jwtString) if err != nil &#123; log.Println(err) context.JSON(http.StatusBadRequest, gin.H&#123; "code": constant.JWT_PARSE_ERROR, "data": make(map[string]string), "msg": constant.GetMessage(constant.JWT_PARSE_ERROR), &#125;) context.Abort() return &#125; if utils.IsAuthInRedis(claim.UserName) &#123; context.Set("user_name", claim.UserName) context.Next() &#125; else &#123; context.JSON(http.StatusBadRequest, gin.H&#123; "code": constant.USER_AUTH_TIMEOUT, "data": make(map[string]string), "msg": constant.GetMessage(constant.USER_AUTH_TIMEOUT), &#125;) context.Abort() &#125; &#125;&#125; Note： context就是本次请求的上下文数据，它的生命周期就是整个请求的生命周期。只要请求没完成/没结束，context都会存在。所以我们才能把用户名set到context里，传给下一步。 注意在验证异常时，除了调用context.Abort()来终止执行链之外，还要及时return，避免执行该中间件下面的逻辑。 Refresh中间件所谓的Refresh中间件，所要refresh的就是用户的登录token。试想一下这样的场景：用户在12:00登录了系统，根据我们设定的JWT_EXP_MINUTE = 30，说明他的登录状态在12:31就在Redis里过期了，必须得重新登录。虽然过期时间设短一点相对会安全些，但是这样是非常影响用户体验的。 所以，我们就要设计refresh机制。如果用户在登录之后有各种频繁的操作，那我们就不断地给他刷新他的token有效期。例如用户在12:00登录了，token有效期到12:30，但是他在12:18新建了bucket，那我们就把他的token有效期刷新，刷成12:48，之后他又在12:35添加了20张图片，那就再把他的token有效期刷到13:05，依此类推。 先在gin-photo-gallery/middleware/下新建refresh.go代码文件。 gin-photo-storage|—— conf |—— ......|—— constant |—— ......|—— models |—— ......|—— utils |—— ......|—— middleware |—— pagination.go |—— auth.go |—— refresh.go 然后可以实现refresh.go： package middlewareimport ( "gin-photo-storage/conf" "gin-photo-storage/constant" "gin-photo-storage/utils" "github.com/gin-gonic/gin" "log" "net/http")// A wrapper function which returns the refresh middleware.func GetRefreshMiddleware() gin.HandlerFunc &#123; return func(context *gin.Context) &#123; if userName, ok := context.Get("user_name"); ok &#123; // generate a new valid JWT for the user jwtString, err := utils.GenerateJWT(userName.(string)) if err != nil &#123; log.Fatalln(err) data := make(map[string]string) data["user_name"] = userName.(string) context.JSON(http.StatusBadRequest, gin.H&#123; "code": constant.JWT_GENERATION_ERROR, "data": data, "msg": constant.GetMessage(constant.JWT_GENERATION_ERROR), &#125;) context.Abort() return &#125; // save the new JWT in user's cookie context.SetCookie(constant.JWT, jwtString, constant.COOKIE_MAX_AGE, "/", conf.ServerCfg.Get(constant.SERVER_DOMAIN), true, true) // refresh user in the redis err = utils.AddAuthToRedis(userName.(string)) if err != nil &#123; log.Fatalln(err) context.JSON(http.StatusBadRequest, gin.H&#123; "code": constant.INTERNAL_SERVER_ERROR, "data": make(map[string]string), "msg": constant.GetMessage(constant.INTERNAL_SERVER_ERROR), &#125;) context.Abort() return &#125; context.Next() &#125; context.Abort() &#125;&#125; Note：我们不仅要为用户颁发一个新的JWT，还要在Redis里给用户刷新一下，避免过期。 Auth API把几个必要的中间件实现完，可以开始实现我们的业务逻辑了，首先来看看Auth API。 先在项目根目录下新建apis文件夹，再在apis下新建v1文件夹，代表现在开发的API版本是v1。最后在apis/v1/下新建auth.go代码文件。 gin-photo-storage|—— conf |—— ......|—— constant |—— ......|—— models |—— ......|—— utils |—— ......|—— middleware |—— ......|—— apis |—— v1 |—— auth.go 因为在API的逻辑里要对请求里的参数作合法性验证，所以我们就采用了第三方的beego validation来实现参数验证。（不使用第三方库，纯手写验证逻辑也不是不可以，只不过用库会方便优雅一点） Golang &lt;= 1.10且项目位于$GOPATH/src/下的，可直接go get -u github.com/astaxie/beego/validation 否则可用go mod来安装 在auth.go里，我们要实现2个API：用户注册（AddAuth）和用户登录验证（CheckAuth）： package v1import ( "gin-photo-storage/conf" "gin-photo-storage/models" "gin-photo-storage/constant" "gin-photo-storage/utils" "github.com/astaxie/beego/validation" "github.com/gin-gonic/gin" "log" "net/http")// Add a new auth.func AddAuth(context *gin.Context) &#123; userName := context.PostForm("user_name") password := context.PostForm("password") email := context.PostForm("email") // set up param validation validCheck := validation.Validation&#123;&#125; validCheck.Required(userName, "user_name").Message("Must have user name") validCheck.MaxSize(userName, 16, "user_name").Message("User name length can not exceed 16") validCheck.MinSize(userName, 6, "user_name").Message("User name length is at least 6") validCheck.Required(password, "password").Message("Must have password") validCheck.MaxSize(password, 16, "password").Message("Password length can not exceed 16") validCheck.MinSize(password, 6, "password").Message("Password length is at least 6") validCheck.Required(email, "email").Message("Must have email") validCheck.MaxSize(email, 128, "email").Message("Email can not exceed 128 chars") responseCode := constant.INVALID_PARAMS if !validCheck.HasErrors() &#123; if err := models.AddAuth(userName, password, email); err == nil &#123; responseCode = constant.USER_ADD_SUCCESS &#125; else &#123; responseCode = constant.USER_ALREADY_EXIST &#125; &#125; else &#123; for _, err := range validCheck.Errors &#123; log.Println(err) &#125; &#125; context.JSON(http.StatusOK, gin.H&#123; "code": responseCode, "data": userName, "msg": constant.GetMessage(responseCode), &#125;)&#125;// Check if an auth is valid.func CheckAuth(context *gin.Context) &#123; userName := context.PostForm("user_name") password := context.PostForm("password") // set up param validation validCheck := validation.Validation&#123;&#125; validCheck.Required(userName, "user_name").Message("Must have user name") validCheck.MaxSize(userName, 16, "user_name").Message("User name length can not exceed 16") validCheck.MinSize(userName, 6, "user_name").Message("User name length is at least 6") validCheck.Required(password, "password").Message("Must have password") validCheck.MaxSize(password, 16, "password").Message("Password length can not exceed 16") validCheck.MinSize(password, 6, "password").Message("Password length is at least 6") responseCode := constant.INVALID_PARAMS if !validCheck.HasErrors() &#123; if models.CheckAuth(userName, password) &#123; if jwtString, err := utils.GenerateJWT(userName); err != nil &#123; responseCode = constant.JWT_GENERATION_ERROR &#125; else &#123; // pass auth validation // 1. set JWT to user's cookie // 2. add user to the Redis context.SetCookie(constant.JWT, jwtString, constant.COOKIE_MAX_AGE, conf.ServerCfg.Get(constant.SERVER_PATH), conf.ServerCfg.Get(constant.SERVER_DOMAIN), true, true) if err = utils.AddAuthToRedis(userName); err != nil &#123; responseCode = constant.INTERNAL_SERVER_ERROR &#125; else &#123; responseCode = constant.USER_AUTH_SUCCESS &#125; &#125; &#125; else &#123; responseCode = constant.USER_AUTH_ERROR &#125; &#125; else &#123; for _, err := range validCheck.Errors &#123; log.Println(err) &#125; &#125; context.JSON(http.StatusOK, gin.H&#123; "code": responseCode, "data": userName, "msg": constant.GetMessage(responseCode), &#125;)&#125; Auth API里的核心点在于业务逻辑，在CheckAuth()里如果一个用户验证通过，不仅要为他“颁发”一个JWT串，还要把该用户写入到Redis里，用Redis来记录这个用户是否在有效登录期内。如果一个用户请求的cookie里带有JWT，但是Redis里没有该用户的数据，要么是他伪造了JWT，要么是JWT和Redis里的数据过期时间不同步。 Note： 代码里用的是context.PostForm(xxx)来获取请求参数，这个前提是用户会用POST表单来传参，如果用户用url参数来传参，我们应该用context.Query(xxx)来获取参数。 API调用完成后我们返回的是JSON数据，根据不同的需求，我们也可以返回一个渲染好的html页面，或者就返回裸字符串。 通过context.JSON()返回给调用者的是JSON格式的数据，字段就是gin.H{}里的那些字段，gin.H其实只是一个简写，本质上gin.H就是一个`map[string]interface{}。 总结 在gin框架里，中间件指的是在API调用的执行链处于中间位置的组件。那撇开gin框架，从宏观的角度来看，中间件又指什么呢？有哪些著名的中间件？ 关键词：中间件、业务逻辑、插件 用户成功登录后，我们把JWT串写到了他的cookie里，对于浏览器应用来说写cookie是不错的做法（虽然可能有安全问题）。那如果调用我们API的不是浏览器应用，而是ios/android app，我们应该把JWT串写到哪里呢，应该怎么返回呢？ 在返回的JSON数据里，我们有自定义的code字段，可是本身http协议就有代表不同状态的状态码，为什么我们还要加上自定义的code？ 在实现中间件时，GetXXXMiddleware()返回的是一个func(*gin.Context)，为什么一个函数的返回值可以是另一个函数？在Golang里函数也是一种类型吗？在Java里可不可以实现类似的功能？ 关键词：函数指针、函数作为类型]]></content>
      <categories>
        <category>gin</category>
      </categories>
      <tags>
        <tag>gin</tag>
        <tag>教程</tag>
        <tag>连载</tag>
        <tag>后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Gin教程-3]引入Redis]]></title>
    <url>%2F2019%2F02%2F28%2Fgin-tutorial-3%2F</url>
    <content type="text"><![CDATA[本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 本文写于2019/02/28，基于Go 1.11。至于其他版本的Go SDK，如有出入请自行查阅其他资料。 本文为[Gin-教程]连载系列的第三篇，更多后续请继续关注。 Overview上一篇《[Gin教程-2]数据库与Models实现》里设计了MySQL表以及对应的Model，并且为每个Model都实现了一些必要的增/删/改/查接口。利用MySQL，我们可以把很多数据都持久化下来。但是在本项目中也有一些数据是MySQL不擅长处理的，或者说有别的存储组件比MySQL更适合用来实现我们的需求，如： 记录用户登录状态 用户成功登录后系统要记录Ta已登录；登录的有效期为X秒，X秒内如果没有任何操作，X秒后登录状态要过期，要求用户重新登录；用户可主动退出登录，系统要及时更新Ta的登录状态。更新操作非常频繁，不是那种一次写多次读的数据。 当做伪消息队列，在多个模块间传递消息 当一个操作不是原子操作，而是由多个小步骤组合而成，那么多个小步骤的执行顺序很可能需要通过消息来驱动。例如本项目的添加图片功能，第一步就是早MySQL表里插入新record，第二步就是将图片上传至腾讯云的对象存储，第三步就是将腾讯云的图片url更新到MySQL表中。第二和第三步之间就要通过消息来驱动，只有图片上传成功后，才能通知把图片url更新到表里。 存储图片数据 图片实际上就是一连串的字符数据，如果我们想把图片存下来，可以把这一串字符串存到MySQL里的一个text字段，又或者把图片存到本地的文件系统里，然后MySQL里记录该图片的文件路径。但是其实有更好的办法，就是用一些公有云提供的对象存储服务来存图片、视频、音频等非结构化的数据。 针对1、2的需求，我们可以用Redis来实现。本教程不涉及安装Redis的步骤，请自行查阅并安装。 针对3的需求，我们可以用七牛云/阿里云oss/aws S3/腾讯云cos来实现。由于腾讯云cos有50G的免费额度，基于这个原因本教程就选用了腾讯云cos，实际上各家的对象存储服务在简单存储上都没有太明显的差异，可根据自己的喜好选择。 实现Redis Utils首先在utils下新建redis.go代码文件。 gin-photo-storage|—— conf |—— ......|—— constant |—— ......|—— models |—— ......|—— utils |—— jwt.go |—— redis.go 本教程使用的redis client是go-redis，当然github上还有很多不错的redis client，也可以根据个人喜好更换。 Golang &lt;= 1.10且项目位于$GOPATH/src/下的，可直接go get -u github.com/go-redis/redis 否则可用go mod来安装 根据上面提到的需求，我们要用Redis实现以下接口： 用户登录后，把用户信息添加到Redis里，并设置过期时间 查询用户信息是否在Redis里（是否登录成功） 用户退出登录后，把用户信息从Redis里删除 图片开始上传时，把图片信息添加到Redis里 查询图片上传状态 消息通知，把消息发布到某个Redis channel 先在gin-photo-storage/constant/constant.go里加入几个常量： // Redis constantsREDIS_HOST = "REDIS_HOST"REDIS_PORT = "REDIS_PORT" 然后记得在gin-photo-storage/conf/server.conf里写上Redis的相关配置项。没有自行修改的话，默认地址是本机地址127.0.0.1，默认端口是6379。 &#123; ...... &quot;REDIS_HOST&quot;: &quot;127.0.0.1&quot;, &quot;REDIS_PORT&quot;: &quot;6379&quot;, ......&#125; 接下来开始实现redis.go。 // 以下为redis.go的内容package utilsimport ( "fmt" "gin-photo-storage/conf" "gin-photo-storage/constant" "github.com/go-redis/redis" "log" "strconv" "time")var RedisClient *redis.Clientvar InitComplete = make(chan struct&#123;&#125;, 1)// Init redis clientfunc init() &#123; host := conf.ServerCfg.Get(constant.REDIS_HOST) port := conf.ServerCfg.Get(constant.REDIS_PORT) RedisClient = redis.NewClient(&amp;redis.Options&#123; Addr: fmt.Sprintf("%s:%s", host, port), Password: "", DB: 0, &#125;) InitComplete &lt;- struct&#123;&#125;&#123;&#125;&#125;// Add an auth to redis, meaning that he/she has logged in.func AddAuthToRedis(username string) error &#123; key := fmt.Sprintf("%s%s", constant.LOGIN_USER, username) err := RedisClient.Set(key, username, constant.LOGIN_MAX_AGE * time.Second).Err() if err != nil &#123; log.Fatalln(err) return err &#125; return nil&#125;// Check if an auth is in redis.func IsAuthInRedis(username string) bool &#123; key := fmt.Sprintf("%s%s", constant.LOGIN_USER, username) err := RedisClient.Get(key).Err() if err != nil &#123; log.Println(err) return false &#125; return true&#125;// Remove an auth from redis, meaning he/she is logging out.func RemoveAuthFromRedis(username string) bool &#123; key := fmt.Sprintf("%s%s", constant.LOGIN_USER, username) err := RedisClient.Del(key).Err() if err != nil &#123; log.Println(err) return false &#125; return true&#125;// Set the upload status for a photo.func SetUploadStatus(key string, value int) bool &#123; err := RedisClient.Set(key, value, 0).Err() if err != nil &#123; log.Println(err) return false &#125; return true&#125;// Get the upload status of a photo.func GetUploadStatus(key string) int &#123; val := RedisClient.Get(key).Val() if val == "" &#123; return -2 // means no such key &#125; status, _ := strconv.Atoi(val) return status&#125;// Send a message to the given channel.func SendToChannel(channel string, message string) bool &#123; err := RedisClient.Publish(channel, message).Err() if err != nil &#123; log.Println(err) return false &#125; return true&#125; 这里有几点核心点需要注意： 在init()里，初始化连接后，往InitComplete这个channel发了一条空数据，主要是有别的模块一直监听这个channel，阻塞在那，等待Redis连接初始化的完成。通过这条空数据可以告诉别的模块“我已经把Redis连接初始化好了，你们可以开始使用该连接了”。 用户登录后往Redis里写信息，key是LOGIN_ + 用户名，value是用户名，且设了一个过期时间constant.LOGIN_MAX_AGE，查询和删除用户信息也是用的相同的key。 发消息不是真正意义上的点对点发消息，而是用了Redis本身的订阅发布（pub-sub）机制：Publish(channel, message)。 实现COS UtilsNote：首先注册腾讯云账号（应该用微信账号即可），确保开通了腾讯云COS，并且为本项目新建了一个bucket，bucket名称任意。 首先在utils下新建cos.go代码文件。 gin-photo-storage|—— conf |—— ......|—— constant |—— ......|—— models |—— ......|—— utils |—— jwt.go |—— redis.go |—— cos.go Golang &lt;= 1.10且项目位于$GOPATH/src/下的，可直接go get -u github.com/tencentyun/cos-go-sdk-v5 否则可用go mod来安装 先在gin-photo-storage/constant/constant.go里加入几个常量： // COS constantsCOS_BUCKET_NAME = "COS_BUCKET_NAME"COS_APP_ID = "COS_APP_ID"COS_REGION = "COS_REGION"COS_SECRET_ID = "COS_SECRET_ID"COS_SECRET_KEY = "COS_SECRET_KEY" 然后记得在gin-photo-storage/conf/server.conf里写上COS的相关配置项。以下的配置项均可在腾讯云的控制面板里获取。 &#123; ...... &quot;COS_SECRET_ID&quot;: &quot;......&quot;, &quot;COS_SECRET_KEY&quot;: &quot;......&quot;, &quot;COS_BUCKET_NAME&quot;: &quot;......&quot;, &quot;COS_APP_ID&quot;: &quot;......&quot;, &quot;COS_REGION&quot;: &quot;......&quot;, ......&#125; 接下来开始实现cos.go。 // 以下为cos.go的内容package utilsimport ( "context" "fmt" "gin-photo-storage/conf" "gin-photo-storage/constant" "github.com/tencentyun/cos-go-sdk-v5" "io" "log" "net/http" "net/url")var ( CosClient *cos.Client CosUrlFormat = "http://%s-%s.cos.%s.myqcloud.com" BucketName = "" AppID = "" Region = "")// init COS clientfunc init() &#123; BucketName = conf.ServerCfg.Get(constant.COS_BUCKET_NAME) AppID = conf.ServerCfg.Get(constant.COS_APP_ID) Region = conf.ServerCfg.Get(constant.COS_REGION) u, _ := url.Parse(fmt.Sprintf(CosUrlFormat, BucketName, AppID, Region)) b := &amp;cos.BaseURL&#123;BucketURL: u&#125; CosClient = cos.NewClient(b, &amp;http.Client&#123; Transport: &amp;cos.AuthorizationTransport&#123; SecretID: conf.ServerCfg.Get(constant.COS_SECRET_ID), SecretKey: conf.ServerCfg.Get(constant.COS_SECRET_KEY), &#125;, &#125;) log.Printf("COS client %s init", CosClient.BaseURL)&#125; 结合Redis和COS实现异步图片上传到了这一步，已经把Redis utils和COS utils实现后，可以把它们结合起来实现上一篇教程遗留的一个需求——图片上传。 我们有两种方式可以实现图片上传： 同步（sync）上传。调用AddPhoto后，先插入一条数据库record，再上传图片至COS，等待图片上传完成，更新MySQL record的url字段，最后才返回给调用者。 异步（async）上传。调用AddPhoto后，先插入一条数据库record，马上返回给调用者，状态为“上传中”。同时启动一个goroutine在后台执行上传操作，上传完成后再执行回调（callback）函数来更新MySQL record的url字段。 这两种方式的优劣非常明显。同步上传很容易实现且出错了很容易处理（要么立刻重试，要么放弃），但是需要调用者在调用AddPhoto后长时间等待（取决于网速/COS的速度/COS的稳定性）；异步上传则比较复杂，涉及组件间的通信（回调），但是API返回速度快，调用者无需等待图片上传。 再结合业务层面想一想哪种方式比较好。当一个用户点击新增图片后，是让用户在当前页面等3~5秒钟，当前页面不断转圈圈等待返回？还是让用户马上返回，在页面的某处显示一个进度框提示上传状态比较好？ 显然，异步上传比同步上传更加“人性化”，所以我们就来借助Redis和goroutine实现异步上传。 具体的设计如下图所示： Note: 在第4步启动后台goroutine后，并不需要等待4.1和4.2完成才执行第5步。而是启动了后台goroutine后马上就能返回给用户（5 &amp; 6），goroutine会在后台完成4.1和4.2步，不存在阻塞等待的过程。 基于以上的设计，在代码层面我们要实现： callback监听模块（时刻监听着Redis里的某个channel，监听到新消息后立即更新MySQL） 往COS上传图片的方法 新建goroutine，调用上传图片的方法 首先实现callback监听模块。在gin-photo-storage/constant/constant.go里先加上几个常量。 // Callback constantsURL_UPDATE_CHANNEL = "PHOTO_URL_UPDATE"PHOTO_UPDATE_ID_FORMAT = "photo-%d"PHOTO_DELETE_CHANNEL = "PHOTO_DELETE" 因为callback主要是针对MySQL做更新操作，所以把逻辑写在db.go里。 // 在db.go里补充以下代码// Init the database connection.func init() &#123; // ...... go ListenRedisCallback() // launch a background goroutine to listen to callbacks from redis&#125;// Listen to callback messages from redis channels.// 1. When a photo is uploaded successfully, the callback asks to update the photo url in the db.// 2. When it fails to upload a photo, the callback asks to delete the photo record in the db.func ListenRedisCallback() &#123; // wait until utils package is initialized &lt;- utils.InitComplete // subscribe redis channels updateChan := utils.RedisClient.Subscribe(constant.URL_UPDATE_CHANNEL).Channel() deleteChan := utils.RedisClient.Subscribe(constant.PHOTO_DELETE_CHANNEL).Channel() // loop and listen for &#123; select &#123; case msg := &lt;-updateChan: photoID, _ := strconv.Atoi(msg.Payload[:strings.Index(msg.Payload, "-")]) photoUrl := msg.Payload[strings.Index(msg.Payload, "-") + 1:] if err := UpdatePhotoUrl(uint(photoID), photoUrl); err != nil &#123; log.Println(err) &#125; else &#123; utils.SetUploadStatus(fmt.Sprintf(constant.PHOTO_UPDATE_ID_FORMAT, photoID), 0) &#125; case msg := &lt;- deleteChan: photoID, _ := strconv.Atoi(msg.Payload) if err := DeletePhotoByID(uint(photoID)); err != nil &#123; log.Println(err) &#125; else &#123; utils.SetUploadStatus(fmt.Sprintf(constant.PHOTO_UPDATE_ID_FORMAT, photoID), -1) &#125; default: &#125; &#125;&#125; 此处的核心在于，当db.go所属的package（models）被加载时，在Init()方法的最后会启动一个后台goroutine，该goroutine实际上就是一个无限的for循环，订阅了Redis里的更新图片url的channel、删除图片的channel，即一直保持监听。在channel里接收到新消息后，就去执行“更新MySQL里的图片url”或者“删除图片”。 接着利用COS utils实现上传图片，即在cos.go里实现Upload()和AsyncUpload()。 // 以下代码在cos.go里实现// upload a photo to the tencent cloud COS// @photoID: 图片id// @fileName: 图片名// @file: 图片句柄// @fileSize: 图片大小func Upload(photoID uint, fileName string, file io.Reader, fileSize int) string &#123; uploadID := fmt.Sprintf(constant.PHOTO_UPDATE_ID_FORMAT, photoID) go AsyncUpload(uploadID, photoID, fileName, file, fileSize) // upload in the ASYNC way return uploadID&#125;// upload a photo to the tencent cloud COS in the ASYNC wayfunc AsyncUpload(uploadID string, photoID uint, fileName string, file io.Reader, fileSize int) &#123; // set upload status in redis if !SetUploadStatus(uploadID, 1) &#123; log.Println("Fail to set upload status before upload.") return &#125; // upload the photo using COS SDK putOption := cos.ObjectPutOptions&#123;&#125; putOption.ObjectPutHeaderOptions = &amp;cos.ObjectPutHeaderOptions&#123;ContentLength: fileSize&#125; _, err := CosClient.Object.Put(context.Background(), fileName, file, &amp;putOption) // upload fails, send callback asking for photo deletion if err != nil &#123; log.Println(err) if !SendToChannel(constant.PHOTO_DELETE_CHANNEL, fmt.Sprintf("%d", photoID)) &#123; log.Println("Fail to send delete-photo message to channel") &#125; return &#125; // upload success, send callback asking for updating the photo url fileUrl := fmt.Sprintf(CosUrlFormat, BucketName, AppID, Region) + "/" + fileName updateUrlMessage := fmt.Sprintf("%d-%s", photoID, fileUrl) if !SendToChannel(constant.URL_UPDATE_CHANNEL, updateUrlMessage) &#123; log.Println("Fail to send update-photo-url message to channel") &#125;&#125; 可以看到，在Upload()里直接另起了一个goroutine来执行AsyncUpload()，然后不需要等上传完成就马上返回了，所以才能达到“异步”的效果。当AsyncUpload()里完成了图片上传，上传成功就往Redis里面的URL_UPDATE_CHANNEL里发消息，失败就往PHOTO_DELETE_CHANNEL里发消息，该消息会被我们上面实现的callback模块监听到，由callback来执行后面的更新或删除逻辑。 最后，因为上一篇教程里在AddPhoto()里没有上传图片的代码，所以别忘了在AddPhoto()里加上调用图片上传的代码。 // 以下代码在photo.go里实现// Add a new photofunc AddPhoto(photoToAdd *Photo, photoFileHeader *multipart.FileHeader) (*Photo, string, error) &#123; // ...... // upload to the tencent cloud COS if photoFile, err := photoFileHeader.Open(); err == nil &#123; uploadID := utils.Upload(photo.ID, photo.Name, bufio.NewReader(photoFile), int(photoFileHeader.Size)) return &amp;photo, uploadID, nil &#125; else &#123; log.Println(err) return nil, "", PhotoFileBrokenError &#125;&#125; Others本篇教程到这里，Redis utils、COS utils和图片异步上传已经全部实现了。可能有的读者会觉得怪怪的，如果我们用异步的方式上传图片，调用者确实不需要等待，马上能得到一个“上传中”的response，但它要怎么知道上传到底有没有完成，还有是什么时候完成的呢？ 注意看AsyncUpload()和ListenRedisCallback()里，都有执行SetUploadStatus()。上传之前，都先把图片的upload status设成1。如果成功上传，在ListenRedisCallback()里会将其设为0；如果上传失败，ListenRedisCallback()里会将其设为-1。 再留意redis.go，我们还实现了一个GetUploadStatus()，通过这个接口，就可以查询某张图片的上传状态。从前端实现的角度而言，想要知道图片上传成功与否，每隔N秒调用一下这个接口，询问上传状态即可。 总结 这里把Redis当做伪消息队列来用，利用的是它的pub-sub机制，那么pub-sub的底层是怎么实现的？ 关键词：Redis源码、订阅发布 用真正的消息队列组件来实现通知，如RocketMQ、Kafka等，高并发下可能会有什么问题？ 关键词：消息队列、exactly once、幂等操作 在redis.go里，InitComplete = make(chan struct{}, 1)，为什么要加一个size参数1？不加size参数会生成什么样的channel？他们有什么异同？ 关键词：Golang缓冲通道、阻塞 异步加载看起来很“先进”，不需要阻塞等待，但是潜在的风险也不少。如果一个接口不止对应一个异步操作（例如不只是上传，可能包括了10个独立的异步操作），那假如其中一个操作失败了，怎么处理这个失败？是应该不断重试？还是应该全部步骤都回滚？假如说3号步骤失败了，怎么通知1、2、4~10号步骤都回滚？ 关键词：失败补偿、异步操作 异步上传的核心，除了通过Redis做消息传递实现回调外，更重要的是goroutine不会让当前线程阻塞，自己在后台就能执行逻辑。goroutine看着很像Java、Python、C++里的Thread，那goroutine和其他语言里的线程有什么异同，和内核态的线程又是什么关系？ 关键词：goroutine、协程、线程 我们实现的上传逻辑，是让调用者先把一张图片的数据全部post到服务端，然后再从服务器上传到腾讯云COS的。这样无疑很浪费服务器的带宽，假如一张图片5MB，要从客户端接收5MB，再往COS发送5MB，高并发下绝对带宽不足，有什么方法可以改进吗？]]></content>
      <categories>
        <category>gin</category>
      </categories>
      <tags>
        <tag>gin</tag>
        <tag>教程</tag>
        <tag>连载</tag>
        <tag>后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Gin教程-2]数据库与Models实现]]></title>
    <url>%2F2019%2F02%2F25%2Fgin-tutorial-2%2F</url>
    <content type="text"><![CDATA[本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 本文写于2019/02/25，基于Go 1.11。至于其他版本的Go SDK，如有出入请自行查阅其他资料。 本文为[Gin-教程]连载系列的第二篇，更多后续请继续关注。 Overview在上一篇教程《[Gin教程-1]photo gallery项目》里，我们已经完成了： 配置文件 &amp; 配置解析 常量定义 JWT的实现 本篇将要开始真正的“业务开发”，讲讲数据库表的定义和对应model的实现。 组件&amp;技术数据库我们用的是MySQL 5.7，ORM框架用的是gorm。 为什么我们用MySQL呢？其实真正要用的是“关系型数据库”，因为我们的业务非常适合用关系型数据库来实现。粗略想一想，我们做的是图片存储库，大概要存这么些东西： 用户信息 图片所属的bucket数据 图片本身的数据 …… 不难想到，在业务逻辑层面，一个用户可以创建多个bucket，一个bucket里可以存多张图片，这就形成了“关系”，所以用关系型数据库是没错的。不过，本项目用的是MySQL，用别的关系型数据库如MariaDB、PostgreSQL、Microsoft Sql Server也并无太大差别。（它们本身的差别还是不小的，但是在本项目的业务层面没什么差别） 至于为什么要用gorm呢？其实Golang的ORM框架还是有不少的，具体ORM框架之间的对比文章也有很多，本项目选择gorm一是因为EDDYCJY的教程中是用的gorm，二是自己一路用下来确实觉得还不错，所以就继续沿用gorm了。如果读者有别的用习惯了的ORM框架，例如beego orm或者xorm，也可以自行替换，应该不会出什么问题。 数据库 E-R图定义 E-R图，就是实体关系图。为什么要在定义数据库表之前先画好E-R图呢？一是为了帮自己理清楚各个实体之间的关系，二是为了规范开发流程，以后有据可循。如果是团队合作，定义数据库表之前不画好E-R图，之后在开发过程中很可能出一大堆幺蛾子。 基于我们对“图片存储库”的业务描述，系统里的实体目前就是3个： 用户，一个用户可以有多个bucket bucket，一个bucket可以存多张图片 图片，一张图片只能属于一个用户 对应的E-R图如下： 数据库表定义 create database photo_gallery;use photo_gallery;# 用户表drop table if exists auth;create table auth( id int primary key auto_increment, user_name varchar(16) unique not null, password varchar(255) not null, email varchar(128) not null, created_at timestamp default CURRENT_TIMESTAMP, updated_at timestamp default CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP);# bucket表drop table if exists bucket;create table bucket( id int primary key auto_increment, auth_id int, name varchar(64) not null, state tinyint(1) default 1, size int default 0, description text, created_at timestamp default CURRENT_TIMESTAMP, updated_at timestamp default CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, CONSTRAINT UC_bucket UNIQUE(auth_id, name), INDEX idx_aid_name (auth_id, name));# 图片表drop table if exists photo;create table photo( id int primary key auto_increment, bucket_id int, auth_id int, name varchar(255) not null, tag varchar(255), url varchar(255) not null, description text, state tinyint(1) default 1, created_at timestamp default CURRENT_TIMESTAMP, updated_at timestamp default CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, constraint UC_photo UNIQUE(bucket_id, name), INDEX idx_bid_name (bucket_id, name)); 除了E-R图里标注的属性外，每个实体对应的表里都有共同的属性：created_at和updated_at，这是为了debug的时候更加方便地看到数据修改时间，一般都建议带上这两个字段。 实现数据库连接 定义完了数据库表，我们可以用Golang实现一个数据库连接，开始和数据库进行交互。先在项目路径下新建models文件夹，然后在models下新建db.go文件。 gin-photo-storage|—— conf |—— server.conf |—— cfg.go|—— constant |—— constant.go |—— resp_code.go|—— utils |—— jwt.go|—— models |—— db.go 在db.go里，我们要 读取数据库配置，如Host、User、Password等…… 根据配置建立起一个全局的连接，可供其他模块使用 package modelsimport ( "fmt" "gin-photo-storage/conf" "gin-photo-storage/constant" "gin-photo-storage/utils" _ "github.com/go-sql-driver/mysql" // remember to import mysql driver "github.com/jinzhu/gorm" "log" "strconv" "strings" "time")var db *gorm.DB// Init the database connection.func init() &#123; dbType := conf.ServerCfg.Get(constant.DB_TYPE) dbHost := conf.ServerCfg.Get(constant.DB_HOST) dbPort := conf.ServerCfg.Get(constant.DB_PORT) dbUser := conf.ServerCfg.Get(constant.DB_USER) dbPwd := conf.ServerCfg.Get(constant.DB_PWD) dbName := conf.ServerCfg.Get(constant.DB_NAME) var err error db, err = gorm.Open(dbType, fmt.Sprintf(constant.DB_CONNECT, dbUser, dbPwd, dbHost, dbPort, dbName)) if err != nil &#123; log.Fatalln("Fail to connect database!") &#125; db.SingularTable(true) if !db.HasTable(&amp;Auth&#123;&#125;) &#123; db.CreateTable(&amp;Auth&#123;&#125;) &#125; if !db.HasTable(&amp;Bucket&#123;&#125;) &#123; db.CreateTable(&amp;Bucket&#123;&#125;) &#125; if !db.HasTable(&amp;Photo&#123;&#125;) &#123; db.CreateTable(&amp;Photo&#123;&#125;) &#125;&#125; ⚠️Note：记得要先安装mysql的Golang驱动：github.com/go-sql-driver/mysql。 Model定义有了数据库表之后，每张表可以对应来实现一个Model，即Auth、Bucket、Photo。由于多个Model都有一些共同的属性可以抽象出来，如id、created_at、updated_at这种大家都有的，就没必要重复在每个结构体内都定义一遍。所以我们要定义一个包含共同属性的Model：BaseModel。 先在models下新建auth.go、bucket.go、photo.go三个代码文件。 gin-photo-storage|—— conf |—— server.conf |—— cfg.go|—— constant |—— constant.go |—— resp_code.go|—— utils |—— jwt.go|—— models |—— auth.go |—— bucket.go |—— db.go |—— photo.go 然后先在db.go里实现上面提到的BaseModel，之后再逐个实现Auth、Bucket、Photo。 BaseModel // 在db.go里实现// The base model of all models, including ID &amp; CreatedAt &amp; UpdatedAt.type BaseModel struct &#123; ID uint `json:"id" gorm:"primary_key;AUTO_INCREMENT" form:"id"` CreatedAt time.Time `json:"created_at" gorm:"default: CURRENT_TIMESTAMP" form:"created_at"` UpdatedAt time.Time `json:"updated_at" gorm:"default: CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP" form:"updated_at"`&#125; 其实，gorm的开发者也想到了这种抽象出共同属性的Model，所以框架里早已定义好了gorm.Model结构体，包含了ID、CreatedAt、UpdatedAt、DeletedAt。但是在这里我们不需要DeletedAt字段，所以就没有直接用它提供的Model，而是自行定义。 Auth // 在auth.go里实现package modelsimport ( "crypto/md5" "fmt" "github.com/pkg/errors" "io")type Auth struct &#123; BaseModel UserName string `json:"user_name" gorm:"type:varchar(16)"` Password string `json:"password" gorm:"type:varchar(16)"` Email string `json:"email" gorm:"type:varchar(128)"`&#125;var AuthExistsError = errors.New("auth already exists")// Add a new auth.func AddAuth(username, password, email string) error &#123; trx := db.Begin() defer trx.Commit() auth := Auth&#123;&#125; trx.Set("gorm:query_option", "FOR UPDATE"). Where("user_name = ?", username). First(&amp;auth) if auth.ID &gt; 0 &#123; return AuthExistsError &#125; hash := md5.New() io.WriteString(hash, password) // for safety, don't just save the plain text auth.UserName = username auth.Password = fmt.Sprintf("%x", hash.Sum(nil)) auth.Email = email err := trx.Create(&amp;auth).Error if err != nil &#123; return err &#125; return nil&#125;// Check if the auth is valid.func CheckAuth(username, password string) bool &#123; trx := db.Begin() defer trx.Commit() hash := md5.New() io.WriteString(hash, password) password = fmt.Sprintf("%x", hash.Sum(nil)) // for safety, don't just save the plain text auth := Auth&#123;&#125; trx.Set("gorm:query_option", "FOR UPDATE"). Where("user_name = ? AND password = ?", username, password). First(&amp;auth) if auth.ID &gt; 0 &#123; return true &#125; return false&#125; Auth里需要注意的是，注册时不能存明文密码，登陆校验时也不能直接用明文校验，需要用md5加一层壳，这在一定程度上能防止后端数据库被一锅端导致的密码泄露。 Bucket // 在bucket.go里实现package modelsimport ( "gin-photo-storage/constant" "github.com/pkg/errors" "log")// The bucket model.type Bucket struct &#123; BaseModel AuthID uint `json:"auth_id" gorm:"type:int" form:"auth_id"` Name string `json:"bucket_name" gorm:"type:varchar(64)" form:"bucket_name"` State int `json:"state" gorm:"type:tinyint(1)" form:"state"` Size int `json:"size" gorm:"type:int" form:"bucket_size"` Description string `json:"description" gorm:"type:text" form:"description"`&#125;var BucketExistsError = errors.New("bucket already exists")var NoSuchBucketError = errors.New("no such bucket")// Add a new bucket.func AddBucket(bucketToAdd *Bucket) error &#123; trx := db.Begin() defer trx.Commit() // check if the bucket exists, select with a WRITE LOCK. bucket := Bucket&#123;&#125; trx.Set("gorm:query_option", "FOR UPDATE"). Where("auth_id = ? AND name = ? AND state = ?", bucketToAdd.AuthID, bucketToAdd.Name, 1). First(&amp;bucket) if bucket.ID &gt; 0 &#123; return BucketExistsError &#125; bucket.AuthID = bucketToAdd.AuthID bucket.Name = bucketToAdd.Name bucket.State = 1 bucket.Size = 0 bucket.Description = bucketToAdd.Description if err := trx.Create(&amp;bucket).Error; err != nil &#123; log.Println(err) return err &#125; return nil&#125;// Delete an existed bucket.func DeleteBucket(bucketID uint) error &#123; trx := db.Begin() defer trx.Commit() result := trx.Where("id = ? and state = ?", bucketID, 1).Delete(Bucket&#123;&#125;) if err := result.Error; err != nil &#123; return err &#125; if affected := result.RowsAffected; affected == 0 &#123; return NoSuchBucketError &#125; return nil&#125;// Update an existed bucket.func UpdateBucket(bucketToUpdate *Bucket) error &#123; trx := db.Begin() defer trx.Commit() bucket := Bucket&#123;&#125; bucket.ID = bucketToUpdate.ID result := trx.Model(&amp;bucket).Updates(*bucketToUpdate) if err := result.Error; err != nil &#123; return err &#125; if affected := result.RowsAffected; affected == 0 &#123; return NoSuchBucketError &#125; return nil&#125;// Get a bucket by bucket id.func GetBucketByID(bucketID uint) (Bucket, error) &#123; trx := db.Begin() defer trx.Commit() bucket := Bucket&#123;&#125; found := NoSuchBucketError trx.Where("id = ?", bucketID).First(&amp;bucket) if bucket.ID &gt; 0 &#123; found = nil &#125; return bucket, found&#125;// Get all buckets of the given user.func GetBucketByAuthID(authID uint, offset int) ([]Bucket, error) &#123; trx := db.Begin() defer trx.Commit() buckets := make([]Bucket, 0, constant.PAGE_SIZE) err := trx.Where("auth_id = ?", authID). Offset(offset). Limit(constant.PAGE_SIZE). Find(&amp;buckets).Error if err != nil &#123; log.Println(err) return buckets, err &#125; return buckets, nil&#125; 同样地，定义了Bucket结构体外，还定义了对它增/删/改/查的接口。 Photo // 在photo.go里实现package modelsimport ( "bufio" "gin-photo-storage/constant" "gin-photo-storage/utils" "github.com/jinzhu/gorm" "github.com/pkg/errors" "log" "mime/multipart")var NoSuchPhotoError = errors.New("no such photo")var PhotoExistsError = errors.New("photo already exists")var PhotoFileBrokenError = errors.New("photo file is broken")// The photo model.type Photo struct &#123; BaseModel AuthID uint `json:"auth_id" gorm:"type:int" form:"auth_id"` BucketID uint `json:"bucket_id" gorm:"type:int" form:"bucket_id"` Name string `json:"name" gorm:"type:varchar(255)" form:"name"` Tag string `json:"tag" gorm:"type:varchar(255)" form:"tag"` Tags []string `json:"tags" gorm:"-" form:"tags"` Url string `json:"url" gorm:"type:varchar(255)" form:"url"` Description string `json:"description" gorm:"type:text" form:"description"` State int `json:"state" gorm:"type:tinyint(1)" form:"state"`&#125;// Add a new photofunc AddPhoto(photoToAdd *Photo, photoFileHeader *multipart.FileHeader) (*Photo, string, error) &#123; trx := db.Begin() defer trx.Commit() // check if the photo exists, select with a WRITE LOCK photo := Photo&#123;&#125; trx.Set("gorm:query_option", "FOR UPDATE"). Where("bucket_id = ? AND name = ?", photoToAdd.BucketID, photoToAdd.Name). First(&amp;photo) if photo.ID &gt; 0 &#123; return nil, "", PhotoExistsError &#125; photo.AuthID = photoToAdd.AuthID photo.BucketID = photoToAdd.BucketID photo.Name = photoToAdd.Name photo.Tag = photoToAdd.Tag photo.Description = photoToAdd.Description photo.State = 1 err := trx.Create(&amp;photo).Error if err != nil &#123; log.Println(err) return nil, "", err &#125; err = trx.Model(&amp;Bucket&#123;&#125;).Where("id = ?", photoToAdd.BucketID). Update("size", gorm.Expr("size + ?", 1)). Error if err != nil &#123; trx.Rollback() log.Println(err) return nil, "", err &#125; // TODO: upload to the tencent cloud COS // ......&#125;// Delete a photo by photo id.func DeletePhotoByID(photoID uint) error &#123; trx := db.Begin() defer trx.Commit() result := trx.Where("id = ? AND state = ?", photoID, 1).Delete(Photo&#123;&#125;) if err := result.Error; err != nil &#123; log.Println(err) return err &#125; if affected := result.RowsAffected; affected == 0 &#123; return NoSuchPhotoError &#125; return nil&#125;// Delete a photo by its bucket id &amp; its name.func DeletePhotoByBucketAndName(bucketID uint, name string) error &#123; trx := db.Begin() defer trx.Commit() result := trx.Where("bucket_id = ? AND name = ?", bucketID, name).Delete(Photo&#123;&#125;) if err := result.Error; err != nil &#123; return err &#125; if affected := result.RowsAffected; affected == 0 &#123; return NoSuchPhotoError &#125; return nil&#125;// Update a photo.func UpdatePhoto(photoToUpdate *Photo) error &#123; trx := db.Begin() defer trx.Commit() photo := Photo&#123;&#125; photo.ID = photoToUpdate.ID result := trx.Model(&amp;photo).Updates(photoToUpdate) if err := result.Error; err != nil &#123; log.Println(err) return err &#125; if affected := result.RowsAffected; affected == 0 &#123; return NoSuchPhotoError &#125; return nil&#125;// Update the url for a photo.func UpdatePhotoUrl(photoID uint, url string) error &#123; trx := db.Begin() defer trx.Commit() photo := Photo&#123;&#125; photo.ID = photoID err := trx.Model(&amp;photo).Update("url", url).Error if err != nil &#123; return err &#125; return nil&#125;// Get a photo by its photo id.func GetPhotoByID(photoID uint) (*Photo, error) &#123; trx := db.Begin() defer trx.Commit() photo := Photo&#123;&#125; err := trx.Where("id = ?", photoID).First(&amp;photo).Error found := NoSuchPhotoError if err != nil || photo.ID == 0 &#123; log.Println(err) found = err &#125; found = nil return &amp;photo, found&#125;// Get photos by bucket id.func GetPhotoByBucketID(bucketID uint, offset int) ([]Photo, error) &#123; trx := db.Begin() defer trx.Commit() photos := make([]Photo, 0, constant.PAGE_SIZE) err := trx.Where("bucket_id = ?", bucketID). Offset(offset). Limit(constant.PAGE_SIZE). Find(&amp;photos). Error if err != nil &#123; return photos, err &#125; return photos, nil&#125; 这里要提出三个问题： ⚠️AddPhoto里的逻辑并未完善，现在只是往数据库加了一行图片记录，但是还未把图片真正地存下来（上传至云存储）。在下一篇会重点实现这个需求。 读者们应该也能发现，在每个Model结构体的每个字段后面，都会跟着一串“标签”（struct tag），标签里面有json，有gorm，有form。简单来说，标签就是用来标注字段的“别名”或者标注字段属性的。json标签就是在JSON序列化/反序列化过程中用的，gorm标签就是gorm框架使用的，form标签也是gorm框架在绑定请求参数的时候用的。通常，字段的标签会在反射中大量被用到。 在大项目的开发中，增/删/改/查的接口远远不止这么点，根据业务需求还可以有很多针对特定条件的增/删/改/查接口。具体的接口实现要和你的业务上下游做好对接，例如搞清楚前端需要什么样的数据，或者搞清楚在整个pipeline中你的业务下游需要什么样的数据。 总结思考点： 在多个Add和Update的接口中，入参都是指针类型，而不是常见的值类型，为什么？ 关键词：Golang指针、值复制、节省内存 Golang的结构体标签（struct tag）怎么用？ 关键词：struct tag、反射 在bucket表里定义了size属性，意味着每次添加图片都要更新bucket size字段。假如把场景扩大，不局限于bucket size，而是一个什么size，在高并发下频繁更新MySQL表里的size字段，性能好吗？如果用缓存存一下，然后再异步落盘到MySQL可以吗？异步落盘可能带来什么问题？ 关键词：缓存、高并发、缓存一致性 用ORM框架有什么好处？ORM框架可能带来什么问题？ 数据库表为什么要加索引？索引的原理是什么？ 关键词：InnoDB索引、B+树 关系型数据库是不是万能？在什么场景下不好用？ 关键词：全文检索、模糊搜索、OLAP 为什么把数据库表对应的结构体叫作Model？ 关键词：MVC模式 为什么在一些sql查询里要加上FOR UPDATE？ 关键词：InnoDB锁机制、并发安全、读写锁 为什么在所有的查询里，都启动一个事务（trx := db.Begin()）来执行操作？不用事务，直接查询有什么问题吗？ 关键词：数据库事务，ACID]]></content>
      <categories>
        <category>gin</category>
      </categories>
      <tags>
        <tag>gin</tag>
        <tag>教程</tag>
        <tag>连载</tag>
        <tag>后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Gin教程-1]photo gallery项目]]></title>
    <url>%2F2019%2F02%2F23%2Fgin-tutorial-1%2F</url>
    <content type="text"><![CDATA[本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 本文写于2019/02/23，基于Go 1.11。至于其他版本的Go SDK，如有出入请自行查阅其他资料。 本文为[Gin-教程]连载系列的第一篇，更多后续请继续关注。 项目简介作为一个Golang新手，受Github上EDDYCJY的gin教程（EDDYCJY/blog）影响，一路跟着用gin做了一个练手小项目。本着得益之后回馈初学者的精神，决定模仿EDDYCJY的教程，也写写基于gin的后端项目教程，有的地方类似，但也有很多不相同的地方。 本项目没有选取“博客”、“商城”、“音乐播放器”等热门的练手题材，而是要做一个“photo gallery”，也就是在线的图片存储库。由于本人前端知识的极度匮乏（几乎没写过），所以本项目和对应的教程里将不会涉及任何前端的开发。但是有前端基础的读者，可以结合本系列教程的后端代码，适当改造一下，再实现前端页面，最终也能做出一个完整的可展示的系统。 同时，必须强调，本系列教程的目标受众是Golang初学者/gin初学者/后端开发初学者，有一些在特大系统里要特别注意的问题无法在教程里详细实现。例如“失败补偿”，“熔断/限流”，“高可用”，“分布式”，“弹性扩容”等，在本系列教程里基本不予考虑，但是会以思考题的形式提出，引导读者思考。 本项目用到的技能/工具大致如下 Golang gin zap MySQL &amp; gorm Redis &amp; go-redis Elasticsearch Nginx Docker 腾讯云对象存储 本系列教程大致分为10篇 配置文件 + 常量定义 + JWT实现 Models定义与数据库 API开发-1 API开发-2与中间件使用 API开发-3 增加Elasticsearch支持全文检索 用zap替代默认日志库 协同开发之引入swagger文档 本地https证书搭建 + Nginx部署 使用docker隔离服务 项目结构 新建项目 Golang版本 &lt;= 1.10，建议直接把项目根目录创在$GOPATH/src下 Golang版本 &gt;= 1.11，可以放在本地任意位置，用go mod来管理第三方package 目录结构 # Golang version &lt;= 1.10$GOPATH|—— src |—— gin-photo-storage |—— ... |—— ... |—— ... # Golang version &gt;= 1.11Anywhere you like|—— gin-photo-storage |—— ... |—— ... |—— ... 对go mod感兴趣的可以去搜索Golang从低版本到高版本一路走来各种package管理的方式。 如何加载配置文件首先来看这么几个问题 一个项目里是不是很多地方要用到配置项？ 通常来说，是的。一般项目越大，里面涉及的组件越多（关系型数据库、内存数据库、消息队列……），需要的配置项就越多。你要连数据库总得提供数据库地址&amp;用户名&amp;密码……你要从消息队列收发消息总得提供地址&amp;topic……这些全都是“配置项”。 配置项全部写在代码文件里好不好？ 既然项目里无处不存在“配置项”，那干脆在写业务代码的时候随手一写就好了。但是这样麻烦就大了，假如开发前期用的是开发环境的数据库，地址是aaa.bbb.ccc.ddd，到后期上测试了要换成测试环境的数据库，地址是xxx.yyy.zzz.kkk，那就要大幅修改N个代码文件，然后提交重新code review。类似的情况远远不止数据库，可以是各种存在多版本问题的配置项。 如果不写在代码文件里，应该写到哪里？ 很明显，如果不写在代码里（所谓的“hard code”），那就只能写在额外的“非代码”文件里，然后你的代码逻辑里有读配置文件的操作。每次要改配置项的值时，改配置文件即可，不需要再改一大堆代码文件。甚至可以为测试环境定义一个config-test-env文件，为开发环境定义一个config-dev-env文件，为任意环境定义它专属的配置文件，在程序启动时通过命令行参数指定用哪套环境，方便快捷，无痛切换。 所以，整个项目的第一步，我们要新建配置目录conf、配置文件：server.conf、解析配置文件的代码cfg.go。 gin-photo-storage|—— conf |—— server.conf |—— cfg.go 在本项目里，直接把配置项写成JSON格式，易读易修改。当然也可以像EDDYCJY在（Gin搭建Blog API’s （一））里那样，用ini作配置文件的格式，用ini对应的package来解析，这个倒是没有太大的讲究。 # 以下为server.conf的内容&#123; &quot;JWT_SECRET&quot;: &quot;ufqw923dh1nlo9sa&quot;, &quot;DB_TYPE&quot;: &quot;mysql&quot;, &quot;DB_HOST&quot;: &quot;127.0.0.1&quot;, &quot;DB_PORT&quot;: &quot;3306&quot;, &quot;DB_USER&quot;: &quot;......&quot;, &quot;DB_PWD&quot;: &quot;......&quot;, &quot;DB_NAME&quot;: &quot;photo&quot;, &quot;SERVER_PORT&quot;: &quot;9088&quot;&#125; 创建好配置文件后，可以开始写解析配置文件的代码了。因为这里用的JSON格式，所以不需要引用任何第三方package，Golang原生自带JSON序列化&amp;反序列化的库。 // 以下为cfg.go的代码package confimport ( "encoding/json" "log" "os")type Cfg struct &#123; ConfigMap map[string]string&#125;var ServerCfg Cfg// Init config from the local config file.func init() &#123; confFile, err := os.Open("conf/server.conf") defer confFile.Close() if err != nil &#123; log.Fatalln(err) &#125; ServerCfg.ConfigMap = make(map[string]string) err = json.NewDecoder(confFile).Decode(&amp;ServerCfg.ConfigMap) if err != nil &#123; log.Fatalln(err) &#125;&#125;// Get the corresponding config value of the given key.func (cfg *Cfg) Get(key string) string &#123; if val, ok := cfg.ConfigMap[key]; ok &#123; return val &#125; log.Fatalf("No such config term: %s!\n", key) return ""&#125; 可以看到，代码里的init()就是用来读取同个文件夹下的server.conf，然后用原生的JSON解析包对其内容进行解析。另外，为了在其他代码模块中方便读取配置值，我们实现了一个Get(key string)方法。 常量定义——避免magic number除了配置项，任何项目里不可缺少的还有各种常量。相比配置项的“动态”而言，常量一般是指跟代码逻辑紧密结合的值，绝大多数情况下基本不会因外部开发环境的变化而变化。不会说切到测试环境就用测试环境的一套，切到开发环境就用开发环境的一套。 例如，针对客户端的请求所回复的数值型状态码（SUCCESS、FAIL、PENDING之类的），或者是代码里的一些常用字符串。这样的值最好还是定义成常量，集中写到一个具体的常量代码文件里。否则直接在N个代码文件里大量使用magic number的话，万一到时要改就比较麻烦。 首先在项目路径下新建constant目录，然后在constant路径下新建constant.go和resp_code.go。 gin-photo-storage|—— conf |—— server.conf |—— cfg.go|—— constant |—— constant.go |—— resp_code.go 在constant.go里，我们要定义一些代码里经常使用的字符串/整形常量。 // 以下为constant.go的内容package constantconst ( // JWT constants JWT_SECRET = "JWT_SECRET" JWT = "jwt" JWT_EXP_MINUTE = 30 PHOTO_STORAGE_ADMIN = "admin" // Server constants SERVER_PORT = "SERVER_PORT" PAGE_SIZE = 20 // DB constants DB_CONNECT = "%s:%s@tcp(%s:%s)/%s?charset=utf8&amp;parseTime=True&amp;loc=Local" DB_TYPE = "DB_TYPE" DB_HOST = "DB_HOST" DB_PORT = "DB_PORT" DB_USER = "DB_USER" DB_PWD = "DB_PWD" DB_NAME = "DB_NAME" // Auth constants COOKIE_MAX_AGE = 1800 LOGIN_MAX_AGE = 1800 LOGIN_USER = "LOGIN_") 而在resp_code.go里，我们要定义所有API的返回码，毕竟客户端收到回复后基本都要从返回码来判断服务端的执行情况。 // 以下为resp_code.go的内容package constantconst ( // User related responses USER_ALREADY_EXIST = 1001 USER_ADD_SUCCESS = 1002 USER_AUTH_SUCCESS = 1003 USER_AUTH_ERROR = 1004 USER_AUTH_TIMEOUT = 1005 USER_SIGNOUT_SUCCESS = 1006 // JWT related responses JWT_GENERATION_ERROR = 2001 JWT_MISSING_ERROR = 2002 JWT_PARSE_ERROR = 2003 // Bucket related responses BUCKET_ALREADY_EXIST = 3001 BUCKET_ADD_SUCCESS = 3002 BUCKET_NOT_EXIST = 3003 BUCKET_DELETE_SUCCESS = 3004 BUCKET_UPDATE_SUCCESS = 3005 BUCKET_GET_SUCCESS = 3006 // Photo related responses PHOTO_ALREADY_EXIST = 4001 PHOTO_ADD_IN_PROCESS = 4002 PHOTO_UPLOAD_SUCCESS = 4003 PHOTO_UPLOAD_ERROR = 4004 PHOTO_NOT_EXIST = 4005 PHOTO_DELETE_SUCCESS = 4006 PHOTO_UPDATE_SUCCESS = 4007 PHOTO_GET_SUCCESS = 4008 // Internal server responses INTERNAL_SERVER_ERROR = 5001 PAGINATION_SUCCESS = 8001 INVALID_PARAMS = 9001)var Message map[int]string// Init the message map.func init() &#123; Message = make(map[int]string) Message[INVALID_PARAMS] = "Invalid parameters." Message[USER_ALREADY_EXIST] = "User already exists." Message[USER_ADD_SUCCESS] = "Add user success." Message[USER_AUTH_SUCCESS] = "User authentication success." Message[USER_AUTH_ERROR] = "User authentication fail." Message[USER_AUTH_TIMEOUT] = "User authentication timeout." Message[USER_SIGNOUT_SUCCESS] = "User sign out success." Message[JWT_GENERATION_ERROR] = "JWT generation fail." Message[JWT_MISSING_ERROR] = "JWT is missing." Message[INTERNAL_SERVER_ERROR] = "Internal server error." Message[BUCKET_ALREADY_EXIST] = "Bucket already exists." Message[BUCKET_ADD_SUCCESS] = "Add bucket success." Message[BUCKET_NOT_EXIST] = "Bucket does not exist." Message[BUCKET_DELETE_SUCCESS] = "Bucket delete success." Message[BUCKET_UPDATE_SUCCESS] = "Bucket update success." Message[BUCKET_GET_SUCCESS] = "Bucket get success." Message[PHOTO_ALREADY_EXIST] = "Photo already exists." Message[PHOTO_ADD_IN_PROCESS] = "Adding photo is in process." Message[PHOTO_UPLOAD_SUCCESS] = "Photo upload success." Message[PHOTO_UPLOAD_ERROR] = "Photo upload error." Message[PHOTO_NOT_EXIST] = "Photo does not exist." Message[PHOTO_DELETE_SUCCESS] = "Photo delete success." Message[PHOTO_GET_SUCCESS] = "Photo get success."&#125;// Translate a response code to a detailed message.func GetMessage(code int) string &#123; msg, ok := Message[code] if ok &#123; return msg &#125; return ""&#125; 这里要注意的是，我们不仅定义了返回码，还为每个返回码定义了对应的详细信息。当然，不定义详细信息也不是不可以，但是那样的话对客户端的可读性就很差，每次想查某个返回码对应的是什么信息时都得翻查文档。 JWT定义完配置文件&amp;常量后，我们来实现一下JWT。JWT = JSON Web Token，是用来做鉴权的一种手段。毕竟我们辛辛苦苦写好，部署好的API不是随便一个人都可以任意调用的，只有通过鉴权的调用才是真正的合法调用。关于JWT的详细解释，可以查看我之前的一篇文章《[Golang]JWT及其Golang实现》。 因为JWT还不算我们业务逻辑里的模块，将其看作utils就行，所以要在项目路径下新建utils目录，并新建jwt.go。 gin-photo-storage|—— conf |—— server.conf |—— cfg.go|—— constant |—— constant.go |—— resp_code.go|—— utils |—— jwt.go 至于JWT的具体实现，因为它涉及一些数字签名的算法，造轮子手写大概率不是一个好的选择，所以我们直接用一个第三方的JWT库——github.com/dgrijalva/jwt-go来实现。 Golang &lt;= 1.10且项目位于$GOPATH/src下的可以直接用go get来安装：go get -u github.com/dgrijalva/jwt-go Golang &gt;= 1.11且使用go mod来管理package的可以直接修改go.mod然后go mod init &lt;module-name&gt; // 以下为jwt.go的内容package utilsimport ( "gin-photo-storage/conf" "gin-photo-storage/constant" "github.com/dgrijalva/jwt-go" "log" "time")// self-defined user claimtype UserClaim struct &#123; UserName string `json:"user_name"` jwt.StandardClaims&#125;// Generate a JWT based on the user name.func GenerateJWT(userName string) (string, error) &#123; // define a user claim claim := UserClaim&#123; userName, jwt.StandardClaims&#123; Issuer: constant.PHOTO_STORAGE_ADMIN, ExpiresAt: time.Now().Add(constant.JWT_EXP_MINUTE * time.Minute).Unix(), &#125;, &#125; // generate the claim and the digital signature token := jwt.NewWithClaims(jwt.SigningMethodHS256, claim) jwtString, err := token.SignedString([]byte(conf.ServerCfg.Get(constant.JWT_SECRET))) if err != nil &#123; log.Fatalln("JWT generation error") return "", err &#125; return jwtString, nil&#125;// Parse a JWT into a user claim.func ParseJWT(jwtString string) (*UserClaim, error) &#123; token, err := jwt.ParseWithClaims(jwtString, &amp;UserClaim&#123;&#125;, func(token *jwt.Token) (interface&#123;&#125;, error) &#123; return []byte(conf.ServerCfg.Get(constant.JWT_SECRET)), nil &#125;) if token != nil &amp;&amp; err == nil &#123; if claim, ok := token.Claims.(*UserClaim); ok &amp;&amp; token.Valid &#123; return claim, nil &#125; &#125; return nil, err&#125; 实现JWT的核心点有3个： 自定义一个claim的结构，在这里就是UserClaim，其中简单地包含一个用户名。 实现生成JWT的逻辑，也就是传入自定义claim的字段，为其生成一个JWT字符串。 实现解析JWT的逻辑，也就是传入一个JWT字符串，反生成出一个自定义的claim。 总结发散思考： 配置文件server.conf放在conf文件夹里，往git仓库上提交代码时，应不应该把配置文件也提交上去？如果不应该提交，要怎么设置？ 关键词：git命令、git ignore 跟hard code在代码里相比，配置文件已经灵活了无数倍，但还有没有更好的方案来存储&amp;读取配置？假如想动态更新配置值，不用重启程序就能读取到最新的修改值，要怎么做？ 关键词：配置中心、热更新 JWT有什么功能上的缺点？有没有别的鉴权机制可以克服JWT的缺点？多种鉴权方式互相对比各有什么优劣？ 关键词：Cookie-Session、主动过期、单点登录 用JWT的安全性如何？常见的web攻击手段能不能攻破JWT？ 关键词：CSRF、XSS]]></content>
      <categories>
        <category>gin</category>
      </categories>
      <tags>
        <tag>gin</tag>
        <tag>教程</tag>
        <tag>连载</tag>
        <tag>后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Golang]字符串拼接方式的性能分析]]></title>
    <url>%2F2019%2F02%2F16%2Fgolang-str-join%2F</url>
    <content type="text"><![CDATA[本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 本文写于2019/02/16，基于Go 1.11。至于其他版本的Go SDK，如有出入请自行查阅其他资料。 Overview写本文的动机来源于Golang中文社区里一篇有头没尾的帖子《Go语言字符串高效拼接》，里面只提了Golang里面字符串拼接的几种方式，但是在最后却不讲每种方式的性能，也没有给出任何的best practice。本着无聊 + 好奇心，就决定自行写benchmark来测试，再对结果和源码进行分析，试图给出我认为的best practice吧。 性能测试根据帖子里的内容，在Golang里有5种字符串拼接的方式： 直接+号拼接 // 直接+号拼接func ConcatWithAdd(strs []string) string &#123; s := "" for _, str := range strs &#123; s += str &#125; return s&#125; fmt.Sprint()拼接 // fmt拼接func ConcatWithFmt(strs []string) string &#123; s := fmt.Sprint(strs) return s&#125; strings.Join()拼接 // strings.Join拼接func ConcatWithJoin(strs []string) string &#123; s := strings.Join(strs, "") return s&#125; Buffer拼接 // bytes.Buffer拼接func ConcatWithBuffer(strs []string) string &#123; buf := bytes.Buffer&#123;&#125; for _, str := range strs &#123; buf.WriteString(str) &#125; return buf.String()&#125; Builder拼接 // strings.Builder拼接func ConcatWithBuilder(strs []string) string &#123; builder := strings.Builder&#123;&#125; for _, str := range strs &#123; builder.WriteString(str) &#125; return builder.String()&#125; 为了测试各自的性能，就用Golang自带test模块的benchmark来进行测试。 在测试中，分3组数据，5组测试，即一共3 * 5 = 15次独立测试。其中3组数据是指： size = 10K的字符串数组，每个元素均为&quot;hello&quot; size = 50K的字符串数组，每个元素均为&quot;hello&quot; size = 100K的字符串数组，每个元素均为&quot;hello&quot; 5组测试是指： 直接+号拼接，要跑10K、50K、100K的数据 fmt.Sprint()拼接，要跑10K、50K、100K的数据 strings.Join()拼接，要跑10K、50K、100K的数据 Buffer拼接，要跑10K、50K、100K的数据 Builder拼接，要跑10K、50K、100K的数据 Benchmark代码如下： package mainimport ( "os" "testing")var ( Strs10K []string // 长度为10K的字符串数组 Strs50K []string // 长度为50K的字符串数组 Strs100K []string // 长度为100K的字符串数组 word = "hello" // 待拼接的字符串)const ( ADD = iota BUFFER BUILDER JOIN FMT _10K = 10000 _50K = 50000 _100K = 100000)// preset和teardownfunc TestMain(m *testing.M) &#123; Strs10K = make([]string, 0, _10K) Strs50K = make([]string, 0, _50K) Strs100K = make([]string, 0, _100K) for i := 0;i &lt; _100K;i++ &#123; if (i &lt; _10K) &#123; Strs10K = append(Strs10K, word) Strs50K = append(Strs50K, word) &#125; else if (i &lt; _50K) &#123; Strs50K = append(Strs50K, word) &#125; Strs100K = append(Strs100K, word) &#125; exitCode := m.Run() os.Exit(exitCode)&#125;// 测试直接+号拼接func BenchmarkConcatWithAdd(b *testing.B) &#123; b.Run("Concat-10000", GetTestConcat(Strs10K, ADD)) b.Run("Concat-50000", GetTestConcat(Strs50K, ADD)) b.Run("Concat-100000", GetTestConcat(Strs100K, ADD))&#125;// 测试bytes.Buffer拼接func BenchmarkConcatWithBuffer(b *testing.B) &#123; b.Run("Concat-10000", GetTestConcat(Strs10K, BUFFER)) b.Run("Concat-50000", GetTestConcat(Strs50K, BUFFER)) b.Run("Concat-100000", GetTestConcat(Strs100K, BUFFER))&#125;// 测试strings.Builder拼接func BenchmarkConcatWithBuilder(b *testing.B) &#123; b.Run("Concat-10000", GetTestConcat(Strs10K, BUILDER)) b.Run("Concat-50000", GetTestConcat(Strs50K, BUILDER)) b.Run("Concat-100000", GetTestConcat(Strs100K, BUILDER))&#125;// 测试strings.Join拼接func BenchmarkConcatWithJoin(b *testing.B) &#123; b.Run("Concat-10000", GetTestConcat(Strs10K, JOIN)) b.Run("Concat-50000", GetTestConcat(Strs50K, JOIN)) b.Run("Concat-100000", GetTestConcat(Strs100K, JOIN))&#125;// 测试fmt拼接func BenchmarkConcatWithFmt(b *testing.B) &#123; b.Run("Concat-10000", GetTestConcat(Strs10K, FMT)) b.Run("Concat-50000", GetTestConcat(Strs50K, FMT)) b.Run("Concat-100000", GetTestConcat(Strs100K, FMT))&#125;// 根据拼接类型（testType），返回对应的测试方法func GetTestConcat(strs []string, testType int) func(b *testing.B) &#123; concatFunc := func([]string) string &#123;return ""&#125; switch testType &#123; case ADD: concatFunc = ConcatWithAdd case BUFFER: concatFunc = ConcatWithBuffer case BUILDER: concatFunc = ConcatWithBuilder case JOIN: concatFunc = ConcatWithJoin case FMT: concatFunc = ConcatWithFmt &#125; return func(b *testing.B) &#123; for i := 0;i &lt; b.N;i++ &#123; concatFunc(strs) &#125; &#125;&#125; 经过测试（go test -bench=. -benchmem），结果如下： ...... 4 BenchmarkConcatWithAdd/Concat-10000-4 20 57050217 ns/op 270493320 B/op 9999 allocs/op 5 BenchmarkConcatWithAdd/Concat-50000-4 2 937660008 ns/op 6435464656 B/op 49999 allocs/op 6 BenchmarkConcatWithAdd/Concat-100000-4 1 3748714961 ns/op 25388918224 B/op 99999 allocs/op 7 BenchmarkConcatWithBuffer/Concat-10000-4 10000 138797 ns/op 209376 B/op 12 allocs/op 8 BenchmarkConcatWithBuffer/Concat-50000-4 3000 481466 ns/op 840160 B/op 14 allocs/op 9 BenchmarkConcatWithBuffer/Concat-100000-4 2000 966963 ns/op 1659360 B/op 15 allocs/op 10 BenchmarkConcatWithBuilder/Concat-10000-4 10000 103924 ns/op 227320 B/op 21 allocs/op 11 BenchmarkConcatWithBuilder/Concat-50000-4 3000 495917 ns/op 1431545 B/op 28 allocs/op 12 BenchmarkConcatWithBuilder/Concat-100000-4 2000 891950 ns/op 2930682 B/op 31 allocs/op 13 BenchmarkConcatWithJoin/Concat-10000-4 10000 106288 ns/op 114688 B/op 2 allocs/op 14 BenchmarkConcatWithJoin/Concat-50000-4 3000 505209 ns/op 507904 B/op 2 allocs/op 15 BenchmarkConcatWithJoin/Concat-100000-4 2000 990317 ns/op 1015808 B/op 2 allocs/op 16 BenchmarkConcatWithFmt/Concat-10000-4 1000 1293589 ns/op 227716 B/op 10002 allocs/op 17 BenchmarkConcatWithFmt/Concat-50000-4 200 6260637 ns/op 1131960 B/op 50003 allocs/op 18 BenchmarkConcatWithFmt/Concat-100000-4 100 12005780 ns/op 2499702 B/op 100006 allocs/op ...... 可以看出 运行速度上，Builder、Buffer、Join的速度属于同一数量级，绝对值也差不了太多；fmt要比它们一个数量级；直接+号拼接是最慢的。 内存分配上，Join表现最优秀，Buffer次之，Builder第三；而fmt和直接+号拼接最差，要执行很多次内存分配操作。 源码分析 速度&amp;内存分配都很优秀的strings.Join() func Join(a []string, sep string) string &#123; // 专门为短数组拼接做的优化 // 详情查阅golang.org/issue/6714 switch len(a) &#123; case 0: return "" case 1: return a[0] case 2: return a[0] + sep + a[1] case 3: return a[0] + sep + a[1] + sep + a[2] &#125; // 计算总共要插入多长的分隔符，n = 分隔符总长 n := len(sep) * (len(a) - 1) // 遍历待拼接的数组，逐个叠加字符串的长度 // 最后n = 分隔符总长 + 所有字符串的总长 = 拼接结果的总长 for i := 0; i &lt; len(a); i++ &#123; n += len(a[i]) &#125; // 一次性分配n byte的内存空间，并且把第一个字符串拷贝到slice的头部 b := make([]byte, n) bp := copy(b, a[0]) // 从下标为1开始，调用原生的copy函数 // 逐个把分隔符&amp;字符串拷贝到slice里对应的位置 for _, s := range a[1:] &#123; bp += copy(b[bp:], sep) bp += copy(b[bp:], s) &#125; // 最后将byte slice强转为string，返回 return string(b)&#125; 可以看出strings.Join()为什么表现如此优秀，主要原因是只有1次的显式内存分配（b := make([]byte, n)）和1次隐式内存分配（return string(b)），不需要在拼接过程中反复多次分配内存，挪动内存里的数据，减少了很多内存管理的消耗。 略差一筹的bytes.Buffer.WriteString() // 尝试扩容n个单位func (b *Buffer) tryGrowByReslice(n int) (int, bool) &#123; // 如果底层slice的剩余空间 &gt;= n个单位，就不需要重新分配内存 // 而是reslice，把底层slice的cap限定在l + n if l := len(b.buf); n &lt;= cap(b.buf)-l &#123; b.buf = b.buf[:l+n] return l, true &#125; // 如果底层slice的剩余空间不足n个单位，放弃reslice // 说明需要重新分配内存，而不是reslice那么简单了 return 0, false&#125;// 扩容n个单位func (b *Buffer) grow(n int) int &#123; m := b.Len() // 边界情况，空slice，先把一些属性reset掉 if m == 0 &amp;&amp; b.off != 0 &#123; b.Reset() &#125; // 先试试不真正分配空间，通过reslice来“扩容” if i, ok := b.tryGrowByReslice(n); ok &#123; return i &#125; // bootstrap是一个长度为64的slice，在buffer对象初始化时， // bootstrap就已经分配好了，如果n小于bootstrap长度， // 可以利用bootstrap slice来reslice，不需要重新分配内存空间 if b.buf == nil &amp;&amp; n &lt;= len(b.bootstrap) &#123; b.buf = b.bootstrap[:n] return 0 &#125; // 上述几种情况都无法满足 c := cap(b.buf) if n &lt;= c/2-m &#123; // 理解为m + n &lt;= c/2比较好 // 如果扩容后的长度（m + n）比c/2要小，说明当前还有一大堆可用的空间 // 直接reslice，以b.off打头 copy(b.buf, b.buf[b.off:]) &#125; else if c &gt; maxInt-c-n &#123; // c + c + n &gt; maxInt，申请扩容n个单位太多了，不可接受 panic(ErrTooLarge) &#125; else &#123; // 当前剩余的空间不太够了，重新分配内存，长度为c + c + n buf := makeSlice(2*c + n) copy(buf, b.buf[b.off:]) b.buf = buf &#125; // Restore b.off and len(b.buf). b.off = 0 b.buf = b.buf[:m+n] return m&#125;// 拼接的方法func (b *Buffer) WriteString(s string) (n int, err error) &#123; b.lastRead = opInvalid // 先尝试reslice得到len(s)个单位的空间 m, ok := b.tryGrowByReslice(len(s)) if !ok &#123; // 无法通过reslice得到空间，直接粗暴地申请grow m = b.grow(len(s)) &#125; return copy(b.buf[m:], s), nil&#125; 为什么bytes.Buffer.WriteString()性能比Join差呢，其实也是内存分配策略惹的祸。在Join里只有两次内存空间申请的操作，而Buffer里可能会有很多次。具体来说就是buf := makeSlice(2*c + n)这一句，每次重申请只申请2 * c + n的空间，用完了就要再申请2 * c + n。当拼接的数据项很多，每次申请的空间也就2 * c + n，很快就用完了，又要再重新申请，所以造成了性能不是很高。 略差一筹的strings.Builder() func (b *Builder) WriteString(s string) (int, error) &#123; b.copyCheck() b.buf = append(b.buf, s...) return len(s), nil&#125; 代码很简洁，就是最直白的slice append，一时append一时爽，一直append一直爽。所以当底层slice的可用空间不足，就会在append里一直申请新的内存空间。跟bytes.Buffer不同的是，这里并没有自己管理“扩容”的逻辑，而是交由原生的append函数去管理。 最差劲的fmt.Sprint() type buffer []bytetype pp struct &#123; buf buffer ......&#125;func Sprint(a ...interface&#123;&#125;) string &#123; p := newPrinter() p.doPrint(a) s := string(p.buf) p.free() return s&#125; printer里的核心数据结构就是buf，而buf其实就是一个[]byte，所以给buf不停地拼接字符串，空间不够了又继续开辟新的内存空间，所以性能低下。 总结实际上，只有当拼接的字符串非常非常多的时候，才需要纠结性能。像本文里动辄拼接10K、50K、100K个字符串的情况在实际业务中应该是很少很少的。 如果实在要纠结性能，参考以下几点 Join的速度最好，但是不至于完爆Builder和Buffer。三者的速度属于同一数量级。fmt和直接+号拼接速度最慢。 Join的内存分配策略最好，内存分配次数最少；Builder和Buffer的内存分配策略还算可以，类似于线性增长；fmt和直接+号拼接的内存分配策略最差。]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[再次实践MySQL InnoDB索引]]></title>
    <url>%2F2019%2F02%2F12%2Fmysql-innodb-index-in-practice%2F</url>
    <content type="text"><![CDATA[本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 本文写于2019/02/12，基于MySQL 5.7.18。至于其他版本的MySQL，如有出入请自行查阅其他资料。 Overview在上一篇博客《MySQL InnoDB索引的那些事》里，已经用一些例子来简单讲解InnoDB里的B+树索引是什么了。但是可能在内容上比较理论和生硬，讲的都是数据结构和分析类的东西。所以今天这篇博客主要想讲讲“in-practice”方面的东西，再来看看InnoDB的索引，特指B+树索引。 怎么知道有无触发索引要分析一条select语句有没有触发索引，最简单的做法就是用EXPLAIN来分析一下该select语句。 EXPLAIN select ... from ... where ...; 可是EXPLAIN得到的结果不是一句人能看懂的话，或者log类型的数据，而是一堆字段，接下来就逐个字段看看它们的含义。 id：在你所执行的sql语句中，某个select子句的执行顺序。id为1的是第一个被执行的子查询，以此类推。 select_type：该条select子句的类型，在MySQL 5.7中有11种可能取值。 select_type 含义 SIMPLE 简单查询，不包含union或者子查询 PRIMARY 在多层select中的最外层查询语句 UNION 在union中第二个或者最后一个查询语句 DEPENDENT UNION 在union中第二个或者最后一个查询语句，但依赖于外层查询 SUBQUERY 子查询中的第一条select语句 DEPENDENT SUBQUERY 子查询中的第一条select语句，但依赖于外层查询 DERIVED 派生表的查询，在from里的子查询 MATERIALIZED ??? UNCACHEABLE SUBQUERY 查询结果不可被缓存的子查询 UNCACHEABLE UNION 在union中第二个或最后一个查询语句，该查询语句属于一个UNCACHEABLE的SUBQUERY table：被查询的表的表名 partitions：对于分区表而言，该条select语句命中的记录所在的分区 type：按照官方手册的说法，就是表与表之间join的类型 type 含义 system 表中只有一行数据 const 表中最多只有一行匹配数据（例如select … from … where pk = ?） eq_ref ref 常出现于辅助索引的等值查询 fulltext 出现于全文检索索引 ref_or_null SUBQUERY index_merge unique_subquery index_subquery range 常出现于范围索引 index 与all几乎一样，常见于扫描secondary index上的覆盖索引数据 all 全表扫描 possible keys：该select语句中所有可能触发的索引 key：实际触发的索引（有可能多于1个） key_len：实际触发的索引列的长度 ref：等值查询时为const，如果用了表达式则可能为func rows：预估需要扫描行数 filtered：预估百分之多少的行会被查询条件过滤 Extra：备注信息，从中观察可优化的点 同时只能触发一个索引？在之前的那篇文章，还有一些其他网上的文章里会提到：如果一张表上有多个索引，在查询时只会触发其中一个。 其实单看这句话是不准确的。比较严谨准确的描述应该是：在查询时，遇到某一列包含在多个索引内时，同时只能触发其中一个索引。 举个例子，假如在a列上建了： 单列索引idx_a (a) 联合索引idx_abc (a, b, c) 联合索引idx_ak (a, k) 在执行select ... from ... where a = ?;时，只能同时触发以上3个索引的其中一个。 所以，如果查询语句的where条件中不止一列，是不是就可以触发多个索引呢？下面实际来试一试《MySQL技术内幕》里的例子。 先建一张测试表t create table t ( a int, b int, key (a), key (b)) ENGINE=INNODB;insert into t select 1, 1;insert into t select 1, 2;insert into t select 2, 3;insert into t select 2, 4;insert into t select 1, 2;explain select * from t where a = 1 and b = 2; 得到的结果如下图所示： 确实在一条select语句中触发了两个独立的单列索引，然后再将它们的中间结果取交集（Using intersect(b, a)），得到最终结果。 但是，不是所有情况下都能触发多个独立索引，然后进行集合运算。当使用存储过程大量插入数据后： DELIMITER //CREATE PROCEDURE `proc_insert_t`()BEGIN declare i int default 1; WHILE i &lt;= 10000 DO INSERT INTO t(a, b) VALUES(i, i * 2); SET i = i + 1; END WHILE;END //DELIMITER ;call proc_insert_t();explain select * from t where a = 100 and b = 200; 就会得到以下结果： 这说明，MySQL内置的sql优化器会对select语句进行优化，当它觉得走单个索引性能比较高时会走单个索引，当它觉得两个索引取交集的性能好时，它就会触发两个索引。 如果想要人为地在某条查询语句里使用索引，可以试试 select ... from ... USE INDEX(...) where ...; // 建议优化器使用某索引select ... from ... FORCE INEDX(...) where ...; // 强行绕开优化器，强制使用某索引 什么情况下索引会失效？索引是不是百利而无一害的？不是。 索引是不是建好了就一定会被触发的？不是。 以下基于示例表t作解释： create table t( a int, b int, key (a)); 绝大多数情况下，我们说数据库索引都是指的那一棵棵的B+树，在某些情况下索引失效 = 无法遍历B+树，常见的有： 带or的条件查询 select ... from ... where a OR b;，会直接走全表扫描。 不等值查询 当where里的条件是不等值如!=或者&lt;&gt;时，没有办法把这个当做遍历B+树时比大小的依据，所以无法触发索引。 联合索引不满足最左前缀原则 已知表上建好了联合索引idx_abc (a, b, c)，但是查询的where条件里不满足最左前缀原则，例如select ... from ... where b = ? and c = ?，这样是没有办法触发上面的联合索引的。 试想一下idx_abc (a, b, c)对应的B+树结构，每个节点里存的东西都是按照三元组(a, b, c)来排序的，先有a的顺序，在a的有序性基础上才有b的顺序，在b的有序性基础上才有c的顺序。所以如果在where条件里没有“最左边”的那一列，那根本就无法正常遍历B+树，因为B+树里的每个节点都是按照先a再b再c的顺序排序的，那也就谈不上“触发索引”了。 like条件的字符串以通配符“%”开头 同上，试想一下idx_name (name)对应的B+树，每个节点里都是按字符串字典序排好的数据，用通配符%开头，遍历的时候怎么比大小？？？怎么可能正常地遍历B+树？？？？？？自然就索引失效了。 但是如果查询条件以通配符%结尾，却是可以走索引的，因为&quot;abc%&quot;在遍历B+树的时候可以简单当成&quot;abc&quot;来比大小。 where条件中用了计算表达式或者函数 同上，也是B+树遍历的问题。用表达式的结果或者函数的结果做filter的话，除非能另外建一棵新的B+树，树节点的数据按表达式的输出来排序，那样才能正常遍历。不能正常遍历B+树 = 索引失效。 MySQL优化器决定不走索引 有一些情况下，如上一篇文章提到的sex列索引，MySQL优化器未必会选择触发“性价比低”的索引。如果经过优化器判断，走secondary index再回clustered index查表将产生大量的磁盘I/O，甚至比直接全表扫描的磁盘I/O更多的话，很可能就不会走索引。 什么时候需要加索引给某一列或者某几列加索引的优点很明显：加速目标数据查询，尽量避免全表扫描，减少磁盘I/O。而不加索引在别的方面也有好处：在某张表上的索引越多，意味着维护的B+树越多，当对该表频繁地进行insert/update/delete时，意味着要频繁修改诸多B+树的数据&amp;结构，性能很可能有大幅的下降。 所以，加不加索引，什么场景需要加索引，哪些列应该加索引，也是一个很麻烦的问题。 下面简单地讲几个加索引能带来较大收益的场景。 某一列/几列经常作为条件在where中作等值过滤，且选择性很高 例如电商业务里的order表，很可能经常需要通过user_id来查询该user的订单。 create table order ( id int primary key auto_increment, user_id int, ......);select ... from order where user_id = ?; 当这张表的容量上升至千万级别，不为user_id加索引的话，每次执行上述查询语句会全表扫描千万行。如果为user_id加索引，将根据user_id建一棵B+树，层高大概为3~4层，也就是最多3~4次磁盘I/O就能定位到要找的user_id对应的页。 为user_id加索引，另外一个很重要的原因是在order表里user_id的选择性还是很不错的。所谓选择性，上一篇文章说过选择性 = unique值的数量 / 行数，虽然在order表里user_id的选择性不可能为1，但是也比sex那类二值属性的选择性要好得多，值得加索引。 某一列/几列经常需要排序 create table t ( ...... a int, b int ......);select ... from t where a = ? order by b; 为什么在这种情况下加索引idx_ab (a, b)也是不错的选择呢？其实很简单，再次回想一下idx_ab (a, b)对应的B+树的结构。在B+树里，无论是非叶子还是叶子节点，里面存的数据都是按(a, b)二元组的顺序存的。当通过where a = ?的条件遍历B+树定位到叶子节点里的a = ?的记录后，其实b属性在a = ?的记录上也已经有序了。 有了这个索引的帮助，就不需要一股脑地把数据取出来后再另外做order by的排序工作，提高了很多效率。 某一列/几列经常在select中被查询 create table t ( ...... a int, b int, c int ......);select a, b, c from t where a = ?; 如上面的表，如果经常要查询某几个固定的字段，如(a, b, c)，而非全部字段*，加一个联合索引idx_abc (a, b, c)效果应该不错。再次考虑其对应的B+树的结构，对于secondary index（辅助索引）来说，最底层叶子节点里就已经存了(a, b, c)三元组的数据了（当然还有primary key）。那走一遍secondary index（辅助索引）就能得到全部想要的字段值，不需要通过primary key又回clustered index（聚集索引）扫一遍，减少了不少磁盘I/O，提高了查询效率。 InnoDB内置的索引优化技术索引优化技术部分基本是对《MySQL技术内幕：InnoDB存储引擎》内的5.6.6和5.6.7节作概括总结，更详细的描述可以直接看书。 MRR（Multi-Range Read） MRR适用于range、ref、eq_ref类型的查询 其第一个作用是，查完secondary index（辅助索引）得到一堆大概率不连续的primary key/row ID后，将其放到缓存里，然后对其进行排序。最后根据primary key/row ID遍历clustered index（聚集索引）的B+树时，因为primary key/row ID的有序性，所以遍历过程都是顺序磁盘I/O，而非随机磁盘I/O，提高了一定的读性能 其第二个作用是，将某类型的range查询拆分为k-v对的等值查询。例如 select * from t where a &gt;= 1000 and a &lt; 2000and b = 10000; 会将以上range查询拆成多个(a=1000, b=10000)……(a=1999, b=10000)的k-v对等值查询，避免range查询中先取出大量的无用数据再作filter的性能损耗。 ICP（Index Condition Pushdown） ICP = 过滤条件下推，把where中的过滤条件下推到了存储引擎层，减少无用数据的取出。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>InnoDB</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL InnoDB索引的那些事]]></title>
    <url>%2F2019%2F02%2F05%2Fmysql-innodb-index%2F</url>
    <content type="text"><![CDATA[本文80%由本人（Haoxiang Ma）原创，20%参考其他博客，如需转载请注明出处。 本文写于2019/02/05，基于MySQL 5.7.18。至于其他版本的MySQL，如有出入请自行查阅其他资料。 索引（Index）是什么索引 = 目录，这就是我认为最直观的解释。 不妨类比一下，一本有着上千页的书 = 一张存了上亿行的数据库表。 如果想在这本书里快速翻找到某一章、某一节、甚至是某一段内容，最快的方法莫过于直接查书的目录，翻到目录里跟内容对应的那一页，再从该页的开头第一个字开始看，直到找到目标内容。 从数据库领域也莫过于此，如果我们能给一张表维护一个“目录”，在查我们想要的数据时，那就不需要从表的第一行开始检查，一直检查到该表的最后一行，极大提高了查表的效率。 讲完了大白话，我尝试用一句较为精炼严谨的话描述索引是什么： 索引 = 一种记录了数据表里数据所在位置的结构。 明显，它具有两种特性： 索引是一种数据结构，你可以试着用数组、链表、队列、栈、二叉树、图……去实现，why not？ 索引里存的是一张表里的数据所在的位置，借助索引，应该可以快速轻松地查到xx字段 = yy的数据在哪一行。 MySQL InnoDB索引类型因为最近一直在看MySQL（其实是InnoDB）的内容，所以就专门针对InnoDB里的索引做了一点研究。接下来的内容如无特殊说明，均是基于MySQL 5.7.18及该版本默认自带的InnoDB plugin。 上面说了，索引是一种存东西的数据结构，理论上可以用我们学过的任意数据结构来实现。但是不同的数据结构在实现某个需求时肯定会有不同的效率（时间复杂度），所以从数据结构的角度来看，InnoDB内置了3种数据结构来实现索引： Hash 本质上就是一张HashMap，可以想象一下key就是字段值，value就是该行的行号/primary key/存放位置offset等等 例如有一张user表： 如果为user_name列建一个Hash索引的话，大概会像： hash_index = &#123; "ii": 1, "jj": 2, "kk": 3&#125; 不过在InnoDB中，Hash索引是无法由用户自行创建的，一般讲Hash索引是指存储引擎内部在运行过程中慢慢自动为热门数据构建的索引。 全文索引 本质上就是一个倒排表（inverted index），专门为文本（长文本）类数据而设的。从MySQL 5.6开始才支持InnoDB的全文索引功能。 B+树 As its name indicates，这就是用标准的B+树实现的数据结构。默认情况下，一个disk page单位大小为16KB，为了匹配这个特性，在InnoDB的B+树索引里的一个节点也对应一个disk page，也最多存16KB的数据。 在B+树里，非叶子节点不存真正的数据，只存索引值，只有最底层的叶子节点才会存真正的数据。同时，在数据库领域为了尽可能地减少磁盘I/O，所以B+树的fan-out（阶/扇出）都会设置得比较大，意味着一个树节点可以有非常多的孩子节点，那样树的高度就不会过高。宁愿“宽胖”也不要“高瘦”，高瘦意味着从root到leaf要访问很多个中间树节点，意味着很多次磁盘I/O。 除此之外，其他的B+树基础特性都得到了保留。B+树数据结构基础可以参考以下Calcular的博客：《B+树完全解析》。 再拿user表来做例子： 假如基于id列构建B+树索引，建出来的B+树大概像： 以上从数据结构的角度介绍了3种索引类型，接下来从逻辑角度介绍InnoDB里的2种索引类型。 聚集索引（Clustered Index） 聚集索引 = 全表数据 + 基于主键大小排序构建出的B+树 所谓的“聚集”，就是全部行数据和索引数据的“聚集”，都放在了同一棵B+树里，非叶子节点存的是主键值的索引，叶子节点存的是真实的一行一行的表数据。 因为一张表只能有一个主键，所以一张表只能有一个聚集索引。 上面的user表和对应的B+树就是一个聚集索引，因为那棵B+树是基于主键的大小排序构建出来的，非叶子节点存的都是主键值的索引，叶子节点存了全部行数据。 辅佐索引（Secondary Index） 辅佐索引 = 主键数据 + 基于某N列组合排序构建出的B+树 （1 &lt;= N &lt;= 表总列数） 对于一张表来说，除了唯一一个聚集索引外，其他所有索引都称为辅佐索引。 辅佐索引的B+树里，非叶子节点存的是某N列的值的索引，叶子节点存的不再是行数据，而是该行对应的主键值！ 拿student表做例子： 假设在hobby这一列上建一个索引，毫无疑问这就是一个辅佐索引： 注意观察，最底层的叶子节点不再存完整的行数据，而是存了对应行的主键，相当于一个指向行数据的指针。 单列索引及工作流程接下来看看最简单的索引：单列索引，展开来讲就是“在单列上构建的辅佐索引”。其结构可以参考上面的student表和其对应的B+树。 那么单列索引为什么可以提高查表效率呢？以student表上的hobby索引为例，以下就是它的工作流程： 执行select * from student where hobby = &#39;swimming&#39;; 执行引擎检查发现hobby列上已建索引，决定使用该索引 遍历hobby索引的B+树，比较swimming和各中间节点上的索引值的大小，遍历过程发生多次磁盘I/O，最终定位到最右下角的节点 发现该节点对应的disk page还未加载到内存，加载该页 页加载后，从页中找到hobby = swimming的记录，读出该记录的主键为3 遍历聚集索引（Clustered Index），比较3和各中间节点上的主键索引值的大小，最终定位到某个叶子节点 发现该节点对应的disk page还未加载到内存，加载该页 页加载后，从页中找到id = 3的记录，读出该记录的所有字段值 把结果返回给客户端 从以上的步骤可以看到，利用单列辅佐索引来查数据，实际上分了两段逻辑： 遍历辅佐索引的B+树，找到想要的行的primary key 根据primary key，去遍历聚集索引的B+树，最终得到完整的行数据 联合索引及工作流程那如果不是单列索引，而是联合（多列）索引呢？ 例如student表除了hobby字段还有age字段，想联合(hobby, age)来建索引可不可以？ 答案当然是没有问题，而且先上结论：在一定情况下联合索引比单列索引性价比更高。 其对应的B+树跟之前没有太大差别，仅仅是索引比大小时用的是二元组(hobby, age)来比，而不仅仅是hobby单属性来比。 如上图，如果再插入一条(hobby=pingpong, age=20)的数据，这条数据就不会落到左下角的叶子节点，而是右下角的叶子节点。因为比大小比的是整个(hobby, age)二元组的大小，(hobby=pingpong, age=20) &gt; (hobby=pingpong, age=16)。 至于联合索引触发的工作流程，跟单列索引的逻辑几乎一样，也是分两段走，不必多说。 那为什么说在一定情况下联合索引比单列索引性价比更高？参考《mysql里创建联合索引的意义?》一文，我详细拓展了以下原因： 1个联合索引 = N个索引。如果建一个在(a, b, c)三列上的联合索引，相当于建了(a)，(a, b)，(a, b, c)三个索引。为什么呢？结合上述B+树的结构来思考，如果以三元组(a, b, c)建了B+树，B+树里的节点都按“先比a的大小，再比b的大小，最后比c的大小”的顺序排列。当执行select * from ... where a = ?和select * from ... where a = ? and b = ?时，一样能利用按(a, b, c)排好序的B+树。 如上图，已按(a, b, c)三元组的大小构建B+树。当执行select * from ... where a = 2，自然也能利用该B+树的排序性质，走到左下角的叶子节点；当执行select * from ... where a = 4 and b = 1时，在root处比较得到：(a=3, b=3, c=...) &lt; (a=4, b=1) &lt; (a=6, b=5, c=...)，所以自然也会往右边走。 综上所述，联合索引确实可以以一己之力起到N个索引的作用，还不用单独建多个独立的索引，节省磁盘空间&amp;介绍写表的开销。 有效减少回表查询的次数。还是参考以上按(a, b, c)三元组的大小构建的B+树。当执行的sql为select a, b, c from ... where ...;，联合索引的B+树的底层叶子节点其实已经包含了a, b, c的数据，不需要查出primary key再回聚集索引查询。 筛选度高，减少表遍历的行数。执行select * from ... where a = ? and b = ? and c = ?时，假如独立满足a, b, c的各有10%的数据。在联合索引的筛选下，能够筛选出N * 10% * 10% * 10%的数据，再拿它们的primary key到聚集索引查询。如果不走联合索引，只走a的单列索引，那么只能筛选出N * 10%的数据，将会拿大量的primary key回聚集索引查询，磁盘I/O次数会高非常多。 上索引会毁性能？前面讲了一大堆，貌似索引就是个纯天然24k好东西，但是世界上永远没有绝对的好与坏，在computer science里更是如此，所有东西都有它的trade-off，要看具体的问题和场景来选择方案。 来看一个在面试中经常被问到的例子：在用户表的sex列应不应该加索引？ DROP TABLE IF EXISTS people;CREATE TABLE `people` ( `id` int unsigned PRIMARY KEY AUTO_INCREMENT, `name` varchar(255) DEFAULT NULL, `sex` tinyint(1) unsigned DEFAULT NULL) ENGINE=InnoDB; 建表后，插入100w行数据，其中50w行sex=0，另外50w行sex=1。 开启MySQL的profiling，set profiling = 1;用来查看sql运行时间。 未对sex列加索引前，连续执行三次select * from people where sex = 0;，用show profiles查看执行时间，得到： Duration Query1.32345100 SELECT * FROM people WHERE sex = 01.32988600 SELECT * FROM people WHERE sex = 01.33359200 SELECT * FROM people WHERE sex = 0 单次查询的时间大概是1.3秒出头。 然后对sex列加索引，ALTER TABLE people ADD INDEX idx_sex (sex);。再连续执行三次select * from people where sex = 0;，用show profiles查看执行时间，得到： Duration Query3.74329700 SELECT * FROM people WHERE sex = 02.80777500 SELECT * FROM people WHERE sex = 02.70100800 SELECT * FROM people WHERE sex = 0 单次查询的时间大概是3秒左右，比未加索引时足足慢了一倍以上。 下面来分析一下为什么会出现这种情况，在这个场景下为什么会有如此负面的影响？ 100w数据里性别分布均匀，男女各50w。根据sex列建成B+树后，假设： B+树的fan-out足够大且单条记录所占空间极小 辅佐索引上，一个叶子节点上能装1000条记录（(sex, primary key)为一条记录） 得到该B+树的层高为2 极端情况下无任何内存缓存，全靠磁盘I/O，为了在辅佐索引读出50w条男性记录的primary key，要访问50w / 1000 = 500 pages = 500次磁盘I/O。然后拿50w个primary key去查聚集索引，聚集索引B+树的层高也为2，也就是说在极端情况下查50w次primary key要触发100w次磁盘I/O。全过程的磁盘I/O为100w + 500次。 在不加索引的情况+无缓存极端情况下，进行全表扫描。全表100w条数据，要访问100w / 1000 = 1000 pages = 1000次磁盘I/O，全过程的磁盘I/O就是1000次。 经过分析，可以看到在均匀分布的sex列上走索引，反而得不偿失，磁盘I/O数会剧增。当然，以上分析撇除了内存缓存因素，所以差距被夸大了。但是实际应用中，差距即使没有百万和一千那么大，还是比较明显的。 这个案例告诉我们：索引不要加在选择性很差的列上，也就是那些distinct值极少的列。（选择性 = #distinct / #row） 总结数据库索引是一个很大的研究方向，即使把圈子限定在MySQL InnoDB里，索引也能讲上十几篇文章。本文只是为了让大家简单入门MySQL InnoDB的索引体系，并且尽力提供一些容易让人理解的图表和例子，好让初学者不要像我当初一样满脑子浆糊，在网上搜来搜去都是那些互相复制粘贴的文章。 我觉得有兴趣、且懂c++的读者可以尝试阅读InnoDB的源码，从工程优雅性的角度而言未必是一个牛逼的项目，但是可以深入理解一些数据结构和算法的实现，之后在使用MySQL的时候会发现如鱼得水，融会贯通。 感谢以下的参考资料和博客： [InnoDB]性别字段为什么不适合加索引 by gnocuohz SQL中的where条件，在数据库中提取与应用浅析 by 何登成 mysql里创建‘联合索引’的意义？ on segmentfault]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>InnoDB</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Golang]JWT及其Golang实现]]></title>
    <url>%2F2019%2F01%2F27%2Fjwt%2F</url>
    <content type="text"><![CDATA[本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 本文写于2019/01/27，基于Go 1.11。至于其他版本的Go SDK，如有出入请自行查阅其他资料。 什么是JWTJWT，全称是JSON-WEB-TOKEN，可以理解为：在web系统中用于鉴权，基于JSON格式的一种token。 白话JWT 鉴权 所谓“鉴权”，就是服务器验证用户是不是真的有操作的权利。在web系统里，有无数的地方需要进行“鉴权”，例如在某宝上买了一堆东西，提交结算时；例如在ICBC网银上转账，输入完金额按下确认按钮时；例如在某个论坛水贴，写下一大段文字，按下发表按钮时。类似的操作都需要对操作者进行验证，看看是不是真的用户在操作，而不是其他阿猫阿狗冒认的。 不说那么宏观的，就说个人开发了几个api供别人调用，那也必须对调用者进行鉴权，不能被人分析前端源码后疯狂乱调用你的api。 所以，鉴权很重要。 JSON格式 在鉴权的时候提到JSON，其实就是用于鉴权的消息（数据）全都encode成JSON格式，方便传输和解析。 token token = 凭证 = 一个字符串。调用一些需要鉴权的api时，用户端（web页面、ios、android……）要把token传给server端，server端检查token有效后才能放行（进行下一步操作）。 JWT定义 JWT = Base64URL(Header.Payload.Signature) 可以看出，JWT就是一个很长很长的字符串，由3部分：Header、Payload、Signature组成，各部分之间用.号分隔。 Header Header里用JSON格式记录了整个token的必要的元数据，例如： &#123; "alg": "HS256", // 使用了哪种签名算法 "typ": "JWT" // 该token的类型&#125; Payload Payload里则存了鉴权所需的实际的数据，例如： &#123; "iss": "nobody", // 签发方 "exp": "2019/08/08", // 过期时间 "sub": "auth", // 主题 "aud": "kids", // 受众 "nbf": "2019/01/01 06:00:00", // 生效时间 "iat": "2019/01/01 00:00:00", // 签发时间 "jti": "13579" // 编号 ...... // 更多自定义字段&#125; 除了以上7个官方字段，还可以自定义更多字段。通常为了鉴权，可以加一个user_id或类似的字段，指明这是哪个user在请求操作。 Signature 顾名思义，Signature就是一个数字签名，使用Header里指明的签名算法，对Header和Payload签名，以防止第三方对其进行篡改。 secret_key = "......"str = base64URL(header) + "." + base64URL(payload)signature = xx_algorithm(str, secret_key) 算出Signature后，即可将三者拼凑在一起，用.号分隔，作为一个JWT的整体返回。 需要注意的点 在JWT生成时，很多地方用到了Base64URL，这里要特别提2个细节： Base64URL就是普通Base64算法的改良版，为了适应url里的特殊字符，把=省略，把+替换为-，把/替换为_，其他一致。 Base64URL本质上并不是加密算法，只是一种编码方式，是可逆的，很容易能把base64字符串还原成原字符串。 JWT的优点与缺点web系统里实现鉴权的传统手法是session-cookie机制。如下图所示： 根据上图，传统session-cookie机制也可以很好地完成鉴权工作，但是最大的问题在于扩展和维护。想象一下，当业务量剧增，后端不可能只部署一台server，必然要把api服务部署在N台物理server上。那么N台server必然就要共享session数据，那就要部署一个第三方存储组件（例如Redis）来存session数据。可是有可能单个Redis实例也存不下，那就要弄成Redis集群，或者别的存储集群，那就又涉及HA和一致性问题。。。 所以，session-cookie机制不是不行，而是在拓展和维护上比较麻烦。 那用JWT的话，会更容易拓展和维护吗？ 貌似还真的可以，server端不需要保存session数据，一切数据相当于“寄存”到了client端。server只起到2个作用： 用户最初登录时，校验用户名与密码，颁发JWT 用户之后每次请求时带上JWT，server进行简单校验 但是，世界上没有100%完美的机制，一切都要有trade-off，JWT也有它的缺点。由于它把一切数据“寄存”到了client端，所以server端无法控制token过期，失效等操作。一旦把JWT颁发出去，只能等到JWT里定义的exp到期，才能执行失效操作，而传统的session-cookie机制则可以主动在server端把session删了或者置为过期。 虽然原生的JWT不能主动控制token过期，我们还是可以在工程上额外实现过期机制的，例如在server端维护一个黑名单，黑名单记录了需要“被”失效的client_id或者user_name，当server收到用户请求，从其JWT拿到user_name后应和黑名单里的user_name比较，如果在黑名单中，就拒绝请求，达到“过期”、“失效”的效果。 有人会问，这样做来实现JWT主动过期，不是又走回session-cookie机制的老路吗？server端还是要保存数据，还不如直接用session-cookie机制呢。当然不是。在session-cookie机制中，server端要为每一个发起过连接的用户（在一定时间内）保存session数据，但是JWT黑名单只需要为一小部分用户保存简单的用户名，总不能说你的系统里绝大部分用户都在黑名单里吧？？？ 所以，采用session-cookie还是JWT并没有绝对的答案，全都取决于业务场景、用户数量、可维护性。 Golang实现要实现JWT，比较麻烦的是签名算法，还好目前Golang已经有好一些现成的JWT库可供调用，下面来看看用github.com/dgrijalva/jwt-go实现的JWT机制，该库的源码请参考 github.com/dgrijalva/jwt-go。 // JWT Payload结构type Claims struct &#123; Username string `json:"username"` Password string `json:"password"` jwt.StandardClaims&#125;// 生成JWTfunc GenerateToken(username string, password string) (string, error)&#123; expireAt := time.Now().Add(30 * time.Minute) issuedBy := "nobody" secret := "secret" // 定义Payload claim := Claims&#123; username, password, jwt.StandardClaims&#123; ExpiresAt: expireAt.Unix(), Issuer: issuedBy, &#125;, &#125; // 定义签名算法, 签名, 生成JWT token := jwt.NewWithClaims(jwt.SigningMethodHS256, claim) ss, err := token.SignedString([]byte(secret)) return ss, err&#125;// 解析JWTfunc ParseToken(ss string) (*Claims, error) &#123; secret := "secret" // 解析Payload token, err := jwt.ParseWithClaims(ss, &amp;Claims&#123;&#125;, func(token *jwt.Token) (interface&#123;&#125;, error) &#123; return []byte(secret), nil &#125;) // 验证Payload if err == nil &amp;&amp; token != nil &#123; if claim, ok := token.Claims.(*Claims); ok &amp;&amp; token.Valid &#123; return claim, nil &#125; &#125; return nil, err&#125; Others 原生JWT只是用Base64URL编码了一下，自行实现时可以用非对称的加密算法加密。 和session-cookie机制一样，JWT的有效时间不应该设置太长。 为了防止CSRF攻击，如果把JWT存在cookie中，进行POST/PUT/DELETE请求时，还应该带上CSRF-TOKEN进行多重验证。]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Golang]探索包初始化(init)]]></title>
    <url>%2F2019%2F01%2F21%2Fgo-package-init%2F</url>
    <content type="text"><![CDATA[本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 本文写于2019/01/21，基于Go 1.11。至于其他版本的Go SDK，如有出入请自行查阅其他资料。 Overview在Java中，我们有包（package）、类、类变量、全局变量等概念，对JVM有了解的同学会对各种变量的内存分配位置和生命周期等特性了如指掌。例如，当类被JVM加载时，类的各种信息&amp;类变量都会进入JVM内存区域，类变量（static）就会被分配在方法区，并且只要该类没有被gc，类变量也会一直存在。 然而，在Golang里面，我们有包（package），但却没有了很多Java里的“类特性”，不存在什么类加载，而且还多了一些语言特性。所以在本文中我想探讨一下Golang自己的特性——包初始化。 初始化顺序在Java中，当谈及“全局变量”、初始化、main方法，永远都离不开一个载体——类（class），一切一切都是写在类的内部，从类加载而生，由类gc而死，对象从new（或者反射）而生，由gc而死。 可是Golang里对“类”概念的依赖被极大地削弱：这里也有“全局变量”，可以脱离类存在，可以直接定义在包这个level上；这里也有“初始化”，不过是包level上的初始化（func init()）；这里也有“main方法”，并不需要定义在“类”的内部。 那么问题就来了，全局变量的初始化，包的初始化，main方法的调用，哪个先被执行？？？ 在各种不同的情况下，它们的执行顺序会不会发生变化？？？ 包变量初始化、init()、main()执行顺序 package mainimport "fmt"var V = prepareV()func init() &#123; fmt.Println("calling main.init()")&#125;func prepareV() int &#123; fmt.Println("calling prepareV()") return 10&#125;func main() &#123; fmt.Println("calling main()")&#125; 上述代码里，在同一个mainpackage中有全局变量（包变量）V，包的初始化方法init()，以及main()方法。 执行结果是 calling prepareV() // 包变量初始化calling main.init() // 调用init，包初始化calling main() // 执行main方法 显而易见，包变量 -&gt; 包init -&gt; main方法 同一个package内，多个init()的执行顺序 很多时候，在一个package中，不可能只有一个代码文件。例如名为utils的package里肯定有着N多个不同的代码文件，实现了不同的模块。如果N个模块都有自己的init()方法，虽然从package的层次来看，它们都属于同一个package，但毕竟代码执行起来肯定是有前后顺序的，那么这N个init()的执行顺序会是怎样的呢？ a.go package mainimport "fmt"var a = prepareA()func init() &#123; fmt.Println("calling a.init()")&#125;func prepareA() int &#123; fmt.Println("calling a.prepareA()") return 10&#125; d.go package mainimport "fmt"var d = prepareD()func init() &#123; fmt.Println("calling d.init()")&#125;func prepareD() int &#123; fmt.Println("calling d.prepareD()") return 20&#125; x.go package mainimport "fmt"var x = prepareX()func init() &#123; fmt.Println("calling x.init()")&#125;func prepareX() int &#123; fmt.Println("calling x.prepareX()") return 30&#125; main.go package mainimport "fmt"func main() &#123; fmt.Println("calling main()")&#125; 上述代码里有1个mainpackage，该package下有4个go代码文件：a.go，d.go，x.go，main.go，且每个代码文件里都有一个init()方法。 程序的输出如下： calling a.prepareA() // a.go里的A变量初始化calling d.prepareD() // d.go里的D变量初始化calling x.prepareX() // x.go里的X变量初始化calling a.init() // a.go里的init()调用calling d.init() // d.go里的init()调用calling x.init() // x.go里的init()调用calling main() // main方法执行 由此可见，在遵循包变量 -&gt; 包init -&gt; main方法顺序的基础上，会严格按照代码文件名的字典序，从小到大执行。 多个不同package的初始化顺序 上面讨论了同一个package内多个不同代码文件中的init()顺序，接下来看看多个不同package的初始化顺序。 先说明一个基本结论：优先初始化依赖项（import项）最少的package。 简单依赖 整个程序有3个package：package a, package b, package main。其中main中import了a中的变量，a中import了b中的变量，形成了一条依赖链。 b.go package bimport "fmt"var B = prepareB()func init() &#123; fmt.Println("calling b.init()")&#125;func prepareB() int &#123; fmt.Println("calling b.prepareB()") return 20&#125; a.go package aimport ( "analyze_init/case_three/b" "fmt")var A = prepareA()func init() &#123; fmt.Println("calling a.init()")&#125;func prepareA() int &#123; fmt.Printf("calling a.prepareA(), reading b.B = %d\n", b.B) return 10&#125; main.go package mainimport ( "analyze_init/case_three/a" "fmt")func main() &#123; fmt.Printf("calling main(), reading a.A = %d\n", a.A)&#125; 上述代码输出结果为： calling b.prepareB()calling b.init()calling a.prepareA(), reading b.B = 20calling a.init()calling main(), reading a.A = 10 显然就是根据import的路径链，从上到下进行初始化的。也就是说当有多个不同package时，会先初始化依赖项最少的，在这里就是package b，因为b没有import任何其他的自定义package。 多个无依赖项的package 那如果整个程序里有多个package都无依赖项呢？如下图所示，package b和package c都无依赖项，会先初始化哪个？ b.go package bimport "fmt"var B = prepareB()func init() &#123; fmt.Println("calling b.init()")&#125;func prepareB() int &#123; fmt.Println("calling b.prepareB()") return 20&#125; a.go package aimport ( "analyze_init/case_four/c" "fmt")var A = prepareA()func init() &#123; fmt.Println("calling a.init()")&#125;func prepareA() int &#123; fmt.Printf("calling a.prepareA(), reading c.C = %d\n", c.C) return 10&#125; c.go package cimport "fmt"var C = prepareC()func init() &#123; fmt.Println("calling c.init()")&#125;func prepareC() int &#123; fmt.Println("calling c.prepareC()") return 30&#125; main.go package mainimport ( "analyze_init/case_four/a" "analyze_init/case_four/b" "fmt")func main() &#123; fmt.Printf("calling main(), reading a.A = %d, reading b.B = %d\n", a.A, b.B)&#125; 上述代码的输出结果为： calling c.prepareC() // 初始化ccalling c.init()calling a.prepareA(), reading c.C = 30 // 初始化acalling a.init()calling b.prepareB() // 初始化bcalling b.init()calling main(), reading a.A = 10, reading b.B = 20 从这个输出结果，可以看出两个问题： 为什么先初始化package c，而不是package b？b和c均无依赖项，且从名字大小而言，b字母的字典序还比c小，为什么不是先初始化package b？ 初始化完package c后，为什么不是初始化package b？ 对于第一个问题，答案就是：在package初始化时，如果有多个package都无依赖项，会优先选择依赖路径最长的那个package开始。如上图main-&gt;a-&gt;c的路径是长于main-&gt;b的，所以会优先初始化package c，而非package b。 对于第二个问题，为什么package c之后不是package b，而是package a。因为它是参照DFS的逻辑进行初始化的，c搞定之后就会去初始化c的邻接节点，直到没有依赖项为0的节点为止。所以package c结束后，边a-&gt;c消失，此时发现package a也无依赖项，就继续初始化package a，然后边main-&gt;a消失，可是main还依赖于package b，所以要先初始化package b。 多package依赖路径长度相等时 在上面的例子里两条依赖路径长度不等，所以会先初始化路径长的package。 那如果两个不同的package，从main出发到达的依赖路径长度完全相等呢？ a.go package aimport "fmt"var A = prepareA()func init() &#123; fmt.Println("calling a.init()")&#125;func prepareA() int &#123; fmt.Println("calling a.PrepareA()") return 10&#125; b.go package bimport "fmt"var B = prepareB()func init() &#123; fmt.Println("calling b.init()")&#125;func prepareB() int &#123; fmt.Println("calling b.PrepareB()") return 20&#125; main.go package mainimport ( "analyze_init/case_five/a" "analyze_init/case_five/b" "fmt")func main() &#123; fmt.Printf("calling main(), reading a.A = %d, reading b.B = %d\n", a.A, b.B)&#125; 代码的执行结果为： calling a.PrepareA() // 初始化acalling a.init()calling b.PrepareB() // 初始化bcalling b.init()calling main(), reading a.A = 10, reading b.B = 20 显然，当依赖路径的长度相等时，会按照包名的字典序，从小到大进行初始化。 总结在Golang里，“全局变量”的初始化和初始化方法init()的调用都跟package初始化密切相关。通过以上的多个例子，可以看出package初始化的几条规则： package初始化是以package为单位进行的 单个package的初始化，会按照全局变量 -&gt; init() -&gt; 其他的顺序执行 单个package中有多个init()的话，会按照init()所在的代码文件名，从小到大顺序初始化 多个package时，要先找到无对外依赖项的package，而不是单纯地按照文件名排序进行初始化 多个package时，还要参考从main到该package的依赖路径长度，路径长的优先初始化，后续按照DFS的逻辑进行 多个package时，若两条路径长度相等，就简单地考虑包名的字典序就好了 总而言之，关键词就这几个： 名称字典序 依赖项 依赖路径 DFS]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[MIT 6.824]Lab1笔记]]></title>
    <url>%2F2019%2F01%2F03%2Fmit6824-lab1%2F</url>
    <content type="text"><![CDATA[本文100%内容为本人（Haoxiang Ma）原创，转载请标明出处。 Overview前几天开始自学MIT的分布式神课，大名鼎鼎的MIT 6.824。由于网上只搜得到2015 Spring的课程视频（youtube），感觉时间有点久远，就不看视频直接看materials了。 （如果找得到2017或者2018的就好了，然而人家MIT也没有任何义务把自己的核心课免费公开= =，能把materials全部免费公开就真的已经很慷慨了） 在Day 1，主要的内容还是热热身，先入分布式计算的门槛。所以要求读map-reduce的论文（MapReduce: Simplified Data Processing on Large Clusters），然后基于已有的代码和在paper中读到的理论来解决以下5个子问题 实现doMap()和doReduce() 实现wordcount 实现多worker并发的map reduce mr过程中rpc失败处理 利用mr实现倒排索引 Sub-problems 实现doMap()和doReduce() doMap()和doReduce()其实就是map job和reduce job的高层封装。 先来看看doMap()。 func doMap( jobName string, // mr job名称 mapTask int, // map任务id inFile string, // 该map任务对应的输入文件名 nReduce int, // reducer数量 mapF func(filename string, contents string) []KeyValue, // map函数) &#123; // 把该map任务对应的文件内容全部读出来 contents, err := ioutil.ReadFile(inFile) if err != nil &#123; panic(err) &#125; // 建一个map, 记录file name和要写入该file的所有k-v对 intermediateMap := make(map[string]*list.List) // 调用map函数, 得到文件内容中的所有k-v对 kvs := mapF(inFile, string(contents)) // 为每个k-v对算出它对应的reducer id, 用map记录一下 for _, kv := range kvs &#123; reducer := ihash(kv.Key) % nReduce intermediateFileName := reduceName(jobName, mapTask, reducer) if currentList, ok := intermediateMap[intermediateFileName]; ok &#123; currentList.PushBack(kv) &#125; else &#123; currentList = list.New() currentList.PushBack(kv) intermediateMap[intermediateFileName] = currentList &#125; &#125; // 写中间文件，把文件对应的所有k-v对都写到该文件中 for fileName, kvList := range intermediateMap &#123; file, err := os.OpenFile(fileName, os.O_CREATE | os.O_WRONLY, 0777) if err != nil &#123; panic(err) &#125; encoder := json.NewEncoder(file) for element := kvList.Front(); element != nil; element = element.Next() &#123; encoder.Encode(element.Value) &#125; file.Close() &#125;&#125; 用Java裸写过Hadoop mr程序的要实现doMap()应该很轻松，但是在实现的时候估计都会有点别扭。因为在写Hadoop mr的时候，只需要自己实现map函数，而且map函数接收的参数默认就是一行内容（或者某种InputFormat的一个单位）。 而在这里我们要实现更高层的抽象，要把map函数的外层逻辑都实现好，调用map函数只是整个逻辑中的一小步。 下面再来看看doReduce()。 func doReduce( jobName string, // 任务名称 reduceTask int, // reduce任务id outFile string, // 输出到哪个文件 nMap int, // map任务数 reduceF func(key string, values []string) string, // reduce函数) &#123; reduceMap := make(map[string][]string) // 先从各个mapper输出的中间文件中读出属于本reducer的k-v对 for i := 0;i &lt; nMap;i++ &#123; fileName := reduceName(jobName, i, reduceTask) file, err := os.Open(fileName) if err != nil &#123; panic(err) &#125; reader := bufio.NewReader(file) for &#123; // 逐行读取, 一行一个k-v对 line, err := reader.ReadString('\n') if err != nil &#123; break &#125; kv := KeyValue&#123;&#125; json.Unmarshal([]byte(line), &amp;kv) key := kv.Key value := kv.Value // 通过key来聚集, group by key if values, ok := reduceMap[key]; ok &#123; reduceMap[key] = append(values, value) &#125; else &#123; reduceMap[key] = make([]string, 0, 0) reduceMap[key] = append(reduceMap[key], value) &#125; &#125; file.Close() &#125; // 根据key来升序排序 keys := make([]string, 0, len(reduceMap)) for k := range reduceMap &#123; keys = append(keys, k) &#125; sort.Slice(keys, func(i, j int) bool &#123; return keys[i] &lt; keys[j] &#125;) // 对于每个key, 调用reduce函数得到reduce后的结果, 写入结果文件 out, err := os.OpenFile(outFile, os.O_CREATE | os.O_RDWR, 0777) defer out.Close() if err != nil &#123; panic(err) &#125; encoder := json.NewEncoder(out) for _, key := range keys &#123; reduceResult := reduceF(key, reduceMap[key]) encoder.Encode(KeyValue&#123;Key: key, Value:reduceResult&#125;) &#125;&#125; 看上述代码，doReduce()里的逻辑也非常清晰。主要的点在于mr模型中的key的有序性，本来在map和reduce之间会存在shuffle过程，在shuffle中会先对key进行排序，再broadcast到reducer端。但是在lab 1中没有严格定义一个shuffle过程，所以要在reducer端对key进行排序，再reduce，最后才根据key的升序来处理+输出。 实现wordcount 实现了doMap()和doReduce()后，说明框架性的map和reduce的入口已经写好了，那么就来实现第一个最简单的mr应用：wordcount。 根据lab 1的map函数定义，map函数的入参是一个文件的#全部内容#，而不是Hadoop mr中map函数的#一行内容#，所以调用一次map函数，就得把某文件中的全部k-v对都生成并返回。 func mapF(filename string, contents string) []mapreduce.KeyValue &#123; kvs := make([]mapreduce.KeyValue, 0, 0) // 用FieldsFunc来进行分词, 剔除非letter的内容 terms := strings.FieldsFunc(contents, func(r rune) bool &#123; return !unicode.IsLetter(r) &#125;) // 要实现wordcount, 每个词要map成一个&lt;word, 1&gt;的二元组 for _, term := range terms &#123; kvs = append(kvs, mapreduce.KeyValue&#123;Key: term, Value: "1"&#125;) &#125; return kvs&#125; 在mapF()里，一开始我是傻乎乎地用strings.Split()来分词的，后来发现Split分词的逻辑太死板，只能根据某个特定的字符串来分割，不太符合需求。一番查阅才发现了strings.FieldsFunc()也可以实现分词，只需要传入一个匿名判断方法，告诉它要过滤哪些类型的字符就行了。 至于reduce，逻辑更简单，入参是一个key和该key对应的所有value组成的slice。为了实现wordcount，只需要简单地输出某key有多少个对应的values就行，也就是len(values)。 func reduceF(key string, values []string) string &#123; return strconv.Itoa(len(values))&#125; 实现多worker并发的map reduce lab 1里mr的运行模式有两种 Sequential（串行，多个map任务串行执行，然后多个reduce任务串行执行） Distributed（并发，多个map任务并发，然后多个reduce任务并发） 为了实现并发执行，我们需要完成schedule.go里的schedule()方法。在这个框架里，所谓的并发执行（Distributed），就是假设你有一个集群，里面有多台worker，作为调度者不需要亲自去逐个串行执行map或者reduce任务，只需要并发地把任务“扔”给空闲的worker，让worker去干活，它完成后会通知调度者。 这么并发执行的好处就是类异步机制，调度者不用串行等待，只管分配。当调度者收到了N个worker的完成回复，代表本次任务全部完成，即可结束。 接下来看看描述此逻辑的伪代码。 // assign tasks to workersforeach task &#123; t = new thread worker = t.get_available_worker() t.call_rpc_to_assign_task(worker, task) t.notify_master_success_or_fail()&#125;master.wait_for_all_task_completion() *在golang中，thread的作用可以用goroutine替代。 并发mr过程中rpc失败处理 在并发mr过程中，由于采用了master - worker这样的架构，所以master必然是通过rpc来给worker分配任务。但是由于 网络异常导致失败 worker宕机 worker资源不足导致超时 worker代码逻辑出错 等种种原因，在分配任务时往往不是一次rpc就能顺利完成的，所以就需要对rpc的结果进行失败处理。在我的实现中策略是无限重复，当rpc失败后，再尝试从已注册的worker队列中取出另一个worker，进行重试，直到成功。 当然还可以设计更复杂的策略，例如专门安排一个计算节点来出错处理，当某个rpc失败次数超过M次后，由特定计算节点接手，执行任务；或者master记录哪些节点成功次数多，当某个rpc失败次数超过M次后就把任务分发到高频节点上执行。 直接看我实现的代码。 func schedule(jobName string, mapFiles []string, nReduce int, phase jobPhase, registerChan chan string) &#123; var ntasks int var n_other int // number of inputs (for reduce) or outputs (for map) switch phase &#123; case mapPhase: ntasks = len(mapFiles) n_other = nReduce case reducePhase: ntasks = nReduce n_other = len(mapFiles) &#125; fmt.Printf("Schedule: %v %v tasks (%d I/Os)\n", ntasks, phase, n_other) // 用WaitGroup让主线程在最后阻塞, wait_for_all_tasks_completion wg := sync.WaitGroup&#123;&#125; wg.Add(ntasks) // 把每一个task, 都分发给一个空闲的worker for i := 0;i &lt; ntasks;i++ &#123; // 使用goroutine并发 go func(taskId int) &#123; // 设置task参数 args := DoTaskArgs&#123;JobName: jobName, Phase: phase, TaskNumber: taskId, NumOtherPhase: n_other&#125; if phase == mapPhase &#123; args.File = mapFiles[taskId] &#125; done := make(chan bool, 1) // 无限循环, 直到本task被worker回复完成 for &#123; // 尝试从已注册的worker队列中取worker select &#123; case workerAddr := &lt;- registerChan: if call(workerAddr, "Worker.DoTask", args, nil) == true &#123; wg.Done() done &lt;- true &#125; registerChan &lt;- workerAddr // 把worker归还队列 default: &#125; select &#123; case &lt;- done: return // 直到本task被回复完成才离开循环 default: fmt.Printf("waiting for the task-%d being done\n", taskId) &#125; &#125; &#125;(i) &#125; wg.Wait() // 此处发生阻塞, 只有N个goroutine全部完成才解除阻塞 fmt.Printf("Schedule: %v done\n", phase)&#125; 在上述代码中，无限for loop + select case语句在读写channel的场景中十分常见！在第一个select case中，如果能够从worker队列里成功取出worker就发rpc，如果取不出，就无限循环尝试去取。 至于第二个select case，它的作用就是判断rpc是否成功，从而判断是否跳出无限循环。如果rpc成功，done里就会有一条新消息，select case语句自然就能执行到return跳出循环；如果rpc不成功，只会打印一句log，然后继续无限循环。 在解决出错处理的问题时，发现有一个很奇怪的地方，如果rpc返回false，就把worker归还到队列里去，某个任务会卡死在这个地方。后来检查调用schedule()的地方，也就是Distributed()里，发现worker队列是一个#阻塞（非缓冲）channel#，ch := make(chan string)。⚠️当某个goroutine尝试把worker归还，却没有别的goroutine从channel取该worker的时候，尝试归还的goroutine会卡死。这样的设计感觉也是不合理的，存放worker的队列不应该是非缓冲的，那样最后一个归还worker的goroutine永远都会被卡死。 所以，我决定修改Distributed()里的channel的定义，把非缓冲channel改成缓冲channel，以解决此问题。 利用mr实现倒排索引 最后一个小题，用mr实现倒排索引（Inverted Index），主要就是自行实现mapF()和reduceF()里的逻辑，生成一个倒排索引的output文件。 （至于Hadoop mr实现的倒排索引，可以参考我之前的文章《谈谈倒排索引，升级版“WordCount”》） 具体的逻辑就是：从文件中读出全部内容，进行分词得到N个独立的word；然后把每个独立的wordmap成&lt;word, file_name&gt;的k-v对；在reducer中把同一个word的多个file_name集到一起，输出，搞定~ // slice去重func removeDuplicate(terms []string) []string &#123; dict := make(map[string]struct&#123;&#125;) result := make([]string, 0, len(terms)) for _, term := range terms &#123; if _, ok := dict[term]; !ok &#123; dict[term] = struct&#123;&#125;&#123;&#125; result = append(result, term) &#125; &#125; return result&#125;// map函数func mapF(document string, value string) (res []mapreduce.KeyValue) &#123; terms := strings.FieldsFunc(value, func(r rune) bool &#123; return !unicode.IsLetter(r) &#125;) // *关键, 对单词进行去重 terms = removeDuplicate(terms) result := make([]mapreduce.KeyValue, 0, len(terms)) for _, term := range terms &#123; result = append(result, mapreduce.KeyValue&#123;Key: term, Value: document&#125;) &#125; return result&#125;// reduce函数func reduceF(key string, values []string) string &#123; // *关键, 对单词进行去重 values = removeDuplicate(values) numDoc := len(values) docs := "" for _, value := range values &#123; docs += fmt.Sprintf("%s,", value) &#125; return fmt.Sprintf("%d %s", numDoc, docs[:len(docs) - 1])&#125; 此处值得一提的就是要对单词进行去重操作，同一篇文章里的多个相同的单词，只应该生成一个&lt;word, file_name&gt;的k-v对，不然就会造成冗余计数。 而golang里貌似又没有提供对slice去重的标准库方法，所以只能自己实现一个去重函数，利用map的key的不可重复性进行去重，使用map[string]struct{}是因为空struct不占任何字节，节省内存空间。 总结可能因为之前有深入研究过map reduce的模型，所以Lab 1感觉没有太大的难点，看来阅读原版的map reduce论文还是很有必要的。为数不多的问题主要出现在golang的一些特性上，例如 slice去重 指针和值的区别 非缓冲（阻塞）channel的使用 另外就是先要大致看懂已提供的代码里的框架逻辑，再作修改，那样debug起来也比较快。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>bigdata</tag>
        <tag>hadoop</tag>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为2019年立下的flag]]></title>
    <url>%2F2018%2F12%2F31%2F2019-flags%2F</url>
    <content type="text"><![CDATA[2018遥想2017/12/31晚上，当时还是我室友喊我去了Kerry Park跨年看烟花什么的。在萧瑟的寒风中，两三度的温度下，🎆🎆🎆绽放的一瞬间，默默地许了一个愿，希望自己的flag(s)都能不辱使命地完成。 现在2018马上就过去了，想想2017年给自己立下的flag(s)，好像也大概完成了个80%，感觉自己的自驱力确实还凑合，或者说兴趣 or 成就感驱动确实是有用的。 当然主要是在🎆🎆🎆中许愿起的作用吧！！！ 2019——也是一条有梦想的咸鱼🐟至于即将到来的2019年，既然从2018开始用心维护我这个没有人看的博客，那就恬不知耻地直接把flag(s)以blog post的形式公诸于世好了。为了 给自己更强烈的心理助推，都公开了必须要尽全力做到吧，不然要被笑死了🤣 万一日后某天有幸成为了一名伪大佬，看看当年立下的flag(s)应该会很有意思🤣 或者日后某天彻底gg成为了一名油腻+愤世嫉俗+碌碌无为的loser，也能感慨一下自己当年也是个有梦想的咸鱼🤣 立下flags 技术篇 积极follow几大computer system领域的顶会，争取认真研读+总结&gt;=4篇核心paper 读完《ceph设计原理与实现》 读完《Linux Kernel Development》 重读一遍《MySQL InnoDB存储引擎》 产出原创技术博客&gt;=18篇 简单粗暴，&gt;=70%的天数有代码提交记录 全年自学时间&gt;=365h 爱好篇 买一台单反 or 微单 产出&gt;=50张自己满意的静态照片 产出&gt;=3个自己满意的vlog 去&gt;=2个从来没有去过的城市旅游 学会final cut pro或者pr的简单操作，学会自己剪简单的视频 等到2019/12/31再来挖坟🤣，看看能给自己打多少分，希望起码及格（60%）吧🤣。]]></content>
      <categories>
        <category>目标</category>
      </categories>
      <tags>
        <tag>目标</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Raft]Log Replication笔记]]></title>
    <url>%2F2018%2F12%2F17%2Fraft-log-replication%2F</url>
    <content type="text"><![CDATA[OverviewRaft中的核心步骤是 选主 日志复制 在上一篇《Raft选主笔记》中，相信已经清晰说明了Raft中选主的多个限制。通过一系列严格的筛选后，被选出来的Raft Leader就能接收来自客户端的请求，进行各种数据处理。 在本篇笔记中，主要打算记录日志复制（Log Replication）中的一些知识点。 数据结构既然说到了“日志复制”，那么先来看看复制过程中用到的各种数据结构。 在基础的Raft中，只有两种RPC请求： RequestVote RPC —— 由Candidate主动发出，请求大家进行投票 AppendEntries RPC —— 由Leader主动发出，请求Followers进行日志复制 根据Raft论文里的原图，我们可以总结出以下的数据结构。 // RequestVote RPCtype RequestVote struct &#123; TermId int64 // candidate的term id LeaderId int64 // candidate的leader id LastLogIndex int64 // candidate存的最后一条log entry的下标 LastLogTerm int64 // candidate存的最后一条log entry的term id&#125; // AppendEntries RPCtype AppendEntries struct &#123; TermId int64 // leader的term id LeaderId int64 // leader id PrevLogIndex int64 // 当前发送entries的前一条entry的下标 PrevLogTerm int64 // 当前发送entries的前一条entry的term LastCommittedIndex int64 // leader上最后一条commit的entry的下标 Entries []LogEntry // 当前要发送的1或N条log entry&#125; 除了两种RPC请求外，每台server（无论是Leader还是Follower还是Candidate）上都肯定要存一些系统变量，用来记录自己或者系统的状态。 type ServerState struct &#123; CurrentTerm int64 // 本机上的当前term id VoteFor int64 // 本轮投票投给了谁（该机器的id） LogEntries []LogEntry // 本机上存的log entries LastCommittedIndex int64 // 本机最后一条commit的log entry的下标 LastAppliedIndex int64 // 本机最后一条被状态机执行的log entry的下标 NextIndex []int64 // *leader特有，为每一台follower维护一个下次要发送的log entry的下标 MatchIndex []int64 // *leader特有，为每一台follower维护一个已经成功匹配上的log entry的下标&#125; Log Replication过程 当集群接收到来自客户端的存储请求 客户端发给Follower，Follower转发给Leader？或者告诉客户端请它发给Leader？ 客户端直接发给Leader，Leader接收请求 Leader自己先把请求里的数据落盘，存在本地，更新自身的meta data Leader发起AppendEntries RPC，请求集群中的其他Follower进行日志复制 每台Follower都接收到AppendEntries RPC，检查RPC请求里的各项参数，以确定是否接受该请求 比较AppendEntriesRPC.TermId与Follower.CurrentTerm // 只有当RPC请求里的Term*不小于*当前Follower的Term，才能接受if AppendEntriesRPC.TermId &lt; Follower.CurrentTerm &#123; IgnoreRPC()&#125; else if AppendEntriesRPC.TermId &gt; Follower.CurrentTerm &#123; UpdateCurrentTerm()&#125;AcceptRPC() 比较完Term后，若成功接受，则要处理PrevLogIndex和PrevLogTerm了。为什么要处理这两个参数呢？因为要通过它们来保证：在本次AppendEntries之前的所有log entry都是主从之间正确同步的。 index := AppendEntriesRPC.PrevLogIndexterm := AppendEntriesRPC.PrevLogTermif Follower.LogEntries[index].Term != term &#123; StopAndNotifyLeader() // 告知Leader双方达成一致的位置&#125; else &#123; AcceptLogEntries() // 已经达成一致，接受所有log entry&#125; 可以看出，当Leader的历史log entry和Follower的历史log entry出现不一致时，是不能进行日志复制的，完成一次日志复制的前提是：在本次日志复制之前的所有log entry都必须是一致的。 所以当发生了不一致时，应该停止日志复制，并把本机上最后一个term == AppendEntriesRPC.PrevLogTerm的log entry index告知Leader，好让Leader下次重新发起AppendEntriesRPC时能够定位到大家都共同一致的位置。 Leader重新定位PrevLogIndex和PrevLogTerm后 / 或者无需调整本身就是一致的情况下，Follower就接受该AppendEntriesRPC，把AppendEntriesRPC.Entries落盘到本机上，回复Leader已成功落盘 Leader为每个Follower维护一些落盘成功与否的变量，当Leader通过Follower的回复监测到某条log entry已被过半的Follower落盘，即可更新Leader自身的LastCommittedIndex，待下次AppendEntriesRPC发送时告诉各个Follower要commit哪一条log entry // 例如可以用一个map类容器实现这个逻辑const ( SERVER_COUNT = 5)var committedMap = make(map[int64]int64)// ......followerId := AppendEntrisResponse.FollowerIdsavedLogIndices := AppendEntrisResponse.savedIndicesfor _, savedIndex := range savedLogIndices &#123; committedMap[savedIndex] += 1 if committedMap[savedIndex] &gt;= SERVER_COUNT / 2 &#123; Leader.LastCommittedIndex = savedIndex &#125; // 更新NextIndex和MatchIndex数组 Leader.NextIndex[followerId] = savedIndex + 1 Leader.MatchIndex[followerId] = savedIndex&#125; 待下次AppendEntriesRPC时，Leader把最新的LastCommittedIndex告知Follower，Follower就可以在本地的状态机上执行LastCommittedIndex之前的log entry，然后回复Leader哪些log entry已被成功执行，Leader对应更新MatchIndex数组 NextIndex &amp; MatchIndex在ServerState中大部分变量都很好理解，只有2个变量——NextIndex和MatchIndex是比较奇怪的，因为它们都是数组类型的变量，而且是Leader特有的变量。 NextIndex：为每个Follower记录了将要发给该Follower的下一条log entry的index（初始值为Leader.LastLogIndex + 1） MatchIndex：为每个Follower记录了该Follower上已经成功匹配的log entry的index（初始值为0） 乍一看上去，正常情况下MatchIndex[i]貌似一直等于NextIndex[i] - 1。但总是会有特殊情况的，假如某台Follower的机子在某一时刻宕机了几十秒，在这几十秒里也经历了Leader的更迭，那么MatchIndex[i]就很有可能不等于NextIndex[i] - 1。在这种情况下，Leader要先根据MatchIndex[i]确定最后一个正确同步的位置，然后重新设置NextIndex[i] = MatchIndex[i] + 1，重新进行历史log entry的同步。]]></content>
      <categories>
        <category>分布式</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>算法</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Raft]Leader Election(选主)笔记]]></title>
    <url>%2F2018%2F12%2F09%2Fraft-leader-election%2F</url>
    <content type="text"><![CDATA[Raft采用Leader - Follower的架构，主要的工作都由Leader来主动调度和完成，所以选出一个合法的Leader是首要任务。 为了保证不丢数据（特指已经commit且成功通知client的数据），Raft对Leader有严格的要求，不是集群里随便一台服务器都能够当上Leader，这种“严格的要求”大多体现在选主过程中。 选主过程最重要的限制是： Leader必须存有所有已经commit过的log entry，这样才能保证整个分布式系统的一致性。 在外部用户看来，已经commit成功的就必然是100%正常存储好了的。如果选出一个Leader，结果它上面缺了几条已经commit过的数据，那这个分布式系统就没有任何意义了。几分钟前跟用户说“我已经commit了，你放心吧”，过一会用户想查一查数据却发现“尼玛，怎么丢数据了，说好的已经commit了呢？！” 为了满足这个条件（Leader上必须存有所有已经commit过的数据），Raft中提出了两个限制条件，如果严格遵守以下两个条件，是能保证选出合法Leader的。 在选主过程中，每台服务器只会投票给拥有比自己更新的log entry的服务器。 Leader在commit log entry的时候，只允许直接commit当前term（任期）的log entry，决不允许直接commit“历史”log entry。（历史log entry只能间接被动commit） （1）先来看看第一个条件：每台server只会投票给拥有比自己更新的log entry的server。这句话里最关键的是如何定义“更新的log entry”。按照Raft论文里给出的定义： Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs. If the logs have last entries with different terms, then the log with the later term is more up-to-date. If the logs end with the same term, then whichever log is longer is more up-to-date. 两台服务器上的最后一条log entry，如果它们的term不同，则term更大者是“更新”的。 两台服务器上的最后一条log entry，如果它们的term相同，则index更大者是“更新”的。 （这里我觉得论文里漏了一个小细节，如果两条log entry的term和index都相同，应该会默认把发起投票的服务器当成更新的，term和index都一样的情况下不存在所谓的“更新”，一切为了选主的顺利进行，能快速有效选出leader才是最重要的。） 如上图所示，根据Raft的判断标准，下者的记录比上者更新，因为下者最后一条log entry的term为5，5比4大。 如上图所示，根据Raft的判断标准，上者的记录比上者更新，因为上者最后一条log entry的index为4，下者最后一条log entry的index为3，4比3大。 （2）再研究第二个条件：Leader只允许直接commit当前term（任期）的log entry，决不允许直接commit“历史”log entry。这条前提一看上去有点云里雾里的，感觉并没有什么必要性。实则不然，这条前提非常重要，是用来避免异常情况下频繁换Leader可能造成的commited log entry被覆盖的问题。 Raft原版论文里也给出了一个经典场景： 在(a)阶段，已经处于term:2了，S1是本轮（term:2）的leader。在S1上任后没多久，收到了客户端的请求，写入一条{term:2, index:2}的log，并将此log成功复制到S2这个小兄弟上。结果天不如人愿，还没来得及把log复制到其他小兄弟上，S1自己就宕机了。 到了(b)阶段，S1宕机后，整个集群处于无领导状态。经过了一小会，S5很幸运率先结束timeout，自增term号（从term:2自增到term:3），发起了选主。由于此时S3和S4上的记录都和S5一样新，所以都愿意投票给S5，S5成功当选，成为leader。S5成为leader后马上又收到了客户端的请求，于是在本机上写入一条{term:3, index:2}的log。然而S5的命运比S1更惨，只来得及存在本地，来不及把log entry复制给任何一个小兄弟，直接就挂了。 此时来到了(c)状态，S5挂了之后，S1成功重启恢复运转。S1自告奋勇，自增全局term号（从term:3自增到term:4），请求大家选主。S2 ~ S4都会投给S1，S1顺利当选leader。S1成了leader后做了第一件事情，发现之前{term:2, index:2}的log还没有成功复制给大部分兄弟，于是开始复制工作，把log复制到了S3上。突然S1又收到了客户端的请求，写入一条{term:4, index:3}的log到本机。 以上3步都非常理所当然，没有任何的争议点，最大的争议点就出现在了(d)和(e)上。(d)和(e)实际上是(c)发生之后的两种互斥的可能情况，(d)是忽视第二条前提会发生的情况，(e)是满足第二条前提会发生的情况。 先看(d)。假设我们忽略第二条前提，也就是说leader可以随意commit任何term的log entry。那么在(c)结束之后，作为term:4的leader的S1可以commit掉{term:2, index:2}的log，并且把结果返回给客户端。结果刚完成以上步骤，S1又倒霉地宕机了，{term:4, index:3}的log没来得及复制给任何一个小兄弟。过了一会，S5恢复正常，自增全局term号（从term:4自增到term:5），要求选主。由于此时S5上的最后一条log是{term:3, index:2}，比S2 ~ S4的都更新，大家都会投票给S5，S5成功当上term:5的leader。当上leader后，S5的第一件事情就是把它还没来得及复制给多个小兄弟的log复制出去，所以就造成了(d)状态，所有服务器上的log记录都被S5自己的log记录覆盖了。之前已经成功commit的{term:2, index:2}的log直接被覆盖，消失无踪⚠️！ 再看(e)。假设我们一定要坚守第二条前提，也就是说leader只能直接commit当前term的log entry，不能直接commit历史log entry。那么在(c)结束后，S1也不能commit{term:2, index:2}的log，因为S1此时是term:4的leader，而不是term:2的leader。只有如(e)所示，之后S1有机会commit属于当前term的log entry（{term:4, index:3}）时，才有机会间接地把之前term:2的历史记录也commit掉。即使此时S1宕机，S5也绝对不可能选上下一轮的leader，因为S5上最新的log{term:3, index:2}已经不如S1 ~ S3的新了，他拿不到过半的选票，做不了leader，也就不会发生已经commit的记录被覆盖的错误了。 经过这一轮例子分析，可以很清晰地看到第二条前提的重要性了。如果允许leader随便直接commit历史记录的话，极端情况下很可能会造成数据丢失的系统错误（客户端知道你commit成功了，他就应该能放心了，结果过了一会你跟客户说commit也不算数，我搞丢了。。。。。。）所以，对于历史记录的commit只能被动触发！！！在commit当前term的log entry时顺便把之前未处理的log entry给commit掉。]]></content>
      <categories>
        <category>分布式</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>算法</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Golang]groupcache项目解析——Part2]]></title>
    <url>%2F2018%2F12%2F06%2Fgroupcache-part2%2F</url>
    <content type="text"><![CDATA[本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 本文写于2018/12/05，基于Go 1.11。至于其他版本的Go SDK，如有出入请自行查阅其他资料。 Introduction本文是《GroupCache项目解析》系列文章的第二篇，在上一篇([Golang]groupcache项目解析——Part1)中已经对GroupCache的背景、架构、代码结构作了介绍，也提供了一个简单的用例。现在来到了第二篇，主要是对GroupCache中的几个辅助性的模块作详细的讲解与分析，其中包括 Consistent Hash模块 LRU模块 SingleFlight模块 Consistent Hash（一致性哈希）所谓的一致性哈希，根本目的就是将数据打散，均匀地分布到集群上多个不同的节点。它和普通哈希最不同的地方在于，除了数据外，它把节点本身（ip地址或者节点id）也进行了哈希，放到和数据同一个哈希空间内。（具体可参考本人之前的文章《一致性Hash算法——分析与模拟》） 接下来看看groupcache里面的具体代码。 先看数据结构的定义。 // Hash就是一个返回unit32的哈希方法type Hash func(data []byte) uint32 // Map就是一致性哈希的高级封装type Map struct &#123; hash Hash // 哈希算法 replicas int // replica参数，表明了一份数据要冗余存储多少份 keys []int // 存储hash值，按hash值升序排列（模拟一致性哈希环空间） hashMap map[int]string // 记录hash值 -&gt; 节点ip地址的映射关系&#125; 接下来看看工厂方法。 // 一致性哈希的工厂方法func New(replicas int, fn Hash) *Map &#123; m := &amp;Map&#123; replicas: replicas, hash: fn, hashMap: make(map[int]string), &#125; if m.hash == nil &#123; m.hash = crc32.ChecksumIEEE // 不指定自定义Hash方法的话，默认用ChecksumIEEE &#125; return m&#125; 最后分析最关键的Add和Get方法。 // Add方法，参数为...string，一般就是多个节点的ip地址（或者节点id）func (m *Map) Add(keys ...string) &#123; for _, key := range keys &#123; // 每一个key都会冗余多份（每份冗余就是一致性哈希里的虚拟节点 v-node） for i := 0; i &lt; m.replicas; i++ &#123; // 1. 先算出当前冗余的hash值 // 2. 把hash值塞进哈希环里 // 3. 记录下hash值 -&gt; 节点ip地址的映射，之后可以凭借hash值找到具体服务器地址 hash := int(m.hash([]byte(strconv.Itoa(i) + key))) m.keys = append(m.keys, hash) m.hashMap[hash] = key &#125; &#125; sort.Ints(m.keys) // 一致性哈希要求哈希环是升序的，最后执行一次排序操作&#125;// Get方法，输入一个key，找到该key应该存于哪个节点，返回该节点的地址func (m *Map) Get(key string) string &#123; if m.IsEmpty() &#123; return "" &#125; // 1. 算出key的hash值 // 2. 二分查找大于等于该key的第一个hash值的下标（哈希环是升序有序的，所以可以二分查找） hash := int(m.hash([]byte(key))) idx := sort.Search(len(m.keys), func(i int) bool &#123; return m.keys[i] &gt;= hash &#125;) // 下标越界，循环找到到0号下标 if idx == len(m.keys) &#123; idx = 0 &#125; // 通过查询记录了hash -&gt; 节点地址的hashMap，得到节点地址，返回 return m.hashMap[m.keys[idx]]&#125; 通过上述代码可以看到，groupcache中的一致性哈希非常简单清晰。在groupcache里用到一致性哈希的地方，就是多节点部署时，要把多个节点地址用一致性哈希管理起来，从而让缓存数据能够均匀分散，降低单台服务器的压力。 但是这里实现的一致性哈希还比较粗糙，没有实现动态删除节点，还不支持节点宕机后自动数据迁移，这两个功能是一致性哈希的另一大精髓。（感兴趣的可参考我之前的文章） LRU第二个模块我们来研究下LRU。所谓LRU其实就是操作系统里那个内存页管理的经典算法——最近最少被使用（Least Recently Used Algorithm）。其实除了操作系统底层，很多数据库或者缓存产品里都实现了LRU，例如Innodb存储引擎的buffer pool里的LRU List就是一个关键数据结构。 LRU的思想非常朴素，基本都是基于一条双向链表，无非就是热门的、经常被访问的数据就放到链表头部，久而久之冷门数据就会被“排挤”到链表尾部，当内存不够时把尾部的数据移除，清理出更多空间来存新的数据。 在groupcache里，LRU用来存最底层的K-V数据，先来看看数据结构的定义。 // Key是任意可比较（Comparable）类型type Key interface&#123;&#125;// entry是一个K-V对，value也是任意类型（不必Comparable）type entry struct &#123; key Key value interface&#123;&#125;&#125;// LRU的高层封装（非并发安全！）type Cache struct &#123; MaxEntries int // 最多允许存多少个K-V entry OnEvicted func(key Key, value interface&#123;&#125;) // 回调函数，当一个entry被移除后回调 ll *list.List // LRU链表 cache map[interface&#123;&#125;]*list.Element // 记录Key -&gt; entry的映射关系，O(1)时间得到entry&#125; 接下来看看关键的Add和Get方法。 // Add方法，插入一个K-V对func (c *Cache) Add(key Key, value interface&#123;&#125;) &#123; if c.cache == nil &#123; c.cache = make(map[interface&#123;&#125;]*list.Element) c.ll = list.New() &#125; // 如果该key已存在，更新entry里的value值，并将entry挪到链表头部 if ee, ok := c.cache[key]; ok &#123; c.ll.MoveToFront(ee) ee.Value.(*entry).value = value return &#125; // 如果该key不存在，新建一个entry，插到链表头部 ele := c.ll.PushFront(&amp;entry&#123;key, value&#125;) c.cache[key] = ele // 如果超出链表允许长度，移除链表尾部的数据 if c.MaxEntries != 0 &amp;&amp; c.ll.Len() &gt; c.MaxEntries &#123; c.RemoveOldest() &#125;&#125;// Get方法，通过Key来拿对应的valuefunc (c *Cache) Get(key Key) (value interface&#123;&#125;, ok bool) &#123; if c.cache == nil &#123; return &#125; // 如果该key存在，获取对应entry的value，将该entry挪到链表头部，返回 if ele, hit := c.cache[key]; hit &#123; c.ll.MoveToFront(ele) return ele.Value.(*entry).value, true &#125; return&#125; SingleFlightSingleFlight是一个非常重要的模块，看它的名字里有一个Single有一个Flight，其实Single指的是N条对同一个key的查询命令中只有1条被真正执行，而Flight大家就把它等价于Execution就行了。 先来看看SingleFlight里的数据结构的定义。 // call等价于一条被真正执行的对某个key的查询操作type call struct &#123; wg sync.WaitGroup // 用于阻塞对某个key的多条查询命令，同一时刻只能有1条真正执行的查询命令 val interface&#123;&#125; // 查询结果，也就是缓存中某个key对应的value值 err error&#125;// Group相当于一个管理每个key的call请求的对象type Group struct &#123; mu sync.Mutex // 并发情况下，保证m这个普通map不会有并发安全问题 m map[string]*call // key为数据的key，value为一条call命令，记录下某个key当前时刻有没有客户端在查询&#125; 接下来看看SingleFlight里面唯一一个，也是最重要的一个方法——Do() // Do里面是查询命令执行的逻辑。// 当客户端想查询某个key对应的值时会调用Do方法来执行查询。// 参数传入一个待查询的key，还有一个对应的查询方法，返回key对应的value值func (g *Group) Do(key string, fn func() (interface&#123;&#125;, error)) (interface&#123;&#125;, error) &#123; // 为了保证普通map的并发安全，要先上锁 g.mu.Lock() // 检查map有无初始化 if g.m == nil &#123; g.m = make(map[string]*call) &#125; // 检查当前时刻，该key是否已经有别的客户端在查询 // 如果有别的客户端也正在查询，map里肯定存有该key，以及一条对应的call命令 if c, ok := g.m[key]; ok &#123; g.mu.Unlock() // 解锁，自己准备阻塞，此时已不存在并发安全问题，允许别人进行查询 c.wg.Wait() // 阻塞，等待别的客户端完成查询就好，不用自己再去耗费资源查询 return c.val, c.err // 阻塞结束，说明别人已经查询完成，拿来主义直接返回 &#125; // 如果能执行到此步，说明当前时刻没有别人在查询该key，当前客户端是 // 当前时刻第一个想要查询该key的人，就插入一条key -&gt; call记录 // 注意，此时的map仍然是上锁状态，因为还要对map进行插入，有并发安全问题 c := new(call) c.wg.Add(1) g.m[key] = c g.mu.Unlock() // 执行作为参数传入的查询方法 // **同一时刻对于同一个key只可能有一个客户端执行到此处** c.val, c.err = fn() c.wg.Done() // 执行完查询方法，把map中的key -&gt; call删掉 g.mu.Lock() delete(g.m, key) g.mu.Unlock() return c.val, c.err&#125; 结合上述代码注释里的分析，SingleFlight的逻辑应该很清楚了。特别提一提里面的几个思维亮点： 时刻谨记go里面的普通map不是并发安全的，要在有并发安全隐患的地方手动上锁和解锁。 用一个map来记录key与查询请求，可以迅速得知（理想情况下O(1)）当前时刻某个key是否有人在执行查询。 本来用set类容器来存当前正被人查询的key也可以完成以上需求。但是第二个亮点就是call结构，call里面封装了一个WaitGroup和一个val。当某一时刻有N个对某个key的查询请求，通过WaitGroup来阻塞其中的N-1个，只执行1次查询方法，然后把查询结果塞到call.val中，通知WaitGroup完成任务。这样做，不仅执行查询的那一个“天选之子”可以返回该值，而且那N-1个被阻塞的也可以直接取call.val作为结果返回。 总结groupcache这个项目的代码量虽不多，但有很多精华的地方。如 实现一致性哈希来管理多节点 实现LRU算法来管理底层K-V数据 实现SingleFlight来提高并发查询效率 其中，SingleFlight的逻辑最让我开了眼界。之前对于“并发查询”的优化方面，我考虑的可能也就是如何优化存储的数据结构，或者类似于把请求分发到多台机器上处理，用多机的计算能力来抗。但是这些都不如SingleFlight里的逻辑这么粗暴明了，同时又高效。]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>笔记</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Golang]groupcache项目解析——Part1]]></title>
    <url>%2F2018%2F11%2F30%2Fgroupcache-part1%2F</url>
    <content type="text"><![CDATA[本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 本文写于2018/11/29，基于Go 1.11。至于其他版本的Go SDK，如有出入请自行查阅其他资料。 Introduction本文是《groupcache项目解析》系列文章的第一篇，在本文中主要是对groupcache这个项目作简单的介绍，包括其 架构 代码文件结构 简单用例 groupcache简介groupcache(on Github)是一个用Go实现的K-V cache的库，可以起到memcached的部分功能。它支持单节点部署，也支持多节点部署。 其中最值得一提的两个特性是： 不支持update和delete（基本只能用于静态资源缓存） 热门缓存自动镜像（auto mirroring） ⚠️注意，groupcache并不是一个可直接运行的存储组件，不像MySQL或者Redis之类那样提供编译后的可运行程序。groupcache只是一个K-V cache的第三方库，基于它的代码可以自己写代码实现一个cache层。 groupcache架构 节点管理 以上提到，groupcache是一个支持多节点部署的K-V cache。当有多个存储节点时，内部会以consistent hash（一致性哈希）的方式管理多个节点。关于一致性哈希的解析，详情可参考我之前的文章《一致性Hash算法——分析与模拟》。 通过一致性哈希，可以统一管理多个节点。如果哈希算法设计得比较好，可以把大量K-V数据均匀打散，存储到不同的节点上。 group（存储组） 在groupcache里，group是一个相对独立的存储容器，每个group都有自己的名字，多个group之间不共享数据。然而group只是一个逻辑概念，一个group里存的K-V数据是可以存在多个分散的物理节点上的（分散的策略依赖于一致性哈希算法）。也就是说每个物理节点上实际上存了多个group的K-V数据，组与组之间的访问隔离全靠groupcache的代码逻辑来实现。 缓存系统 LRU 在缓存系统的底层，每个K-V Entry都是通过一条LRU链表来管理的。经常被访问的数据会被放置在LRU链表的前端，久而久之冷数据会下沉到链表尾端，甚至直接被移出链表。 并发查询优化 在groupcache中，如果某节点同时收到N个对于同一个key的查询请求，但是请求的key不在当前节点上，groupcache会自动阻塞N-1个请求，只执行其中一个请求，去其他节点或者数据库中fetch数据。最后才恢复N个请求，把数据放到N个请求中返回。因为无论多少个对同一个key的查询请求并发到达，只执行一次查询，所以并发查询效率很高。 热门缓存自动镜像 每个节点都包含了两类缓存：main cache（属于本节点的数据）和hot cache（不属于本节点但是全局热门的数据）。当节点收到了对某个key的查询请求，它首先会检查本地hot cache中有没有，如果没有就再看看该key是不是属于本节点的数据，如果不是就向兄弟节点请求。所谓的自动镜像，指的是从兄弟节点处返回的数据可以缓存在本节点的hot cache里，虽然自身没有那个数据的存储权限，但是可以存储成一份热门数据的镜像，以后再收到对该key的请求，无需再向兄弟节点请求，浪费网络资源。 groupcache代码模块 consistenthash consistenthash模块实现了简单的一致性哈希算法。数据（一般是节点地址）进入一致性哈希后，会被自动冗余得到多个备份（取决于replica的设定值），然后插到一致性哈希环上。 groupcachepb groupcachepb模块里，用了第三方库protobuf生成了统一的Request和Response结构，供节点间网络通信使用。 lru lru模块实现了经典的LRU算法，用container/list里的链表实现。 singleflight singleflight模块非常重要。正如它名字里的single，它是用来保证多个对同一个key的请求不被多次执行的。也就是上面简介所说的并发查询优化。 testpb testpb，测试protobuf结构。 byteview byteview是一个对byte数组或者字符串的封装，在外部看来，groupcache里的所有K-V数据最终都是落盘到byte上，都是对byteview的读写操作。 groupcache 核心代码文件，其中定义了Group、GetterFunc、Stats等多个关键数据结构，以及对应的方法。 http 核心代码文件，定义了HTTPPool以及对应的方法，包含了各种网络通信的逻辑。 peers 定义了节点的相关操作。 sinks sinks里定义了Sink接口以及多种不同的sink。其实sink可以理解为一种特殊容器，当节点收到对某个key的查询请求，但是本地没有数据，需要到远程数据库里读取时，会把读取回来的数据下沉到sink容器里面，最后再把数据转成byteview塞到本地缓存里。 Quick Start Example以下提供一个简单的groupcache使用例子。 package mainimport ( "github.com/golang/groupcache" "io/ioutil" "log" "net/http" "os")var ( // 简单起见，hardcode一段兄弟节点的地址 peers = []string&#123;"http://127.0.0.1:8001", "http://127.0.0.1:8002", "http://127.0.0.1:8003"&#125;)func main() &#123; // 先建好http连接池，表明当前节点的兄弟节点有哪些 host := os.Args[1] localAddr:= "http://" + host localHttpPool := groupcache.NewHTTPPool(localAddr) localHttpPool.Set(peers...) // 定义一个逻辑上的分组，叫fileCacheGroup，用来缓存文件内容，缓存大小为64MB // 当本地缓存miss时，直接读取磁盘上的文件 var fileCacheGroup = groupcache.NewGroup("file", 64&lt;&lt;20, groupcache.GetterFunc( func(ctx groupcache.Context, key string, dest groupcache.Sink) error &#123; result, err := ioutil.ReadFile(key) if err != nil &#123; log.Println("Get file error.") return err &#125; log.Printf("Trying to get %s\n", key) dest.SetBytes([]byte(result)) return nil &#125;)) // 为了测试，建立一个对外的http服务，路由是host:port/file_cache?fname=&#123;&#125; http.HandleFunc("/file_cache", func(rw http.ResponseWriter, r *http.Request) &#123; var value []byte key := r.URL.Query().Get("fname") fileCacheGroup.Get(nil, key, groupcache.AllocatingByteSliceSink(&amp;value)) rw.Write([]byte(value)) &#125;) // 启动http服务 log.Fatal(http.ListenAndServe(host, nil))&#125; 如需测试上述代码，可编译后直接命令行执行（记得带上host参数如127.0.0.1:8001）。由于cache miss的逻辑是在本地磁盘上读取文件，所以可以先在目录下新建几个垃圾文本文件，里面随便填充一些内容，进行测试。 测试结果如上图所示。 先在本地随便新建一个hh.txt文件，里面写上This is hh.txt!。 然后编译完上述例子程序后，直接传入参数127.0.0.1:8001以命令行启动程序。 用postman访问代码中定义好的服务路由（host:port/file_cache?fname={}），测试缓存服务，得到response结果为hh.txt的文件内容。]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Golang]浅析几种并发模式]]></title>
    <url>%2F2018%2F11%2F25%2Fgo-concurrency-pattern%2F</url>
    <content type="text"><![CDATA[本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 本文写于2018/11/24，基于Go 1.11。至于其他版本的Go SDK，如有出入请自行查阅其他资料。 Introduction最近读完了整本《Go in Action》，给我印象最深的几章是讲 多态 高级并发模式 常用工具包（http, json, log……） 本文基于《Go in Action》里介绍的3种高级并发模式进行浅析，主要起到解释和笔记的作用，也会简单地讲讲我个人对这几种模式的理解。 并发模式 任务计时器 何谓“任务计时器”？其实就是一个包装了多个要执行的Task和一个Timer的容器。该容器启动后，只有3种情况能够让其退出： 所有Task执行完毕，正常退出 收到外部中断信号（interrupt），退出 Timer超过设定的时长，退出 先来直接看看《Go in Action》中给出的实现代码。 package runnerimport ( "errors" "os" "os/signal" "time")// 任务计时器type Runner struct &#123; interrupt chan os.Signal // 接收中断信号的channel complete chan error // 接收任务完成信号的channel timeout &lt;-chan time.Time // 接收超时信号的channel tasks []func(int) // 存放要执行的多个任务的切片 // 其中每个任务是一个以int为形参的方法&#125;// 定义了两种异常退出的错误var ErrTimeout = errors.New("received timeout")var ErrInterrupt = errors.New("received interrupt")// 容器工厂，通过new直接得到一个容器实例func New(d time.Duration) *Runner &#123; return &amp;Runner&#123; interrupt: make(chan os.Signal, 1), complete: make(chan error), timeout: time.After(d), &#125;&#125;// 往容器里添加任务的方法func (r *Runner) Add(tasks ...func(int)) &#123; r.tasks = append(r.tasks, tasks...)&#125;// 容器启动的方法func (r *Runner) Start() error &#123; // We want to receive all interrupt based signals. signal.Notify(r.interrupt, os.Interrupt) // Run the different tasks on a different goroutine. go func() &#123; r.complete &lt;- r.run() &#125;() select &#123; // Signaled when processing is done. case err := &lt;-r.complete: return err // Signaled when we run out of time. case &lt;-r.timeout: return ErrTimeout &#125;&#125;// 容器内部执行任务的方法func (r *Runner) run() error &#123; for id, task := range r.tasks &#123; // Check for an interrupt signal from the OS. if r.gotInterrupt() &#123; return ErrInterrupt &#125; // Execute the registered task. task(id) &#125; return nil&#125;// 检查是否发生中断信号func (r *Runner) gotInterrupt() bool &#123; select &#123; case &lt;-r.interrupt: // Stop receiving any further signals. signal.Stop(r.interrupt) return true default: return false &#125;&#125; 上述代码的逻辑很清晰，但仍有几个细节需要特别强调一下。 在容器Runner里，timeout是一个接收超时信号（time.Time）的channel，一旦该channel接收到一个超时信号，将通知Runner退出。而在new一个新的Runner时，我们用的是time.After(duration)来生成一个timeout管道，为什么不直接make(chan time.Time)呢？因为用time.After(duration)生成管道时，会潜在自动触发一个机制：经过duration后该管道会收到一个time.Time信号，不需要自己额外去做发送超时信号这一套逻辑。 使用select语句。可以看到在Start()中，开了一个goroutine去执行任务后，我们写了一个select语句，其中两个分支分别是所有任务正常完成且收到complete信号，还有任务超时收到超时信号。其实select语句可以简单地看成是一个定制版的epoll机制，它可以同时监听多个case。如果所有的case分支都不能执行，将阻塞在此；如果有一个case可以执行，一定会执行该case；和epoll唯一不同的是，如果同时有多个case可以执行，select会随机执行其中一个case。同理，在gotInterrupt()里，我们也用了select语句监听有没有中断信号，如果在执行gotInterrupt()的那个时刻没有收到中断信号，那绝对会直接执行default分支（永远都能执行的分支），返回false告知调用者没有收到中断信号。 在容器Runner里，tasks是一个装了多个待执行任务的切片，定义里说明了每个任务都是一个func(int)。但是这并不具有普适性，这不是必须的，设计者可以根据自己的需求来定义每个任务是什么样的方法。可以是func(interfact{}) interface{}，可以是任意的方法。 run()方法用于在容器内部执行多个任务。最奇怪的是，在run()里面其实是遍历了tasks，逐个逐个串行地执行任务，只有上一个任务完成了下一个任务才会开始。个人认为，也许这种设计迎合了一定的场景需求（上下游任务间存在依赖），但是有些时候我们确实是需要并发地执行多个任务。建议改成多个goroutine并发执行tasks中的任务，然后用WaitGroup来阻塞，等待所有任务完成。 func (r *Runner) run() error &#123; wg := sync.WaitGroup&#123;&#125; wg.Add(len(r.tasks)) for id, task := range r.tasks &#123; if r.gotInterrupt() &#123; return ErrInterrupt &#125; // 开多个goroutine并发执行多个任务，用WaitGroup来统一 go func() &#123; defer wg.Done() task(id) &#125;() &#125; wg.Wait() return nil&#125; 资源池 什么是资源池，顾名思义，就是一个放满了各种资源的池子。讲得“专业”一点，就是一个管理着多个可用资源的容器，外部的线程/协程可向资源池申请资源，使用完后可以把资源放回资源池，重复利用。 直接来看看《Go in Action》中的代码。 package poolimport ( "errors" "io" "log" "sync")// 资源池type Pool struct &#123; m sync.Mutex // mutex用于控制同步 resources chan io.Closer // 存放资源的管道 factory func() (io.Closer, error) // 新建资源的工厂 closed bool // 资源池是否关闭的标志&#125;// 错误信号var ErrPoolClosed = errors.New("Pool has been closed.")// 资源池工厂，用于新建资源池func New(fn func() (io.Closer, error), size uint) (*Pool, error) &#123; if size &lt;= 0 &#123; return nil, errors.New("Size value too small.") &#125; return &amp;Pool&#123; factory: fn, resources: make(chan io.Closer, size), &#125;, nil&#125;// 从资源池中拿资源func (p *Pool) Acquire() (io.Closer, error) &#123; select &#123; case r, ok := &lt;-p.resources: log.Println("Acquire:", "Shared Resource") if !ok &#123; return nil, ErrPoolClosed &#125; return r, nil default: log.Println("Acquire:", "New Resource") return p.factory() &#125;&#125;// 把资源放回资源池func (p *Pool) Release(r io.Closer) &#123; p.m.Lock() defer p.m.Unlock() if p.closed &#123; r.Close() return &#125; select &#123; case p.resources &lt;- r: log.Println("Release:", "In Queue") default: log.Println("Release:", "Closing") r.Close() &#125;&#125;// 关闭资源池func (p *Pool) Close() &#123; p.m.Lock() defer p.m.Unlock() if p.closed &#123; return &#125; p.closed = true close(p.resources) for r := range p.resources &#123; r.Close() &#125;&#125; 以下讲讲几个资源池设计中的细节。 关于factory。factory是一个用来新建资源的工厂，当资源池内没有资源时，上述代码的逻辑是通过事先定义好的factory来新建资源。当然，也可以不这么干。根据不同的场景和需求，当资源池内没有资源时可以选择阻塞，而非新建资源。 用mutex来同步Release()和Close()。在同一时刻，不能同时有多个协程进入Release()和进入Close()。也就是说有协程在关闭资源池时，不允许别的协程放回资源；有协程在放回资源时，不允许别的协程关闭资源池。如果不用mutex进行同步，假设同时有协程A进入了Release()，协程B进入了Close()。特殊情况下，当协程A运行到select语句前失去了cpu资源，协程B正常运行关闭了resources管道，协程A再想往一个已关闭了的resources管道里插数据，会直接引起错误。 resources管道是一个缓冲管道。通过工厂new一个资源池时，resources被定义成了一个大小为size的缓冲管道。用缓冲管道的好处是，只有当管道全空或者全满时才会对生产者/消费者进行阻塞，其他情况下正常生产/消费资源。 并发池 所谓并发池，就是一个放了N个待执行任务的池子，或者可以看成是一个可并发执行任务，却不带有定时功能的任务计时器。 package workimport "sync"// 任务接口type Worker interface &#123; Task()&#125;// 并发池type Pool struct &#123; work chan Worker wg sync.WaitGroup&#125;// 并发池工厂，用于新建并发池func New(maxGoroutines int) *Pool &#123; p := Pool&#123; work: make(chan Worker), &#125; p.wg.Add(maxGoroutines) for i := 0; i &lt; maxGoroutines; i++ &#123; go func() &#123; for w := range p.work &#123; w.Task() &#125; p.wg.Done() &#125;() &#125; return &amp;p&#125;// 提交任务func (p *Pool) Run(w Worker) &#123; p.work &lt;- w&#125;// 关闭并发池func (p *Pool) Shutdown() &#123; close(p.work) p.wg.Wait()&#125; 基本上实现的功能跟优化后的任务计时器一样（除了不支持定时），支持多goroutine并发执行。 应用场景 任务计时器适用于各种监控任务，或者对执行时间有限制的任务。 资源池多用于管理各种连接，例如数据库连接，提高连接的复用性。 并发池多用于计算密集型任务，需要多个goroutine并发执行多个小任务。 总结《Go in Action》中介绍的3种并发模式都非常实用，也有很强的普适性。但是在实际项目中还是需要搞清楚具体的需求，要清楚它们的assumption和它们的缺点，并不是100%地适用于所有场景，要基于这几种基本的模式作更深层次的自定义开发。]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Golang]更正“神贴”《如何优雅地关闭Go Channel》]]></title>
    <url>%2F2018%2F11%2F20%2Fclose-channel%2F</url>
    <content type="text"><![CDATA[本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 本文写于2018/11/19，基于Go 1.11。至于其他版本的Go SDK，如有出入请自行查阅其他资料。 Introduction现在在网上搜索Go，Channel，关闭等关键词时，一定会搜到一篇好几年前的“神贴”《How To Gracefully Close Channels》（原文链接请点击），或者各种国人执笔的中文直译版《如何优雅地关闭Go Channel》。 基于Go本身对并发的强支持，不断地有开发者需要学习Channel这个被设计为多个goroutine间安全传递数据的内置数据结构，自然也有很多人学习过以上这篇文章。然而不知道是受当时写作的时间背景限制，还是其他原因，实际上这篇文章里有一些比较严重的缺陷，必须得到纠正。 我这么一个Go的初学者，研究过后暂时没有发现网上对原贴进行优化或者更正的博客，所以我决定写下这篇更正“神贴”的博客，并提供一些我的关闭Go Channel的解决方案。 驳斥理由 原文标题的核心是Close Channels，然而除了第一个最简单的例子（M receivers，one sender）里有关闭数据channel的逻辑外，其他几个复杂例子中压根没有close channel，仅仅只是退出sender 即使我们把“退出sender”等价于“close channel”。但是同样除了第一个最简单的例子（M receivers，one sender）里是sender主动关闭外，其他几个例子中退出sender都是由receiver触发的，类似receiver读到一个什么特殊值就提示sender停止生产。我不否认在某些场景里确实需要receiver提示sender何时结束（例如receiver发生了异常，无法继续处理，可通知sender赶紧结束）。但是在很多场景里是需要sender自己触发停止生产的，而不是让receiver告知才停止（例如100个sender分别读100台机器上的文件，然后把文件数据怼到一个channel里，此时肯定不可能让receiver来主导sender的数据读取何时停止，对吧）。 最后一个问题，原贴例子里receiver读到一个特殊值导致退出后，并没有安排别的goroutine去读完channel中可能剩下的数据，直接导致数据丢失。（当然作者也意识到这一点了，所以在最后提了一下，说读完剩下的数据很简单blabla，我就不写了你们自己去实现就好了） 解决方案其实很明显，关闭channel最主要的麻烦在于sender端如何控制，既不能不去关闭，也不能重复关闭（panic）。所以接下来就讨论两种在sender端关闭channel的解决方案：单个sender和多个sender的应用场景。 单个sender package mainimport ( "fmt" "sync")// 一个sender，一个receiverfunc main() &#123; dataChannel := make(chan int, 100) done := make(chan interface&#123;&#125;) go sender(dataChannel) go receiver(dataChannel, done) // 阻塞直到receiver完成，避免主线程马上退出 &lt;-done fmt.Println("Done.")&#125;func sender(dataChannel chan int) &#123; defer close(dataChannel) for i := 0;i &lt; 1000;i++ &#123; dataChannel &lt;- i &#125;&#125;func receiver(dataChannel chan int, done chan interface&#123;&#125;) &#123; for data := range dataChannel &#123; fmt.Printf("Receive data %d\n", data) &#125; done &lt;- nil&#125; 在单个sender的场景下，没有什么好说的。其实就是当sender把所有数据都塞到channel之后主动关闭该channel。**显式close channel的好处是，当receiver端使用range```从channel中读数据，读到close标识后会自动结束循环。（或者是普通循环里的ok标识为false，用来结束循环）**+ 多个sender ```golang package main // 多个sender，一个receiver const ( SENDER_COUNT = 5 ) func sender(id int, dataChannel chan string, wg *sync.WaitGroup) &#123; defer wg.Done() for i := 0;i &lt; 100;i++ &#123; dataChannel &lt;- fmt.Sprintf(&quot;Sender %d is sending %d&quot;, id, i) &#125; &#125; func receiver(dataChannel chan string, done chan interface&#123;&#125;) &#123; for data := range dataChannel &#123; fmt.Printf(&quot;Receive data: %s\n&quot;, data) &#125; done &lt;- nil &#125; func monitor(dataChannel chan string, wg *sync.WaitGroup) &#123; wg.Wait() close(dataChannel) &#125; func main() &#123; dataChannel := make(chan string, 100) done := make(chan interface&#123;&#125;) wg := &amp;sync.WaitGroup&#123;&#125; wg.Add(SENDER_COUNT) go monitor(dataChannel, wg) for i := 0;i &lt; SENDER_COUNT;i++ &#123; go sender(i, dataChannel, wg) &#125; go receiver(dataChannel, done) &lt;-done fmt.Println(&quot;Done.&quot;) &#125; 参照以上代码，核心的思想是利用sync包自带的WaitGroup（类似于Java里J.U.C包的CountdownLatch），用来统计已完成工作的sender数。除了sender和receiver外，我们还定义了一个monitor协程，用来关闭channel。 可以看到我们定义了1个monitor，1个receiver，5个sender，并且在启动sender之前先启动了monitor，传入waitGroup，让其等待所有sender协程完成工作。在sender里，通过defer来保证sender在完成作业（或者发生异常）之后能够通知waitGroup。当所有sender都完成工作后，waitGroup计数自然减为0，monitor协程主动关闭了数据channel，所以receiver端的for range循环在读完所有数据后就能正常退出。 总结本文是对《How To Gracefully Close Channels》的核心内容表示质疑，并且提出了我自己的解决方案。其实原文里作者提供的解决方案并不是错误的，里面的方案对部分场景肯定是适用的。只是对sender端主动关闭的场景而言有一定的纰漏。 希望读者能理解不同场景下应该有不同的解决方案，具体还是要结合实际项目来分析。]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Golang]奇怪的“类”和多态（Polymorphism）]]></title>
    <url>%2F2018%2F11%2F15%2Fpolymorphism%2F</url>
    <content type="text"><![CDATA[本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 本文写于2018/11/14，基于Go 1.11。至于其他版本的Go SDK，如有出入请自行查阅其他资料。 Introduction多态这个词语每个开发者都一定不会感到陌生。因为在很多不同的编程语言里面，多态都是有具体实现的。例如在C++里面我们可能会说父类的指针可以指向子类的对象，在Java里面虽然没有指针这样的概念，但是我们一般会说，如果某个子类实现了一个接口，那么这个接口的引用就可以引用这个子类的一个实例对象。 但是类似于C++和Java里面的实现基本上都是基于接口和类的相互合作去构建的。而在Go里面，是有接口的定义，但是并不存在严格意义上的类（Class），只是单纯的结构体（Struct），而且实现方法时还有略显奇葩的Receiver机制。所以本文的目标就是来探索一下在Go语言里面多态是如何实现的，以及在Go语言时实现多态的过程中有哪些奇奇怪怪的坑。 类、方法、多态 基本接口实现 首先看看一下在Go语言里面如何去实现一个接口，其实就是让一个结构体去实现接口里定义的所有方法。但是在Go里面结构体（Struct）并不是我们在Java中认知的类（Class），所以我们所谓的实现一个方法是要写在结构体的外部，并且为每个方法定义一个接收者（Receiver）。 // Human接口，内含breathe()方法，因为所有人类都能呼吸type Human interface &#123; breathe()&#125;// 学生结构体，内含姓名和年龄type Student struct &#123; name string age int&#125;// 实现Student类的breathe方法，实现了Human接口func (s Student) breathe() &#123; fmt.Println("I can breathe.")&#125; Receiver的选择 在为每个方法定义Receiver的时候，不仅可以把Receiver定义为一个结构体的对象（值），还可以定义为该结构体的一个指针。当你需要对某个对象（值）里面的属性做修改的时候，例如更新或者删除原值，一般情况下会把Receiver定义为结构体的指针。 // Driver接口，内含updateLicense()方法，因为司机有时需要更新驾照信息type Driver interface &#123; updateLicense()&#125;// Man结构体type Man struct &#123; name string age int license string&#125;// 实现Man的updateLicense方法，实现了Driver接口func (m *Man) updateLicense() &#123; m.license = "new license" fmt.Println("License is updated.")&#125; 方法调用 无论你是用指针还是用某个结构体具体的值，都可以直接对该结构体所实现的方法进行直接的调用。当你使用指针调用某个接收者是值的方法时，Go语言的内部会帮你把指针指向的对象（值）找出来，然后再进行调用。如果某个方法的接收者是一个指针，同样也可以用对象（值）来进行调用。这是因为go语言内部会自动把对象（值）的地址找到，然后构建出指向该对象（值）的指针，就可以顺利进行调用。 // 学生结构体，内含姓名和年龄type Student struct &#123; name string age int&#125;// Receiver为对象（值）的方法func (s Student) breathe() &#123; fmt.Println("I can breathe.")&#125;// Receiver为指针的方法func (s *Student) grow() &#123; s.age += 1 fmt.Println("Grow older.")&#125;func main() &#123; student := Student&#123;name: "hh", age: 18&#125; //学生对象（值） pointer := &amp;student //指针 student.breathe() //直接调用正常，打印出I can breathe. pointer.breathe() //通过指针调用也正常，也打印出I can breathe. student.grow() //直接调用正常，打印出Grow older pointer.grow() //通过指针调用也正常，也打印出Grow older &#125; 从上述代码可以看出，无论某个方法的Receiver是对象（值）还是指针，都可以通过对象（值）或者指针来进行方法调用。再仔细想一想也是非常合理的，因为Go内部可以轻松地找到一个对象（值）的地址，自然就能构建出指向它的指针（&amp;obj），调用Receiver为指针的方法；另外也很容易通过指针找到其指向的对象（值）（*p），自然也能轻松调用Receiver为对象（值）的方法。 最简单的多态示例 // Human接口，内含breathe()方法，因为所有人类都能呼吸type Human interface &#123; breathe()&#125;// 学生结构体，内含姓名和年龄type Student struct &#123; name string age int&#125;// 实现Student类的breathe方法，实现了Human接口func (s Student) breathe() &#123; fmt.Println("I can breathe.")&#125;// 测试多态的Test方法func Test(h Human) &#123; h.breathe()&#125;func main() &#123; student := Student&#123;name: "hh", age: 18&#125; Test(student) //打印出I can breathe.&#125; 上述是最简单直接描述多态的代码例子，Test方法的入参是一个接口类型（Human），只要实现了Human接口的任一类型的对象都可以传进去，可以是这里的Student，也可以是Driver，也可以是Teacher。无所谓，只要实现了Human接口就没问题。 实现多态时诡异的报错 在上面最简单的多态例子里，Student实现了Human接口，实现了一个以值为Receiver的breathe()方法，便可成功传入Test方法里。那假如接口中声明了不止一个方法，且实现时Receiver不一定是值，还可能是指针呢？那样可以吗？ // Human接口，内含breathe()和grow()type Human interface &#123; breathe() grow()&#125;// 学生结构体，内含姓名和年龄type Student struct &#123; name string age int&#125;// 实现Human接口中的breathe()func (s Student) breathe() &#123; fmt.Println("I can breathe.")&#125;// 实现Human接口中的grow()，因为要改变age属性，所以Receiver为指针func (s *Student) grow() &#123; s.age += 1 fmt.Println("I can grow.")&#125;// 测试多态的Test方法func Test(h Human) &#123; h.breathe() h.grow()&#125;func main() &#123; student := Student&#123;name: "hh", age: 18&#125; Test(student) // 此处报错！！！ // 报错信息如下 // cannot use student (type Student) as type Human in argument to Test: // Student does not implement Human (grow method has pointer receiver)&#125; 从上述代码的报错中看到，Student没有实现Human接口，因为grow方法的Receiver是指针？？？可是我明明实现了grow方法啊，只是它的Receiver是指针而已！难道*Student和Student居然被认为是两种不同的类型？？？ 没错！在Go的设计理念中，type pointer和type value确实就是两种不同的类型！ 所以如果想让某个结构体实现一个接口，必须要分离开来思考，你到底是想让type pointer实现还是想让type value实现？ 这个时候，正常人都会想：我要实现一些需要改变对象属性值的方法（像上面的grow()），当然需要让这些方法的Receiver为指针，不然怎么改变对象内部的属性值啊？而对于那些不需要改变对象属性值的方法，Receiver为指针也不会出错，顶多就是看着不规范而已。好，那就把所有实现的方法的Receiver都改成指针，肯定能正常实现那个接口~ 于是便有了下面的代码👇 // Human接口，内含breathe()和grow()type Human interface &#123; breathe() grow()&#125;// 学生结构体，内含姓名和年龄type Student struct &#123; name string age int&#125;// 全改成指针Receiver，美滋滋func (s *Student) breathe() &#123; fmt.Println("I can breathe.")&#125;// 全改成指针Receiver，美滋滋func (s *Student) grow() &#123; s.age += 1 fmt.Println("I can grow.")&#125;// 测试多态的Test方法func Test(h Human) &#123; h.breathe() h.grow()&#125;func main() &#123; student := Student&#123;name: "hh", age: 18&#125; Test(student) // 此处还是报错！！！ // 报错信息如下 // Cannot use 'student' (type Student) as type Human // Type does not implement 'Human' as 'breathe' method has a pointer receiver less... (⌘F1) //Inspection info: Reports incompatible types.&#125; 又出错了？！甚至都不用run，IDE直接报错？！ 之前说好的指针和对象在调用方法时可以随便互换使用呢？？？为什么这里不行了？？？ 此时就要搬出Go specification里的经典表格来解释这个表层现象了👇👇👇。 注意看第二个表格。第一行：当Methods Receivers为对象（值）（t T）的时候，可用对象（值）或者指针作为多态方法的接口参数；第二行：当Methods Receivers为指针时，只能用指针作为多态方法的接口参数。我们实现的方法的Receiver全都是指针，所以我们传一个对象（值）进去，像Test(student)就会报错。如果我们把main中的代码改成Test(&amp;student)，用指针作为方法的多态接口参数，自然就不会报错了。 但这些只是表层现象，再研究得深入一点，为什么会这样呢？原来我们一直认为“有了对象（值）就一定能找到它的地址，从而构建出指向它自己的指针”，其实这种想法在一定程度上是有问题的。在Go In Action一书中给出了一段示意代码，展示了为什么有的时候是找不到对象（值）的地址的👇👇👇。 01 // Sample program to show how you can't always get the 02 // address of a value. 03 package main 04 05 import "fmt" 06 07 // duration is a type with a base type of int. 08 type duration int 09 10 // format pretty-prints the duration value. 11 func (d *duration) pretty() string &#123; 12 return fmt.Sprintf("Duration: %d", *d) 13 &#125; 14 15 // main is the entry point for the application. 16 func main() &#123; 17 duration(42).pretty() 18 19 // cannot call pointer method on duration(42) 20 // cannot take the address of duration(42) 21 &#125; 当你拥有一个对象（值），你有可能拿不到它的地址，那就没有办法构建出指向它的指针，自然也就没有办法访问到Receiver为指针的那些方法。所以该对象所拥有的方法集合（Method Set）中只包含Receiver为对象（值）的那部分方法 而当你拥有一个指针的时候，你肯定、必然、100%能拿到它指向的对象（值）。那你既能访问到Receiver为指针的方法，也能访问到Receiver为对象（值）的方法。所以该指针所拥有的方法集合（Method Set）中包含了Receiver为指针和Receiver为对象（值）的所有方法 这也就是上面第一个表格的含义。然后我们试着把第一个表格转置一下，也就能得到第二个表格。解释了为什么我们把Test(student)改成Test(&amp;student)就能通过的原因。另外如果把那两个方法改成func (s Student) breathe()和func (s Student) grow()，那无论是Test(student)还是Test(&amp;student)都可以正常运行，因为参照第二个表格，当实现接口中的方法的Receiver为对象（值）时，以接口类型作为参数的多态方法可以接收对象（值）也可以接收指针，无所谓。 总结本人文笔水平有限，在文字解释部分可能稍显混乱，如有疑问请反复参照示意代码、表格、图片，也欢迎留言讨论。 在Go实现多态这一部分，最麻烦的莫过于同一个结构体的指针类型和值类型，在实现接口时被认为是不一样的类型。当某个结构体想实现一个接口，统一了所有方法的Receiver后，在传参给接口参数时又出现了类型不匹配方面的小坑。表面上fix掉报错很容易，但其底层的原理掰开来还是有点小复杂的。]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Golang]奇葩数据结构之Slice（切片）]]></title>
    <url>%2F2018%2F11%2F10%2Fgo-slice%2F</url>
    <content type="text"><![CDATA[本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 本文写于2018/11/09，基于Go 1.11。至于其他版本的Go SDK，如有出入请自行查阅其他资料。 Introduction最近新接触Golang，个人有一种习惯，在粗略过完基础语法后就开始深入研究一番语言内部built-in的数据结构。想当年学Java时，就抠了不少JDK7和JDK8里面的集合类源码（特别是Map相关和List相关的实现）。 类似的，在Golang里面，built-in类型的几种经典数据结构（或者说是容器／集合）有 channel map slice array 对于array，没什么好说的，各语言中都一样，就是一片 连续的 可通过index快速定位的 存储相同数据类型 的内存区域。 array相当的方便简单好用，但是最大的问题就是严重缺乏动态性，在array的领域里你很难看到随意扩容或者随意缩减。 于是，在其他部分语言里（如Java），就提供了很多基于LinkedList构建的容器，根据需求随意动态扩容缩减、头插入尾插入、随意删除中间元素，这种动态性用起来非常爽。但是却又失去了数组那种连续空间所支持的最爽的一点 —— 用index直接定位到具体的元素（API上支持，但实际上还是一路遍历过去）。 所以，基于这些问题和背景，在Golang中我们就见到了一个“奇葩”的数据结构 —— slice（切片）。 底层实现 代码层面 根据官方的说法，或者很多Go开发者的说法，slice最直接的定义就是dynamic array。所以其实它的底层真的是基于array实现的。同时它还能支持一定程度上比较良好的动态性。 先来看看Go 1.11中的slice源码。 type slice struct &#123; array unsafe.Pointer len int cap int&#125; 看到了这个数据结构的定义，瞬间让我想起了Redis里面的Simple Dynamic String(SDS)的实现。同样是封装了一个底层数组（此处array指针指向的是一片连续内存区域），同样是维护了一个len属性，同样有一个cap（free）属性来指明容器的使用情况。（感兴趣的可以去查阅一下Redis里的字符串（SDS）实现） 解释一下结构体里各成员的含义： array unsafe.Pointer：一个指向连续内存区域（数组）的指针 len int：此slice中已存了多少个元素 cap int：此slice中最多能存多少个元素（最大容量） 逻辑层面 假设我们新建了一个slice slice := make([]string, 4, 4)，然后赋值 或者slice := []string{&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;} 将会得到如下图所示的逻辑结构，len = 4，cap = 4。 若我们不是从头新建，而是从某个已有的array中建出一个slice strs := [4]string&#123;"a", "b", "c", "d"&#125;slice := strs[0:2] // 取strs下标为[0,2)的元素构造一个slice 将会得到如下图所示的逻辑结构，len = 2，cap = 4（因为原数组的最大空间为4，所以这个slice从index0开始，最大可用区域也可以到达index3，只不过现在还没用到index3罢了）。 所以，基于底层的数组，本质上slice不过是一个数组的wrapper（包装类），并提供一些动态性的操作（append()），让使用者感受不到它是一个数组罢了。 对slice的操作，以及其中的坑对slice操作，无非就5种： 新建（无需多讲） 直接新建 从已有的数组中截取 插入 截断（所谓的删除）-&gt; 用index截取 更新（无需多讲） 读取（无须多讲） 首先明确第一个坑，在同一个数组上构造出来的多个slice都是共享同一个底层数组的。所以你对某一个slice进行写或者更新操作后，很有可能就会影响到基于同一个底层数组的其他slice。详见下述代码与示意图。 strs := [4]string&#123;"a", "b", "c", "d"&#125;slice1 := strs[0:2] // 取strs下标为[0,2)的元素构造一个slice1slice2 := strs[0:3] // 取strs下标为[0,3)的元素构造一个slice2slice2[0] = "hhhhh"fmt.Println(slice1) // [hhhhh, b] slice1被改变了 由此可见，当你的代码里有多个slice都是基于同一个数组构建出来时，务必小心操作，很容易互相影响造成脏数据！ 接下来着重讲讲插入（append()）操作，以及其中奇葩的几个点。 一般执行append(slice, value)时会有以下两种情况 slice的len小于cap，顺理成章可以直接插入（注意会直接改变底层数组!） slice的len等于cap，当前可用空间不足 len小于cap strs := [4]string&#123;"a", "b", "c", "d"&#125;slice1 := strs[0:2] // 取strs下标为[0,2)的元素构造一个slice1, len=2, cap=4slice1 = append(slice1, "hh") // append后len=3, cap=4 如上图所示，在append了&quot;hh&quot;之后，底层数组中index2处直接被更新为了&quot;hh&quot;。 len等于cap strs := [4]string&#123;"a", "b", "c", "d"&#125;slice1 := strs[0:1:1] // 取strs下标为[0,1)的元素构造一个slice1, len=1, cap=1// 为了说明问题, 此处用特殊的写法slice2 := append(slice1, "hh")fmt.Println(len(slice2), cap(slice2)) // len=2, cap=2fmt.Println(len(slice1), cap(slice1)) // len=1, cap=1// 关键点在此slice2[0] = "hello"fmt.Println(slice1) // [a] 当执行了slice2[0] = &quot;hello&quot;后，slice1居然还是[a]而不是[hello]，说好的基于同一个底层数组的多个slice会共享该数组呢？？？在这里怎么出问题了？？？ 其实关键就在于我们append时，slice1的len == cap，在原slice1中没有空间可以允许插入新元素了。此时就会触发slice扩容，然而此扩容并不是对原底层数组进行操作，而是新开辟一段更长的数组空间，把原数组的值copy过来，再让slice中的指针指向新开辟的长数组！详细流程请参考下图。 由图可见，原本slice1里的指针指向的数组是位于0x34处的。当尝试append()并触发扩容时，底层直接在另外的地址(0x88）处开辟了（或者称之为deep clone吧）一段全新的数组。并把值都copy过去，然后才真正把需要append的&quot;hh&quot;塞进去。 很自然，之后我们执行的slice2[0] = &quot;hello&quot;，只是对0x88起始的那个数组进行了修改，完全没有影响到slice1指向的那个从0x34起始的数组。把slice1打印出来，自然也就还是[a]了。 所以在很多Go项目的代码里，或者官方教程里，最常见的append用法都是slice = append(slice, value)，而不是newSlice := append(slice, value)。当把append完成后的结果再一次赋给原slice，无论发生了什么(无论len和cap是什么关系都无所谓），总不会造成runtime时的歧义。 如果在一些特定的场景下，必须要使用newSlice := append(slice, value)的写法，那就要格外小心！如果在append过程中触发了扩容，newSlice和原slice将不再指向同一段数组空间，在这种情况下对newSlice的修改也丝毫不会影响slice（也有可能这恰好就是你的逻辑需要的）。 总结Golang中的slice兼顾了传统数组的优点（连续内存空间），也支持动态扩容／截断，确实是在实际开发中非常有用的容器。但建议每个Go开发者都详细了解其底层实现，因为为了支持其动态性，append和append相关的操作会带来很多奇奇怪怪的坑]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>笔记</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Google]记一道不错的电面题]]></title>
    <url>%2F2018%2F09%2F28%2Fgoogle-interview0%2F</url>
    <content type="text"><![CDATA[最近一位朋友跟我分享了一道Ta在google电面中被问到的算法题，一阵研究后觉得题目出得很不错，不是那种傻X的tricky题目，故分享之，并附上我实现的Java代码。 题目根据输入的有序字符串，构造出目标字符串，保证每个输入的串均服从目标串的原顺序。 target: 3-5-7-1-9 input: 3-5-7, 5-1-9, 7-1, 1-9, 5-7 可以看到输入的每个字符串均服从目标串的顺序，只不过每个输入串都缺失了一些元素。本题的前提条件： 保证根据input能够生成唯一的目标串 target中的元素取值范围是[1, 9]，且不重复 思路拿到题目后，最直观的思路是：每处理一个输入串，把里面的数字抽取出来，建立一个ListNode，然后根据该串中的先后顺序把多个ListNode连接起来。同时维护一个Map记录已经遇到过的ListNode，之后再遇到该数字就不用新建，而是直接从Map中取出数字对应的ListNode。 例如，先建立一个Map&lt;Integer, ListNode&gt;，然后开始处理第一个输入串3-5-7， 先遇到3，Map中没有Node3，建立Node3 然后遇到5，Map中没有Node5，建立Node5，然后让Node3的next指向Node5 最后遇到7，Map中没有Node7，建立Node7，然后让Node5的next指向Node7 接着处理下一个输入串，循环 大多数人第一反应都会想到这样的处理逻辑，非常直观。但是这样的逻辑存在细节问题。因为每个输入串只是保留了target的相对顺序，中间缺失了一些元素，所以在构建链表时，凭借局部相对顺序直接插入ListNode可能会造成全局顺序的错误。例如在得到3-&gt;5-&gt;7这个链表后，再处理第二个输入串5-1-9，根据以上的逻辑就会把Node1直接插到Node5身后，得到3-&gt;5-&gt;1-&gt;9-&gt;7,显然是不对的，主要原因就是局部的相对顺序不一定能直接映射全局顺序。 所以，这道题的核心就在于记录顺序，并且是全局顺序。由之前的想法拓展开来，我们已经有了为每个数字建立对应的ListNode的想法，那么这个Node是否可以是一个GraphNode呢？我们既然可以用LinkedList存这些数据，是否也可以拓展成用Graph存这些数据呢？ 如果我们用Graph存储这些数据，那么在Graph中能体现顺序的无非就是有向边（若存在边A ---&gt; B，代表得先到达A才能到达B）。有向边 + 点，我们就得到了有向图，结合条件中说明的target中不存在重复元素，我们就得到了有向无环图（DAG）。那么在DAG中，能找出所谓的全局顺序的，就是Topological Sort。 具体代码package google;import java.util.*;public class Solution &#123; public static void main(String[] args) &#123; Solution solution = new Solution(); List&lt;String&gt; stream = new ArrayList&lt;&gt;(); stream.add("3-5-7"); stream.add("3-1"); stream.add("5-1-9"); stream.add("7-1"); stream.add("1-9"); solution.reconstruct(stream); &#125; /** * Reconstruct an ordered sequence from the given stream. * @param stream stream of sequences */ public void reconstruct(List&lt;String&gt; stream) &#123; if (stream == null || stream.size() == 0) &#123; return; &#125; // 1. extract nodes, edges from the input stream Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); // mapping between node_num and node_index Map&lt;Integer, String&gt; outputMapping = new HashMap&lt;&gt;(); // mapping between node_index and output_str List&lt;Integer[]&gt; edges = new ArrayList&lt;&gt;(); // record the edges [A, B] means an edge pointing from A to B int nodeIndex = 0; for (String s : stream) &#123; String[] nums = s.split("-"); for (int i = 0;i &lt; nums.length;i++) &#123; int nodeNum = Integer.parseInt(nums[i]); if (!map.containsKey(nodeNum)) &#123; map.put(nodeNum, nodeIndex++); outputMapping.put(nodeIndex - 1, nums[i]); &#125; if (i != nums.length - 1) &#123; edges.add(new Integer[]&#123;nodeNum, Integer.parseInt(nums[i + 1])&#125;); &#125; &#125; &#125; // 2. construct the graph structure int nodeCount = map.keySet().size(); boolean[][] graph = new boolean[nodeCount][nodeCount]; // adjacency matrix int[] indegrees = new int[nodeCount]; // record the indegree of each node for (Integer[] edge : edges) &#123; int from = edge[0]; int to = edge[1]; if (!graph[map.get(from)][map.get(to)]) &#123; // avoid duplicated edges in the input stream graph[map.get(from)][map.get(to)] = true; indegrees[map.get(to)]++; &#125; &#125; // 3. finding out nodes with 0 indegree Queue&lt;Integer&gt; queue = new LinkedList&lt;&gt;(); StringBuilder result = new StringBuilder(); for (int i = 0;i &lt; indegrees.length;i++) &#123; if (indegrees[i] == 0) &#123; queue.add(i); result.append(outputMapping.get(i) + "-"); &#125; &#125; // 4. topological sort while (!queue.isEmpty()) &#123; int index = queue.poll(); for (int j = 0;j &lt; graph[index].length;j++) &#123; if (graph[index][j]) &#123; graph[index][j] = false; indegrees[j]--; if (indegrees[j] == 0) &#123; queue.add(j); result.append(outputMapping.get(j) + "-"); &#125; &#125; &#125; &#125; System.out.println(result.substring(0, result.length() - 1)); &#125;&#125; 解析在上述代码中，我们主要做了以下几件事情 处理每一个输入串，将其内部的数字解析出来，并为每个数字“生成”一个对应的Node和其NodeIndex。 根据第一步得到的各个Node和其NodeIndex，结合输入串，“生成”各点之间的有向边，即我们的图结构，此处用adjacency matrix实现。 开始执行topological sort的逻辑，先构建indegree数组，找到入度为0的点。 基于入度为0的点，利用Queue实现topological sort。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>Java</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《推荐系统实践》笔记 #4]]></title>
    <url>%2F2018%2F06%2F01%2Frecsys-note4%2F</url>
    <content type="text"><![CDATA[本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 利用上下文信息进行推荐在推荐时，应该考虑时间，地点，心情…… 时间上下文信息 时间效应 用户兴趣持续变化 物品本身有生命周期 季节／节日效应（圣诞节，奥斯卡……） 时间上下文推荐算法 最近最热门算法 结合时间上下文的Item-based CF 用户在相隔时间很短内喜欢的物品具有更高相似度，近期行为相比很久之前的行为更能体现用户现在的兴趣，应加强用户近期行为的权重。 结合时间上下文的User-based CF 时间段图模型 地点上下文信息 不同地区的用户兴趣有所不同（距离很重要） 基于位置的推荐算法 物品／用户均可分为有空间属性的和无空间属性的，例如餐馆／商店带有空间属性，图书／电影则没有。 数据结构：(user, user_location, item, item_location, rating) 可将位置信息数据集划分成树状结构 只有user_location时，对于给定的一个user_location，将其分配到某个叶子节点中，利用该叶子节点上的用户行为给用户作推荐。缺点是叶子节点上的用户数量&amp;用户行为数据较少，推荐只是基于局部。所以可以从root出发，到叶子的路径中利用每个中间节点训练出一个推荐模型，最终的推荐模型是一系列的中间推荐列表加权而成。 只有item_location时，先忽略物品位置信息，直接用Item-based CF计算出p(u, i)，最后再引入距离代价作惩罚。 利用社交网络数据 获取社交网络数据的途径 e-mail 用户注册信息 用户的位置数据 论坛 &amp; 讨论组 即时聊天工具 社交网站 社交网络数据简介 图结构G = (V, E, W), 其中V是用户节点，E是用户关系边，W是边权重 单向关注 =&gt; 有向图（如Twitter）；互相关注 =&gt; 无向图（如Facebook）；基于社区关系，无明确方向 基于社交网络的推荐 基于邻域的社会化推荐算法 熟悉程度 = 双方共同好友的比例 相似度 = User-based CF中余弦相似度 优化： 不需要取所有的好友进行计算，只取相似度高的前K个 只取最近短时间内发生的操作记录进行计算 基于图的社会化推荐 结合社交网络图 &amp; 用户物品二分图（也可加入社区的顶点） 用户与用户之间的权重 = a * (相似度 + 熟悉度) 用户与物品之间的权重 = b * (用户对物品的喜爱程度) 通过调节a和b参数确定哪部分对系统影响较大 信息流（Feed）推荐 帮助用户从信息墙上挑选有用的信息，综合考虑信息流中每个会话的长度、时间、用户兴趣间的相似度等。 好友推荐系统 基于内容的匹配 给用户推荐和他们有相似内容属性的用户（人口统计学属性、用户兴趣、用户位置……） 基于共同兴趣的好友推荐 利用User-based CF计算用户之间的兴趣相似度 基于社交网络图 推荐好友的好友 =&gt; 推荐熟悉的好友的好友 算法时间复杂度不高，适合在线应用]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>推荐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《推荐系统实践》笔记 #3]]></title>
    <url>%2F2018%2F05%2F24%2Frecsys-note3%2F</url>
    <content type="text"><![CDATA[推荐系统冷启动问题如何在没有大量用户数据的情况下设计推荐系统 =&gt; 冷启动问题 冷启动问题 用户冷启动 =&gt; 如何给全新的用户作推荐 物品冷启动 =&gt; 如何把新加入的物品推荐给用户 系统冷启动 =&gt; 如何在一个新开发的网站上设计开发个性化推荐系统 常见的冷启动解决方案 作非个性化的推荐：直接按热门排行榜进行推荐 利用用户注册时提供的个人信息：age, sex…… 利用用户的社交网络信息：Facebook, 微博…… 用户注册／登陆时先让其对一些给定物品进行反馈，采集其兴趣爱好 对于新物品，可从内容相似性的方向进行推荐，不一定只考虑行为相似性 事先引入、建立专家知识库，建立物品相关度表 详细解决方案 利用用户注册信息 sex, age, DOB, job, ethnic, edu, location等人口统计学信息 &amp; 用户兴趣描述… 推荐流程 获取用户注册信息 根据信息给用户分类 推荐其所属分类中用户喜欢的物品 理论依据 p(f, i)为物品i在f特征人群中受喜爱的程度。 .png) 启动初期先让用户给部分物品评分 待评分的物品要 比较热门 具有代表性 &amp; 区分性 具有多样性 可用一个Decision Tree来选择启动评分物品集合 利用物品的内容属性 在Item-based CF中，新item加入时，不会立刻更新物品相关性矩阵，因为计算耗时特别大，所以要利用物品的内容属性进行冷启动推荐。 对于文本数据而言，计算内容相似度前，需要利用NLP相关技术转化为向量（keyword vector），但向量空间模型丢失了多个keyword之间的关联和位置信息。而且很多时候，两篇文本没有（或很少）直接相同的关键词，但是主题却高度相关。在这种情况下，可使用LDA（Latent Dirichlet Allocation）来挖掘文本主题。 LDA核心元素 文档D: D[i]代表文档集合中第i篇文档 话题Z: Z[i][j]代表i文档中j词所属的话题 词语W: W[i][j]代表i文档中的j词 计算步骤 先利用LDA挖掘出两篇文本的话题分布 再通过KL-Divergence（KL散度）比较两个分布的相似度 利用专家的作用 =&gt; 让专家手动／半手动地打标签 利用用户标签数据 UGC = User Generated Content 用户生成数据，即让普通用户给物品打标签。 标签系统的推荐问题 如何利用用户打标签的行为为其推荐物品 如何在用户给物品打标签时为其推荐适合该物品的标签 用户标签行为 数据结构：behavior = (u, i, b), u = 用户，i = 物品，b = 标签。 基于标签的简单推荐算法 统计每个用户最常用的标签 对于每个常用标签，统计被打过该标签次数最多的物品 基于邻域的标签扩展 若两个标签同时出现在很多物品的标签集合中，这两个标签就具有较大的相似度，所以可以基于邻域原理计算出相似标签。 标签清理 去除词频很高的stopword 去除因词根不同造成的同义词 去除因分隔符造成的同义词 让用户主动反馈不合适的词 基于图的标签推荐算法 数据结构：V = {V(u), V(i), V(b)}，V(u)代表用户顶点，V(i)代表物品顶点，V(b)代表标签顶点。 建立图结构之后，可用PersonalRank算法进行随机游走。 给用户推荐标签 为什么要给用户推荐标签 方便用户输入标签，降低用户打标签的难度 提高标签质量，减少冗余的同义词 如何给用户推荐标签 直接推荐整个系统里最热门的标签 给用户推荐物品i上的最热门标签item[i][b] 给用户推荐他自己常用的标签 融合以上两个方法，进行线性加权 或者基于图作标签推荐]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>推荐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《推荐系统实践》笔记 #2]]></title>
    <url>%2F2018%2F05%2F19%2Frecsys-note2%2F</url>
    <content type="text"><![CDATA[本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 利用用户行为数据 用户行为数据类别 session log impression log click log 用户行为类型 显性反馈 => 明确选择喜欢 or 不喜欢，数量较少 隐性反馈 => 没有明确选择，多数为浏览 or 点击，数据极多 用户行为记录的数据结构 item remark user_id 用户id item_id 物品id behavior_type 行为类型（购买／浏览／点赞／点灭。。。） context 上下文信息（时间／地点。。。） behavior_weight 权重（视频观看时长／文章评分。。。） behavior_content 内容（评论的文本／打标签中的标签。。。） 基于用户行为数据的算法 用户协同过滤算法（User-based CF） 物品协同过滤算法（Item-based CF） 用户协同过滤算法 算法步骤 找到和目标用户相似的用户集合U 找到U中每个用户u喜欢的物品i，且当前目标用户未对i有过任何行为 利用余弦相似度计算用户间的相似度 三个中间结果矩阵 W[][]：W[u][v]代表了用户u和用户v间的相似度 C[][]：C[u][v]等于|N(u)|∩|N(v)| N[]：N[u]等于用户u发生过行为的物品数 优化改进 为了提高算法效率，其实当|N(u)|∩|N(v)| = 0的时候（即两个用户间没有任何交集），压根不用去计算u和v间的相似度。所以可以建立物品-用户倒排表，没有交集的用户绝对不会出现在同一个物品所属的链中。 从理论上讲，应该多考虑冷门物品的贡献度，适当惩罚热门物品带来的相关性，因为热门物品可能每个人都会买，不应该带来太多的个性化相关性。所以要对原始的公式引入惩罚机制。 物品协同过滤算法 物品相似度并不是利用物品的内容属性计算其相似度，而是从用户行为记录的角度计算 算法步骤 计算物品间的相似度（矩阵） 根据相似度矩阵和用户的历史行为记录生成推荐列表 三个中间结果矩阵 W[][]：W[i][j]代表了物品i和物品j间的相似度 C[][]：C[i][j]等于|N(i)|∩|N(j)|, 就是同时对i和j发生过行为的用户数 N[]：N[i]等于对物品i发生过行为的用户数 算法优点 能够提供推荐解释，利用用户历史上喜欢的物品为现在的推荐结果进行解释。不像User-based CF那样无法提供合理的解释。 优化改进 引入Inverse User Frequency对活跃用户进行惩罚 =&gt; 活跃用户对物品相似度的贡献应小于非活跃用户 在实际生产环境中，可直接忽略特别大的兴趣列表，提高算法效率。 将相似度矩阵按行归一化 Example: A类类内物品相似度0.5，B类类内物品相似度0.6，AB类间物品相似度0.2。由于B类类内物品相似度最高，所以推荐10个物品时10个都会是B类物品。进行归一化后，A类与B类类内物品相似度均为1，推荐10个物品时会有5个A物品&amp;5个B物品 User-based CF与Item-based CF综合比较 User-based CF着重于反映和用户兴趣相似的小群体的热点 Item-based CF着重于维护目标用户的历史兴趣 User-based CF维护用户相似度矩阵，Item-based CF维护物品相似度矩阵。需要考虑数据存储的代价和矩阵计算的代价 => 用户数多 or 物品数多 哈利波特问题 《哈利波特》很热门，几乎买了任何书的人都会去买《哈利波特》，所以对热门物品要进行惩罚。 通过提高alpha的取值（[0.5, 1.0]），可惩罚热门物品。 隐语义模型（Latent Factor Model) 模型背景 仅靠用户行为数据无法解决跨领域的问题，例如很多人看完7点的新闻联播会继续开着电视看8点的电视剧，但给看了电视剧的人推荐新闻联播显然是不合理的。两个不同领域的最热门物品间往往会存在较高的相似度，不是因为它们真的相似，只是因为它们都很热门，所以大家都会看。 需要解决的问题 隐语义模型（Latent Factor Model) => 基于用户行为数据的聚类，解决 如何给物品分类（基于用户行为，而不是内容属性） 如何确定用户对哪些类的物品感兴趣，以及感兴趣的程度 对于一个给定的泪，选择其中哪些物品作推荐，推荐的权重是多少？ 理论 后面带lambda的两项为正则项，防止过拟合。 使用（随机）梯度下降，最小化损失函数C，计算得到p和q。 关键参数 隐特征个数F(或者叫K) 梯度下降的步长／学习速率alpha 正则化参数lambda 正／负样本的比例ratio 常见问题 隐语义模型（Latent Factor Model)在显式评分数据集上表现很好，而对于隐式评分数据集而言重点在于如何生成负样本。生成负样本的原则有 对于每个用户，要保证正／负样本的平衡（相近或相等） 将原数据集中非常热门，但用户却没有过行为的物品当作负样本 => 不将冷门物品作为负样本是因为用户可能压根没有发现冷门物品，而不是对冷门物品不感兴趣。 模型特点 隐语义模型（Latent Factor Model)有较好的理论基础，是一种学习方法，有学习过程 中间结果的存储空间只需要O(F * (M + N))，（F为隐特征个数，M为用户数，N为物品数），而协同过滤则需要O(N * N) 时间复杂度为O(K * F *S)，（F为隐特征个数，K为行为记录数，S为迭代次数） 不适合用于实时推荐，用户发生了新行为后，推荐列表不会发生变化 很难像Item-based CF一样用自然语言解释推荐原因 基于图的推荐模型 用户行为容易用图结构表示，G = (V, E)，V = V(u) ∩ V(i)。用户和物品均是图中的顶点，若用户u对物品i发生过行为，则存在边e(u, i)。 所以给用户u推荐物品 = 找到跟V(u)无直接边相连的顶点中相关性最高的顶点。 顶点两两间的相关性取决于 两点间的路径数 两点间路径的长度 两点间路径所经过的点 如果两点间有很多条不同路径／两点间每条路径的长度较短／两点间的路径不会经过出度较大的点，那么它们的相关性相对而言就比较高。 PersonalRank算法，采用了随机游走的概念，图中每个物品顶点V(i)被访问的概率PR(v(i))即为该物品i最后在推荐列表中的权重。]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>推荐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《推荐系统实践》笔记 #1]]></title>
    <url>%2F2018%2F05%2F12%2Frecsys-note1%2F</url>
    <content type="text"><![CDATA[本文100%由本人（Haoxiang Ma)原创，如需转载请注明出处 概念与定义 推荐系统是一个复合系统，用于将User和Item互相关联，给User推荐“合适的”Item，同时给Item找到潜在的买家／用户。系统起源于信息过载问题，随着互联网信息爆炸性增长，用户没有办法短时间内接受完所有信息。 一般情况下，推荐系统的结构为 前端页面 后端服务（日志系统） 推荐算法系统 如何评价一个推荐系统的优劣？要从多个角度入手进行考察 离线实验 首先从海量日志文件中收集、清洗所需的用户数据，生成标准数据集 将数据集随机分成训练集 &amp; 测试集 用训练集训练出兴趣模型，在测试集上进行预测测试 建立一个评价公式去评估测试结果 用户调查（问卷） 设计问卷，按照一定的规则选取用户进行调查 成本较高，难以设计 在线实验（A/B Test） 将用户随机／按照特定规则分成K组，在每组用户上各自应用不同的算法，然后比较不同组用户的评价指标（点击率，转化率等等） 需要在前端的流量入口就将用户分组，并打上组别标签，各组分别收集日志 设计新系统时需要结合以上3种方法 用离线实验证明各个离线指标优于当前系统 用用户调查（问卷）证明用户满意度高于当前系统 用在线实验（A/B Test）确定商业指标&amp;其他所关心的指标上优于当前系统 评价指标 用户满意度 => 通过问卷调查或前端反馈页面 预测准确度 => 通过离线实验 评分预测型系统（预测用户给他没有见过的Item打多少分） RMSE（均方根误差） MAE（平均绝对误差） TopN推荐型系统（给用户推荐N个他没见过的Item) Precision（准确率） 简单来说就是推荐出来的结果中有多少个是当前用户真正感兴趣的 Recall（召回率） 简单来说就是有多少当前用户真正感兴趣的物品被成功推荐出来 为了全面一点，可以对TopN中的N取多个值，绘制出一条p/r curve 在实际生产环境中更多会使用TopN模型，因为用户给一个Item打高分不等于他很想购买／阅读该物品 覆盖率 => 系统能够推荐出来的物品占总物品集合的比例 多样性 推荐结果列表中物品两两之间的不相似性 系统总体的不相似性 新颖性 => 给用户推荐他们从来未听说过的，千万不能推荐他们已经看过／购买过的 惊喜度 信任度 实时性 健壮性 商业目标]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>推荐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Leetcode题解] - Construct Binary Tree from XX-Order]]></title>
    <url>%2F2018%2F04%2F22%2Fleetcode-105106%2F</url>
    <content type="text"><![CDATA[本文行文由本人(Haoxiang Ma)原创，思路借鉴了Leetcode高票答案，加以个人分析，实现，总结。如需转载请注明出处。 题目背景 [Leetcode 105] Construct Binary Tree from Preorder and Inorder Traversal Given preorder and inorder traversal of a tree, construct the binary tree.Note:You may assume that duplicates do not exist in the tree.For example, givenpreorder = [3,9,20,15,7]inorder = [9,3,15,20,7]Return the following binary tree: 3 / \ 9 20 / \ 15 7 简单来说，就是给定二叉树的先序遍历结果+二叉树的中序遍历结果，重构二叉树。 [Leetcode 106] Construct Binary Tree from Inorder and Postorder Traversal Given inorder and postorder traversal of a tree, construct the binary tree.Note:You may assume that duplicates do not exist in the tree.For example, giveninorder = [9,3,15,20,7]postorder = [9,15,7,20,3]Return the following binary tree: 3 / \ 9 20 / \ 15 7 简单来说，就是给定二叉树的中序遍历结果+二叉树的后序遍历结果，重构二叉树。 思路分析105和106两题的题目背景类似，都是给定某两种二叉树的遍历结果，要求重构二叉树，且两题均提供了中序遍历结果。 首先，对题例中的二叉树结构以及其对应的三种遍历结果进行对比和分析。 3 / \ 9 20 / \ 15 7 preorder = [3,9,20,15,7]inorder = [9,3,15,20,7]postorder = [9,15,7,20,3] 仔细观察以上三种遍历结果，不难发现以下规律： 从左到右遍历preorder序列，对于每一个元素，均能在inorder序列中找到。且该元素在二叉树中的左子树中的所有元素都在inorder序列里该元素的左边，该元素在二叉树中的右子树中的所有元素都在inorder序列里该元素的右边。例如对于3，其在二叉树中左子树里有9，正好在inorder序列里9在3的左边；其右子树里有20,15,7，正好在inorder序列里20,15,7都在3的右边。 从右到左遍历postorder序列，对于每一个元素，均能在inorder序列中找到。且该元素在二叉树中的左子树中的所有元素都在inorder序列里该元素的左边，该元素在二叉树中的右子树中的所有元素都在inorder序列里该元素的右边。例如对于20，其在二叉树中左子树里有15，正好在inorder序列里15在20的左边；其右子树里有7，正好在inorder序列里7在20的右边。 根据以上观察到的规律，我们可以大致得出以下思路： 对于105这道题，给定preorder和inorder，我们可以从左到右遍历preorder序列，对于每一个元素Ei，我们： 选定Ei，构造当前节点。 在inorder序列中找到Ei，假定其index为j。 在inorder序列中，从inorder_start到j-1的元素肯定在Ei的左子树里，从j+1到inorder_end的元素肯定在Ei的右子树里。 继续递归构造左子树和右子树 对于106这道题，给定postorder和inorder，我们可以从右到左遍历postorder序列，对于每一个元素Ei，我们： 选定Ei，构造当前节点。 在inorder序列中找到Ei，假定其index为j。 在inorder序列中，从inorder_start到j-1的元素肯定在Ei的左子树里，从j+1到inorder_end的元素肯定在Ei的右子树里。 继续递归构造左子树和右子树 可以看到，105和106的思路基本一致，差别仅仅在于preorder和postorder本身的性质差异，一个先访问root，另一个后访问root。所以对于preorder序列我们要从左到右遍历，而对于postorder我们要从右到左遍历。 具体代码 [Leetcode 105] Construct Binary Tree from Preorder and Inorder class Solution &#123; public TreeNode buildTree(int[] preorder, int[] inorder) &#123; return build(preorder, inorder, 0, preorder.length - 1, 0, inorder.length - 1); &#125; public TreeNode build(int[] pre, int[] in, int preStart, int preEnd, int inStart, int inEnd) &#123; if(pre == null || in == null || preStart &gt; preEnd || inStart &gt; inEnd) &#123; // corner case return null; &#125; // select current root element int curVal = pre[preStart]; TreeNode cur = new TreeNode(curVal); // search for current root element in inorder sequence int index = -1; for(int i = inStart;i &lt;= inEnd;i++) &#123; if(in[i] == curVal) &#123; index = i; break; &#125; &#125; if(index == -1) &#123; // invalid sequence return null; &#125; // count how many elements are in left &amp; right subtree int left = index - inStart; int right = inEnd - index; // build recursively cur.left = build(pre, in, preStart + 1, preStart + left, inStart, index - 1); cur.right = build(pre, in, preStart + left + 1, preEnd, index + 1, inEnd); return cur; &#125;&#125; [Leetcode 106] Construct Binary Tree from Inorder and Postorder class Solution &#123; public TreeNode buildTree(int[] inorder, int[] postorder) &#123; return build(inorder, postorder, 0, inorder.length - 1, 0, postorder.length - 1); &#125; public TreeNode build(int[] in, int[] post, int inStart, int inEnd, int postStart, int postEnd) &#123; if(in == null || post == null || inStart &gt; inEnd || postStart &gt; postEnd) &#123; // corner case return null; &#125; // select current root element int curVal = pre[preStart]; TreeNode cur = new TreeNode(curVal); // search for current root element in inorder sequence int index = -1; for(int i = inStart;i &lt;= inEnd;i++) &#123; if(in[i] == curVal) &#123; index = i; break; &#125; &#125; if(index == -1) &#123; // invalid sequence return null; &#125; // count how many elements are in left &amp; right subtree int left = index - inStart; int right = inEnd - index; // build recursively cur.left = build(in, post, inStart, index - 1, postStart, postStart + left - 1); cur.right = build(in, post, index + 1, inEnd, postStart + left, postEnd - 1); return cur; &#125;&#125; 总结对于这两道题，需要注意以下几点 preorder序列里root排在前面，postorder序列里root排在后面。 从数组中重构二叉树，往往就是类似于先序遍历的递归写法，先在数组中定位到当前节点的值，构造好当前节点，接着确定好数组边界，递归构造左子树 &amp; 右子树。 类似的构造二叉树题目，可参考 [Leetcode 108] Convert Sorted Array to Binary Search Tree 同样是从数组中重构二叉树。]]></content>
      <categories>
        <category>Leetcode</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>算法</tag>
        <tag>Leetcode</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Leetcode题解] - Unique BST I && II]]></title>
    <url>%2F2018%2F04%2F05%2Flc-uniquebstiii%2F</url>
    <content type="text"><![CDATA[本文行文由本人(Haoxiang Ma)原创，思路借鉴了Leetcode高票答案，加以个人分析与总结。如需转载请注明出处。 题目 Unique Binary Search Tree II Given an integer n, generate all structurally unique BST&apos;s (binary search trees) that store values 1...n.For example,Given n = 3, your program should return all 5 unique BST&apos;s shown below. 1 3 3 2 1 \ / / / \ \ 3 2 1 1 3 2 / / \ \ 2 1 2 3 即给定一个正整数n，求出1 ~ n能构成的所有二叉搜索树，返回所有可能的根节点。 Unique Binary Search Trees Given n, how many structurally unique BST&apos;s (binary search trees) that store values 1...n?For example,Given n = 3, there are a total of 5 unique BST&apos;s. 1 3 3 2 1 \ / / / \ \ 3 2 1 1 3 2 / / \ \ 2 1 2 3 即给定一个正整数n，求出1 ~ n能构成的所有二叉搜索树的总棵数，返回总棵数。 分析以上两道题殊途同归，输入均为一个正整数n，一个是要生成所有合法的二叉搜索树，另一个则想求出合法二叉搜索树的总棵树。 假如我们有办法基于1 ~ n生成所有合法的BST，那么不管是题1还是题2均能被轻松解决了。 思路主要有以下两点： 凡是遇到求所有，全部的题目，基本可参考DFS的思路。因为DFS就是在解集构成的图里不断走走走，一路走到黑，遇到满足条件的就塞进结果里，遇到死路就往回走试试别的路。自然就能把整个解集（所有可能性）给遍历完，所有符合条件的解也就都被找到了。 既然让我们生成BST，那我们就参考参考BST的特性。用最通俗易懂的话来讲，BST中的每个节点，其左子树里所有子孙的值均小于它，其右子树里所有子孙的值均大于它。 基于以上两点，我们进行DFS的流程就很清晰了。 在1 ~ n中，我们随便挑一个数k作为当前的root，k可以是1 ~ n中的任意一个，为了生成所有可能性，我们可用一个for循环对所有可能的取值进行遍历。此外，因为以上提及的BST的性质，所以比k小的1 ~ k-1都要被放到左子树，比k大的k+1 ~ n都要被放到右子树。即到了构建左子树的时候，能用的数字就是1 ~ k-1，到了构建右子树的时候，能用的数字就是k+1 ~ n。一直这样DFS走下去构建左子树和右子树，直到没有任何可用的数字，就返回null。 详细代码/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public List&lt;TreeNode&gt; generateTrees(int n) &#123; // 处理corner case if(n &lt;= 0) &#123; return new ArrayList&lt;&gt;(); &#125; return dfs(1, n); &#125; public List&lt;TreeNode&gt; dfs(int start, int end) &#123; List&lt;TreeNode&gt; result = new LinkedList&lt;&gt;(); if(start &gt; end) &#123; result.add(null); return result; &#125; // 对于当前可选的数字start ~ end，其中任意一个都能选为当前的根节点 for(int i = start;i &lt;= end;i++) &#123; // 生成所有可能的左子树(start ~ i-1)和右子树(i+1 ~ end) List&lt;TreeNode&gt; leftNodes = dfs(start, i - 1); List&lt;TreeNode&gt; rightNodes = dfs(i + 1, end); // 对于左子树的根节点的所有可能性和右子树的根节点的所有可能性， // 作一个笛卡尔积，求出所有组合的可能性 for(TreeNode left : leftNodes) &#123; for(TreeNode right : rightNodes) &#123; TreeNode cur = new TreeNode(i); cur.left = left; cur.right = right; result.add(cur); &#125; &#125; &#125; return result; &#125;&#125; 优化基于以上DFS思路的代码，我们可以用1 ~ n生成所有合法的BST，解决了问题。 如果需要返回所有可能的BST，直接返回最终的List&lt;TreeNode&gt;即可。 如需返回总棵数，返回最终生成的List的size即可。 ⚠️但是，我们能不能再做得更好一点呢？像题目2中只要求出一个总棵数，一个正整数而已，我们需要如此“大兴土木”地把所有合法的BST构造出来，最后“轻描淡写”地返回List的size吗？是否存在一些数学规律，能够让我们直接求出基于1 ~ n的合法BST的总棵数呢？ 首先我们可以尝试将问题公式化。 我们用Sum(n)表示n个连续数能构成的合法BST总棵数；用F(k, n)表示给定n个连续数时，以k为根的合法BST的棵数。显然我们能够得到以下的推导式： // 给定n个连续数，// 能够构成的合法BST总棵数 = 以1为根的BST棵数 + 以2为根的BST棵数 + ... + 以n为根的BST棵数。 Sum(n) = F(1, n) + F(2, n) + F(3, n) + ... + F(n, n) (1) 同时，以k为根的合法BST的棵数：F(k, n)又等于多少呢？很简单，以k为根，根据BST的性质，左子树里只能存放1 ~ k-1这k-1个连续数，右子树里只能存放k+1 ~ n这n-k个连续数。所以F(k, n)取决于其左子树的可能性 * 其右子树的可能性： F(k, n) = Sum(k-1) * Sum(n-k) (2) 基于以上(1)和(2)推导式，我们可以改写得到： `// Corner Case: Sum(0) = 1 因为0个数只能生成空树，空树永远只有1种// Corner Case: Sum(1) = 1 因为1个数只能生成单个节点的树，也永远只有1种 Sum(n) = F(1, n) + F(2, n) + … + F(n, n) = Sum(0)Sum(n-1) + Sum(1)Sum(n-2) + … + Sum(n-1)*Sum(0) (3)` 用(3)式进行计算，一种非常类似于DP的思想，即可避免浪费大片内存和时间生成所有合法的BST，直接能通过数值运算得到最终结果。 代码如下 class Solution &#123; public int numTrees(int n) &#123; if(n &lt;= 0) &#123; return 0; &#125; int[] Sum = new int[n + 1]; Sum[0] = 1; // 0个数时只有1种，空树 Sum[1] = 1; // 1个数时只有1种，单个节点的树 for(int i = 2;i &lt;= n;i++) &#123; for(int j = 0;j &lt; i;j++) &#123; Sum[i] += Sum[j] * Sum[i - j - 1]; &#125; &#125; return Sum[n]; &#125;&#125; 总结此题利用了 BST的特性，“左小右大”来构建所需的结果。 DFS的思想生成所有解。 数学推导式进行优化，当所求的解只是一个简单的整数值时，可思考是否需要真正生成所有数据，很多情况下只是需要我们进行推导，利用DP的思想进行数值运算即可。]]></content>
      <categories>
        <category>Leetcode</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>算法</tag>
        <tag>Leetcode</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是Copy-On-Write]]></title>
    <url>%2F2018%2F03%2F31%2Fwhatiscow%2F</url>
    <content type="text"><![CDATA[本文为本人（Haoxiang Ma)原创，如需转载请注明出处。 简介所谓Copy-On-Write，简称COW，正如它的名字所示，即为写时复制。具体而言，它是容器的一种特性，或者说可以利用这种特性开发一种COW容器。 简单来说，写时复制就是在用户（线程）对某一个容器（如List）进行写操作（增／删／改）时，不直接在容器上进行操作，而是先复制一个与原容器一模一样的容器出来，然后在复制出来的容器上进行真正的写操作。 在Java中，常见的COW容器有CopyOnWriteArrayList&lt;E&gt;和CopyOnWriteArraySet&lt;E&gt;。 具体解析 如上图所示，最开始我们只有一个容器（内存地址0x80），和一个指向该容器的引用A。经过T1和T2后，没有任何用户（线程）对其进行写操作。 到达T3时刻时，某个用户（线程）申请向容器写入123这个数据。此时由于Copy-On-Write特性，并不是直接往0x80地址的容器写入，而是先复制出一个跟原容器一模一样的容器B（内存地址0x95），然后往0x95处的这个容器B写入123这条数据。写入完成后，关键是要将引用A进行重指向，指向0x95，不再指向原本的0x80。一系列动作完成后，到了T4时刻，引用A所指向的就是一个存有123这条数据的容器。 （⚠️如无意外，0x80处的原容器所占的内存空间在某个时候会被gc回收掉） 经过了以上的图解，相信读者已经对Copy-On-Write过程有了一定的理解。接下来谈谈其中的一些细节点。 排他性的写操作 在对COW容器进行写操作时，会于内存中额外复制一个副本出来进行操作，所以写操作必然是排他性的。绝不能允许N个线程同时进行写操作，然后复制出N个副本，各线程自己对自己的那个副本进行操作，那样的话无法保证容器内的数据一致性，N个副本中的数据都不是完整的数据。 为了保证数据的一致性，需要对写操作加上一个排他锁，在某一时刻只能有一个线程对COW容器进行写操作。 读写分离 对于读操作而言，允许多个线程并发读取容器内的数据，不存在任何数据一致性问题或安全问题。对于写操作，因为会复制一个副本容器，在副本容器上写，写成功后再修改引用，所以在某个时刻，写和读针对的并不是同一个容器，实现了读写分离。 有了读写分离，就能显著提高读操作的效率，因为写锁是会排斥其他所有操作的，一旦一个容器／表／文件被加上了写锁，那么任何人都无法再读／写该容器／表／文件的内容。既然COW容器的复制机制能保证读和写是在不同的容器上进行，也就意味着永远都不会因为写锁的存在而阻塞读操作，自然就能顺畅无比地并发读取了。 优缺点基于写时复制的特点，COW容器特别适用于大量读取，极少量写入的应用场景。因为写操作是排他性阻塞的，所以一旦有较多的写操作需求，那COW容器的性能将会灾难性地下降。当写操作较多时，可将多个写操作合并成一批写操作，call一次写入方法写入多条数据，避免多次call写方法，避免多次容器复制。 优点 适用于大量读取，极少量写入的应用场景，支持高效的并发读取 缺点 内存消耗大。当容器中的数据很多时，复制操作将会消耗大量的内存，可能会频繁引发GC 无法保证数据强一致性，只能保证最终一致性，因为读的时候有可能已完成了写操作，但容器指针未来得及重指向，但经过多个时间窗口后最终数据是一致的]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的深复制&浅复制]]></title>
    <url>%2F2018%2F03%2F23%2Fdeepcopyinjava%2F</url>
    <content type="text"><![CDATA[本文内容均为本人（Haoxiang Ma）原创，如需转载请注明出处。 问题背景在程序中，出于特定的需求，往往要对一些对象进行复制，然后对复制后的对象进行操作，且不想让这些操作影响到原对象的数据。在这种情况下，搞清楚深复制（Deep Copy）和浅复制（Shallow Copy）是非常有必要的。 何谓浅复制？简单来说，就是对一个对象进行复制，得到一个新对象，然后“照搬”原对象中的数据来填充新对象的数据域。虽然新旧对象是两个不同的对象（在内存中的地址不同），但是它们内部的数据是一样的。 这样看来，浅复制不正是我们所需的复制嘛，可以得到一个新的对象，且新对象中的数据跟原对象一样，已经是非常完美的复制啊，为什么还要加一个浅字呢？ 原因在于，在Java中，一个对象里可能有primitive类型的数据如int,long等，更可能包含了referrence（引用）类型的数据如指向自定义类实例的引用。众所周知，引用的值仅仅是其所指向对象的内存地址。通过浅复制，我们会把这个内存地址“照搬”过去新对象中，那么新对象和旧对象中的一个成员引用就会指向同一块内存，操作的时候将会互相影响，与我们理想状态下的复制相差甚远。如上图所示，从Building1对象复制出来Building2对象后，它们本身所处的内存地址不一样，已经成为了两个独立的对象，但由于Building中包含了一个引用类型的数据Floor，而浅复制只将引用的值照搬过去新对象中，导致两个不同的Building中的Floor指向了内存里的同一个Floor。 所谓的浅指的就是对于referrence（引用）类型的数据只是“浅显”地“照搬”其值，没有深度地“复制”出一个新的对象。 所以对应浅复制，为了解决它的问题，我们自然就有深复制的概念。所谓深复制就是为了完完全全、彻彻底底地对一个对象进行深度的复制，避免新旧对象中的引用仍指向同一块内存区域，互相影响。 如上图所示，从Building1对象复制出来Building2对象，两者的Floor引用指向的已是内存区域中不同的Floor对象，无论对Building1再怎么折腾，也不会对Building2产生影响，完成真正理想状态下的复制。 本文将探讨在Java中如何实现对象的深复制。 具体实现首先来看看我们的自定义类Building和Floor // Building 类class Building&#123; public Floor floor; public Building() &#123; this.floor = new Floor(); &#125; public Building(Floor floor) &#123; this.floor = floor; &#125;&#125;// Floor类class Floor implements Cloneable &#123; public int count; public Floor() &#123; &#125; public Floor(int count) &#123; this.count = count; &#125;&#125; 可以看到一个Building中包含一个Floor成员变量。 clone方法与Cloneable接口 实现深复制的第一种方法，需要复制的类实现Cloneable接口，并重写clone方法。在Java的上帝类Object中，有一个纯天然的clone方法，但是其中并没有具体的代码逻辑，仅仅是声明了一个CloneNotSupportedException异常，所以必须对其进行重写。 protected native Object clone() throws CloneNotSupportedException; 而Cloneable接口中也没有任何的方法声明，完全是一个标记性的空接口(Mark-Interface)。实现该接口的作用仅仅是作一个标记(mark)，告诉JVM，我实现了这个接口，这个类的实例对象可被复制。 public interface Cloneable &#123;&#125; ⚠️ ⚠️ ⚠️注意，如果不实现Cloneable接口，即使重写了clone方法，在调用时也会自动抛出异常，因为没有标记，没有“告诉”JVM，这是可以被复制的。 所以基于Building类和Floor类的定义，我们应该让它们都实现Cloneable接口，且重写clone方法。 // Building类实现Cloneable接口class Building implements Cloneable &#123; public Floor floor; public Building() &#123; this.floor = new Floor(); &#125; public Building(Floor floor) &#123; this.floor = floor; &#125; @Override public Object clone() throws CloneNotSupportedException &#123; // 先clone出一个Building对象 Building newBuilding = (Building) super.clone(); // 然后再手动clone出一个Floor对象 newBuilding.floor = (Floor) floor.clone(); // 返回clone出的Building对象 return newBuilding; &#125;&#125;// Floor类实现Cloneable接口class Floor implements Cloneable &#123; public int count; public Floor() &#123; &#125; public Floor(int count) &#123; this.count = count; &#125; @Override public Object clone() throws CloneNotSupportedException &#123; return super.clone(); &#125;&#125; 为了测试是否真的进行了深复制，我们看看新旧Building对象的内存地址是否相同，再看看它们内部的Floor引用所指向的对象内存地址是否相同。 public static void main(String[] args) throws Exception&#123; Building b1 = new Building(); Building b2 = b1; System.out.println(b2 == b1); // true Building b3 = (Building) b1.clone(); System.out.println(b3 == b1); // false System.out.println(b3.floor == b1.floor); // false&#125; 通过实现Cloneable接口，重写clone方法，成功进行了深复制。 ⚠️然而，如下图所示，使用这种方法实现深复制，有着巨大的缺陷。 在以上例子中，我们只有2个自定义类，Building包含Floor。但是当这种包含关系变得更加复杂，有无数多个自定义类，一层又一层地包含下去时，我们需要为每个自定义类都实现Cloneable接口，重写clone方法。（例如Building包含Floor，Floor包含Room）。更麻烦的是，处于高层次的类的clone方法会很复杂，要一个一个地对低层次的类调用clone方法，一旦其中有几个类的结构发生了变化，又要重新改写多个clone方法，一点也不科学。 利用Serializable接口 首先说说什么是Serialize（序列化）和Deserialize(反序列化）。 Serialize（序列化）指的是将内存中的对象转化为二进制数据，进而将对象数据存储到磁盘文件里。 Deserialize(反序列化）指的是从磁盘文件中读取二进制数据，根据一定的规则转化为内存中的对象。 public interface Serializable &#123;&#125; 在Java中，如果想将一个对象序列化，首先得标记其为“可序列化”，即实现Serializable接口。跟Cloneable接口一样，Serializable接口也是一个空荡荡的Mark-Interface，它的唯一作用是用于标记该类可被序列化。 为了便于区分，不再使用Building和Floor进行说明。以下使用Human类和Head类。 class Human implements Serializable &#123; public Head head; public Human() &#123; this.head = new Head(); &#125; public Human(Head head) &#123; this.head = head; &#125; /** * 深复制 * @return 复制得到的Human对象 * @throws Exception */ public Human deepClone() throws Exception &#123; ByteArrayOutputStream baos = new ByteArrayOutputStream(); ObjectOutputStream objectOutputStream = new ObjectOutputStream(baos); objectOutputStream.writeObject(this); ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray()); ObjectInputStream objectInputStream = new ObjectInputStream(bais); return (Human) objectInputStream.readObject(); &#125;&#125;class Head implements Serializable &#123; public int val; public Head() &#123; &#125; public Head(int val) &#123; this.val = val; &#125;&#125; 核心代码为Human类中的deepClone()方法。首先先将当前对象(this)写到一个ObjectOutputStream中，然后再新建一个ObjectInputStream，并利用该ObjectInputStream的readObject()方法从流中获得一个新构建的Java对象。 接下来，我们检测一下是否真正完成了深复制。 public static void main(String[] args) throws Exception&#123; Human h1 = new Human(); Human h2 = h1.deepClone(); System.out.println(h1 == h2); // false System.out.println(h1.head == h2.head); // false &#125; ⚠️通过实现Serializable接口，可以避免重写多个clone()方法，也可实现深复制。 总结在Java中，想要实现真正的深复制，有以下两种方法 实现Cloneable接口，重写clone()方法。 优点：直观，容易理解，贴合人类思维。 缺点：当有非常多个自定义类，且互相包含的情况下，需要大量复杂地重写方法，对结构改动非常不友好。 实现Serializable接口，利用序列化和反序列化。 优点：当多个类结构发生变化时，不需要大量重写复制代码。 缺点：不直观，不贴合人类思维。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一致性Hash算法——分析与模拟]]></title>
    <url>%2F2018%2F03%2F17%2Fconsistent-hash%2F</url>
    <content type="text"><![CDATA[本文100%内容由本人(Haoxiang Ma)原创，如需转载请注明出处。 问题背景假设现在有一个存储集群——Cluster_A，Cluster_A包含了N个存储节点，每个存储节点上存储了大量数据。那么在一般情况下，新进入集群的数据会被计算出其对应的Hash值，然后按照一定的规则被分配到某个存储节点上。最常见的分配策略如下： // sha256或者md5均为常见的hash函数node_id = hash(data) % N 在正常情况下，这种数据分配策略的性能取决于hash算法的性能，一般不会出什么大问题。但是往往在一个大集群中会出现增／删节点的需求，试想当N变成N+1或N-1时，数据所对应的新node_id大概率会与原node_id不一样。那么用户想从集群读数据时就会找不到该数据，因为算出来的新node_id与数据真正存储的node_id不一样了。 // 原data_1所属的node_id = 50// data_1被存在50号节点上node_id1 = hash(data_1) % N = 50 // 增加一个存储节点后data_1对应的node_id// 用户读取data_1时，系统算出应该去30号节点取数据，导致读取失败node_id1 = hash(data_1) % (N+1) = 30 为了保证数据的不丢失，在增／删节点发生后，需要对集群里的所有数据进行重新Hash，以进行数据的重新分配。然而，如果每次增／删节点都对所有数据进行重分配，系统开销将会是一个天文数字，在实际工程中难以承受，总不可能让用户等待几十分钟的重Hash过程才能成功读取数据吧。 为了解决这个问题，一种特殊的解决方案——一致性Hash算法横空出世。 算法详解一致性Hash算法的核心，是把节点本身也映射到和数据一致的Hash空间。 这句话听起来似乎有点拗口，简单而言就是使用一样的Hash方法，不仅对数据进行Hash，同时也对节点进行Hash。通过这样做，就能将数据和节点放置到一个空间中，假设hash(x)的值域为[0, M]，如下图所示，可以用一个二维的环表示此Hash空间。然后对每一条数据data计算hash(data)，把数据放置到环上顺时针方向最近的节点进行存储。 正常情况（无节点增删） 假设集群内节点数量N = 100，数据条数DATA_COUNT = 100000。理想情况下，如果Hash算法的结果是均匀的，每个节点应该存储100000 / 100 = 1000条数据。为此我用Python写了一个模拟程序。 # encoding=utf-8from struct import unpack_fromfrom hashlib import md5import matplotlib.pyplot as pltNUM_NODES = 100NUM_DATA = 100000nodes = [0 for i in range(NUM_NODES)]# hash functiondef hash(data): data_md5 = md5(str(data)).digest() return unpack_from("=I", data_md5)[0]# distribute data to different nodesdef distribute(): for data in range(NUM_DATA): h = hash(data) index = h % NUM_NODES nodes[index] += 1if __name__ == '__main__': ''' Case 1: 正常情况下的数据分布 ''' distribute() max_node = max(nodes) min_node = min(nodes) print ("Node with max data: &#123;0&#125; piece of data".format(max_node)) print ("Node with min data: &#123;0&#125; piece of data".format(min_node)) # plot scatter graph x = [i for i in range(NUM_NODES)] y = nodes plt.scatter(x, y, c='r') plt.yticks(range(0, 2 * NUM_DATA / NUM_NODES, 100)) plt.xlabel("Node Index") plt.ylabel("Data Count") plt.show() 经过模拟，得到如下的数据分布散点图： 可以看出每个节点存储的数据均在1000上下浮动，差距不大，可以看作是均匀分布。 增加／删除一个节点 # encoding=utf-8from struct import unpack_fromfrom hashlib import md5LESS_NUM_NODES = 99 # less nodesORIGINAL_NUM_NODES = 100 # original nodesMORE_NUM_NODES = 101 # more nodesNUM_DATA = 100000# hash functiondef hash(data): data_md5 = md5(str(data)).digest() return unpack_from("=I", data_md5)[0]# distribute data to more nodesdef distribute_more(): transfer_count = 0 for data in range(NUM_DATA): h = hash(data) original_index = h % ORIGINAL_NUM_NODES more_index = h % MORE_NUM_NODES if original_index != more_index: transfer_count += 1 return transfer_count# distribute data to less nodesdef distribute_less(): transfer_count = 0 for data in range(NUM_DATA): h = hash(data) original_index = h % ORIGINAL_NUM_NODES less_index = h % LESS_NUM_NODES if original_index != less_index: transfer_count += 1 return transfer_countif __name__ == '__main__': ''' Case 2: 当出现增/删节点时的数据分布情况 ''' # 新增节点 transfer_count = distribute_more() print("##### When one new node is added #####") print("Data that need to be transferred: &#123;&#125;".format(transfer_count)) print("Percentage of data that need to be transferred: &#123;&#125;%".format(transfer_count * 100.0 / NUM_DATA)) # 删除节点 transfer_count = distribute_less() print("\n##### When one old node is deleted #####") print("Data that need to be transferred: &#123;&#125;".format(transfer_count)) print("Percentage of data that need to be transferred: &#123;&#125;%".format(transfer_count * 100.0 / NUM_DATA)) 数据迁移情况如下图所示 可以看到，将节点个数从N增加到N+1后或从N减少到N-1后，有差不多99%的数据经过重Hash后都要进行数据迁移，这样的数据迁移压力绝对是无法承受的。 应用一致性Hash算法后数据迁移情况 # encoding=utf-8from struct import unpack_fromfrom hashlib import md5from bisect import bisect_leftORIGINAL_NUM_NODES = 100 # original node countNEW_NUM_NODES = 101 # new node countNUM_DATA = 100000# hash functiondef hash(data): data_md5 = md5(str(data)).digest() return unpack_from("=I", data_md5)[0]# distribute data to different nodesdef distribute(original_nodes, new_nodes): transfer_count = 0 for data in range(NUM_DATA): h = hash(data) original_index = bisect_left(original_nodes, h) % ORIGINAL_NUM_NODES new_index = bisect_left(new_nodes, h) % NEW_NUM_NODES if original_index != new_index: transfer_count += 1 return transfer_countif __name__ == '__main__': ''' Case 3: 应用一致性Hash后, 需要移动的节点比例 ''' original_nodes = sorted([hash(i) for i in range(ORIGINAL_NUM_NODES)]) # 对node本身也取hash new_nodes = sorted([hash(i) for i in range(NEW_NUM_NODES)]) # 对node本身也取hash transfer_count = distribute(original_nodes, new_nodes) print("Percentage of data that need to be transferred: &#123;&#125;%".format(transfer_count * 100.0 / NUM_DATA)) 数据迁移情况如下图所示 实现了一致性Hash算法后，我们将节点也映射到了同一片Hash空间，成功地将数据迁移的比例从99%降低到37%左右。 应用一致性Hash算法后数据分布情况 # encoding=utf-8from struct import unpack_fromfrom hashlib import md5from bisect import bisect_leftimport matplotlib.pyplot as pltNUM_NODES = 100 # original node countNUM_DATA = 100000# hash functiondef hash(data): data_md5 = md5(str(data)).digest() return unpack_from("=I", data_md5)[0]# distribute data to different nodesdef distribute(nodes, stat): for data in range(NUM_DATA): h = hash(data) index = bisect_left(nodes, h) % NUM_NODES stat[index] += 1if __name__ == '__main__': ''' Case 4: 应用一致性Hash后, 数据的分布情况 ''' nodes = sorted([hash(i) for i in range(NUM_NODES)]) stat = [0 for i in range(NUM_NODES)] distribute(nodes, stat) max = max(stat) min = min(stat) print("Node with max data: &#123;&#125; piece of data".format(max)) print("Node with min data: &#123;&#125; piece of data".format(min)) # plot scatter graph x = [i for i in range(NUM_NODES)] y = stat plt.scatter(x, y, c='r') plt.yticks(range(0, 10 * NUM_DATA / NUM_NODES, 1000)) plt.xlabel("Node Index") plt.ylabel("Data Count") plt.show() 应用一致性Hash后，数据分布情况如下图所示 可以看到，相比期望值1000，出现了偏移的数据，存在很多低于1000条数据的节点。虽然数据迁移率降低了，但是出现了另一个问题——无法充分利用集群上的所有节点进行数据存储，造成了数据不平衡。 一致性Hash算法——解决数据不平衡 数据不平衡问题，根本原因就在于对节点进行Hash后，它们的Hash值在环上分布不够均匀。那么为了“分布均匀”，自然而然我们可以在环上稀疏的区域多添加一些“假”节点，也就是虚拟节点（Virtual Node），以将稀疏的区域填充得满一点。 如图所示，通过在稀疏区域增加虚拟节点（Virtual Node），原本介于Node #3和data #1间的数据可以被“存储”在V-Node #2或者V-Node #1上。再通过查询V-Node到Node的映射关系，即可从实际的存储节点取出数据。 通过保存V-Node到Node的mapping，可以迅速定位到实际存储节点。 总结一致性Hash算法，核心在于解决集群节点增删场景下的数据丢失 &amp; 大量数据迁移问题。实现的关键是将节点本身也通过Hash函数映射到数据所在的Hash空间，从而使数据能够在某一固定空间内作“物理性”（顺时针、逆时针……）的位置分配。 相关代码具体代码请查看我的Github目录]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>数据结构</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈join与数据倾斜解决方案]]></title>
    <url>%2F2018%2F03%2F04%2Fjoin-di%2F</url>
    <content type="text"><![CDATA[浅谈join与数据倾斜解决方案 本文100%为本人（Haoxiang Ma）原创内容，如需转载请注明出处。 问题背景join操作在传统的关系型数据库中特别常见，往往用来连接两张或多张在某些键上有关联的表。但当我们的数据并没有被结构化存储到数据库中，手上仅有大量的raw data —— 成千上万个磁盘数据文件（如.dat或.txt）时，又或者当大量的数据文件存放在分布式的文件系统（HDFS）里，难以将其高效导入MySQL中时，我们很自然地就会思考：有没有不需要依靠关系型数据库而实现的join呢？对于海量文件能不能用Map-Reduce实现join操作呢？ 假设现在系统中有两类数据文件： 用户数据文件（User） 订单数据文件（Order） // user文件结构user_id, user_name, phone 1, aa, 10086 2, bb, 10010 ......// order文件结构order_id, user_id, price, date 1, 1, 15.5 2018-01-01 2, 1, 3.9 2018-01-01 3, 1, 26.0 2018-01-03 4, 2, 2.2 2018-01-04 ...... 需求是将用户数据文件（User）和订单数据文件（Order）通过user_id键相关联，得到总的用户信息+用户订单信息的总“表”（一个数据文件的内容可看作表的一部分）。 // 期望得到的总表结构user_id, user_name, phone, order_id, price, date 1, aa, 10086, 1, 15.5, 2018-01-01 ...... 本文将基于以上需求，对Map-Reduce实现两表关联（join）进行探讨。 问题分析要利用Map-Reduce实现两表关联，关键点有以下几个： 如何辨别当前处理的数据来自于User表还是Order表 因为两种文件有不同的数据格式，我们在Mapper里对数据逐行进行处理时，必须要知道当前这行数据来自于User表还是Order表， 不然根本没有办法编写进一步的处理逻辑。其实看过我之前文章的同学应该见过在Mapper里获取当前数据所属文件名的方法。简单来说，就是map方法的Context参数其实带有了很多本次Job的运行信息，其中就包括了当前数据来自于哪个FileSplit，进一步可以获得该FileSplit所属的文件名。 FileSplit fileSplit = (FileSplit)context.getInputSplit();String fileName = fileSplit.getPath().getName(); 有两种不同的数据文件，可是一个MR任务里只有一种K-V对定义 两种文件格式，一种K-V对定义，那很自然我们就要想想如何将两种“合并”成一种，同时还能随时将这个合并的产物分开成两种。其实之前的文章已经多次提及一种在Map-Reduce里常用的概念——自定义JavaBean，作为一个自定义类，它的复合性质可以帮助我们“集成”很多原生数据类型。在当前问题里，我们可以定义一个JavaBean，集成User表与Order表的所有字段，并额外添加一个flag字段用以表明实例对象是User数据还是Order数据。通过这个JavaBean就可以实现“合并且随时可分离”的需求了。 JavaBean = &#123;user_id, order_id, user_name, phone, price, date&#125; “join”的逻辑具体怎么写 基于第2点，实现了一个集成的JavaBean后，我们可以在map中将数据封装成JavaBean实例对象，然后用flag字段指明数据类型。为了实现join功能，我们必须确保同一个User的所有数据都到达同一个Reducer处，那样才能将该用户的所有订单与其用户信息关联起来，避免遗漏。为了让同一个User的数据都到达同一个Reducer，我们要让Map端输出的K-V对为&lt;UserID, JavaBean&gt;，那么在Partition的时候，同一个UserID的数据自然会被分配到同一个Reducer。 当Reducer拿到一批&lt;UserID, JavaBean&gt;数据后，将其整合为&lt;UserID, Iterable&lt;JavaBean&gt;&gt;。我们可以在Iterable&lt;JavaBean&gt;&gt;里通过判断flag的值找出User的JavaBean，称为U。然后对于每个Order的JavaBean（称为O），从O中取出order_id, price, date，从U中取出user_id, user_name, phone，组成结果数据R。 R = (order_id, price, date，user_id, user_name, phone) Reducer循环将多个&lt;R, NullWritable&gt;写到最终Context中，便完成了join逻辑。 具体实现 自定义JavaBean /** * 自定义JavaBean, 实现Writable接口, 因本任务不存在"比较", 无需实现WritableComparable */public class JoinBean implements Writable &#123; public String userID; public String uName; public String phone; public String orderID; public String price; public String date; public String flag; // 用于标识User还是Order public JoinBean() &#123; &#125; public JoinBean(String userID, String uName, String phone, String orderID, String price, String date, String flag) &#123; this.userID = userID; this.uName = uName; this.phone = phone; this.orderID = orderID; this.price = price; this.date = date; this.flag = flag; &#125; public void set(String userID, String uName, String phone, String orderID, String price, String date, String flag) &#123; this.userID = userID; this.uName = uName; this.phone = phone; this.orderID = orderID; this.price = price; this.date = date; this.flag = flag; &#125; @Override public String toString() &#123; return "uName='" + uName + '\'' + ", phone='" + phone + '\'' + ", orderID='" + orderID + '\'' + ", price='" + price + '\'' + ", date='" + date + '\''; &#125; public String getUserID() &#123; return userID; &#125; public void setUserID(String userID) &#123; this.userID = userID; &#125; public String getFlag() &#123; return flag; &#125; public void setFlag(String flag) &#123; this.flag = flag; &#125; public String getuName() &#123; return uName; &#125; public void setuName(String uName) &#123; this.uName = uName; &#125; public String getPhone() &#123; return phone; &#125; public void setPhone(String phone) &#123; this.phone = phone; &#125; public String getOrderID() &#123; return orderID; &#125; public void setOrderID(String orderID) &#123; this.orderID = orderID; &#125; public String getPrice() &#123; return price; &#125; public void setPrice(String price) &#123; this.price = price; &#125; public String getDate() &#123; return date; &#125; public void setDate(String date) &#123; this.date = date; &#125; @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeUTF(userID); dataOutput.writeUTF(uName); dataOutput.writeUTF(phone); dataOutput.writeUTF(orderID); dataOutput.writeUTF(price); dataOutput.writeUTF(date); dataOutput.writeUTF(flag); &#125; @Override public void readFields(DataInput dataInput) throws IOException &#123; userID = dataInput.readUTF(); uName = dataInput.readUTF(); phone = dataInput.readUTF(); orderID = dataInput.readUTF(); price = dataInput.readUTF(); date = dataInput.readUTF(); flag = dataInput.readUTF(); &#125;&#125; Mapper /** * Mapper类 */public class JoinMapper extends Mapper&lt;LongWritable, Text, Text, JoinBean&gt; &#123; public static final String USER = "user"; public static final String ORDER = "order"; public JoinBean info = new JoinBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; FileSplit fileSplit = (FileSplit)context.getInputSplit(); String fileName = fileSplit.getPath().getName(); // 获取数据所属文件名 String line = value.toString(); String[] data = line.split(","); String userID = data[0]; // 通过flag标识实例对象是User还是Order if(fileName.startsWith("user")) &#123; info.set("", data[1], data[2], "", "", "", USER); &#125; else &#123; info.set("", "", "", data[1], data[2], data[3], ORDER); &#125; context.write(new Text(userID), info); &#125;&#125; Reducer /** * Reducer类 */public class JoinReducer extends Reducer&lt;Text, JoinBean, JoinBean, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;JoinBean&gt; values, Context context) throws IOException, InterruptedException &#123; JoinBean userInfo = new JoinBean(); List&lt;JoinBean&gt; orders = new ArrayList&lt;&gt;(); for(JoinBean info : values) &#123; // 同一个UserID，有且仅有1个User对象，其他均为该User的Order对象 if(info.getFlag().equals("user")) &#123; try &#123; BeanUtils.copyProperties(userInfo, info); &#125; catch (Exception e) &#123; System.out.println(e.toString()); &#125; &#125; else if(info.getFlag().equals("order"))&#123; JoinBean order = new JoinBean(); try &#123; BeanUtils.copyProperties(order, info); &#125; catch (Exception e) &#123; System.out.println(e.toString()); &#125; orders.add(order); &#125; &#125; // 提取所需属性, 合并, 输出 for(JoinBean order : orders) &#123; JoinBean record = new JoinBean(); record.set(key.toString(), userInfo.getuName(), userInfo.getPhone(), order.getOrderID(), order.getPrice(), order.getDate(), order.getFlag()); context.write(record, NullWritable.get()); &#125; &#125;&#125; Driver（程序入口） public class JoinDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://localhost:9000"); Job joinJob = Job.getInstance(conf); joinJob.setMapperClass(JoinMapper.class); joinJob.setReducerClass(JoinReducer.class); joinJob.setJarByClass(JoinDriver.class); joinJob.setMapOutputKeyClass(Text.class); joinJob.setMapOutputValueClass(JoinBean.class); joinJob.setOutputKeyClass(JoinBean.class); joinJob.setOutputValueClass(NullWritable.class); FileInputFormat.setInputPaths(joinJob, new Path("/join/input")); FileOutputFormat.setOutputPath(joinJob, new Path("/join/output")); System.exit(joinJob.waitForCompletion(true) ? 0 : 1); &#125;&#125; 极端情况——“数据倾斜”在这个问题里，我们的数据分为User和Order两类，其中明显User的数据会比Order少得多。一位用户只可能有一行User记录（在系统不出错的情况下），而一位用户却可以有N行Order记录，因为他可以“疯狂购物”产生了成千上万条的订单数据。 极端情况下，有一位User：user_1对应了1000000条Order数据，另一位User：user_2对应了2条Order数据。那么必然有一个负责处理user_1的Reducerreducer_1的负载过高，因为要处理1000000条数据，其他负载极少的Reducer干完活之后就得白白浪费时间等着reducer_1完成任务（往往表现为Job进度卡在99%），才能汇报整个Job已完成。 如图所示，因为大量的数据涌向少数的节点，像是一个倾斜的天平，一端重一端轻，所以以上的情况便称为数据倾斜。 为了解决当前问题引起的数据倾斜，我们可以采用一种”map端join“的方式进行关联操作，代替之前在Reducer端收集数据进行join的逻辑。相比Order表，User表无疑小得多，甚至可能是几百倍的量级差距，所以完全可能把这个小表分发给各个Mapper，让每个Mapper都拥有一份完整的User表，进而将其加载入内存构造一个&lt;user_id, user_info&gt;的哈希表。这么一来，在Mapper内只需要处理Order的数据，然后根据Order里的user_id查到哈希表里该id对应的user_info，连接起来，即可完成join操作。 MapJoinMapper /** * 实现Map端join的Mapper */public class MapJoinMapper extends Mapper&lt;LongWritable, Text, MapJoinBean, NullWritable&gt; &#123; private Map&lt;String, List&lt;String&gt;&gt; userTable = new HashMap&lt;&gt;(); private MapJoinBean outputKey = new MapJoinBean(); public static final String ORDER_FILE = "order.txt"; /** * Mapper会自行调用的一个初始化方法 * @param context Job信息 * @throws IOException * @throws InterruptedException */ @Override protected void setup(Context context) throws IOException, InterruptedException &#123; // 获取系统"投放"到本地的cache文件, 本问题中只有1个，即user.txt URI[] uris = context.getCacheFiles(); FileSystem fs = FileSystem.get(context.getConfiguration()); FSDataInputStream inputStream = fs.open(new Path(uris[0])); BufferedReader reader = new BufferedReader(new InputStreamReader(inputStream)); // 逐行读取user.txt中的数据, 构建userTable String line = null; while((line = reader.readLine()) != null) &#123; String[] terms = line.split(","); if(!userTable.containsKey(terms[0])) &#123; userTable.put(terms[0], new ArrayList&lt;&gt;()); &#125; userTable.get(terms[0]).add(terms[1]); userTable.get(terms[0]).add(terms[2]); &#125; reader.close(); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; FileSplit fileSplit = (FileSplit) context.getInputSplit(); String fileName = fileSplit.getPath().getName(); // 不需要再处理User数据文件了, 因为已经通过setup把其数据加载到内存中的userTable if(fileName.equals(ORDER_FILE)) &#123; String line = value.toString(); String[] terms = line.split(","); // 从本地的"小表" —— userTable处获得userID对应的数据 String userName = userTable.get(terms[0]).get(0); String phone = userTable.get(terms[0]).get(1); // 不再需要flag字段, 简单置为空串 outputKey.set(terms[0], userName, phone, terms[1], terms[2], terms[3], ""); context.write(outputKey, NullWritable.get()); &#125; &#125;&#125; MapJoinReducer /** * Map端Join的Reducer */public class MapJoinReducer extends Reducer&lt;MapJoinBean, NullWritable, MapJoinBean, NullWritable&gt; &#123; @Override protected void reduce(MapJoinBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, NullWritable.get()); &#125;&#125;``` + MapJoinBean``` java/** * 自定义JavaBean, 实现WritableComparable接口 */public class MapJoinBean implements WritableComparable&lt;MapJoinBean&gt; &#123; public String userID; public String uName; public String phone; public String orderID; public String price; public String date; public String flag; public MapJoinBean() &#123; &#125; public MapJoinBean(String userID, String uName, String phone, String orderID, String price, String date, String flag) &#123; this.userID = userID; this.uName = uName; this.phone = phone; this.orderID = orderID; this.price = price; this.date = date; this.flag = flag; &#125; public void set(String userID, String uName, String phone, String orderID, String price, String date, String flag) &#123; this.userID = userID; this.uName = uName; this.phone = phone; this.orderID = orderID; this.price = price; this.date = date; this.flag = flag; &#125; @Override public String toString() &#123; return "uName='" + uName + '\'' + ", phone='" + phone + '\'' + ", orderID='" + orderID + '\'' + ", price='" + price + '\'' + ", date='" + date + '\''; &#125; public String getUserID() &#123; return userID; &#125; public void setUserID(String userID) &#123; this.userID = userID; &#125; public String getFlag() &#123; return flag; &#125; public void setFlag(String flag) &#123; this.flag = flag; &#125; public String getuName() &#123; return uName; &#125; public void setuName(String uName) &#123; this.uName = uName; &#125; public String getPhone() &#123; return phone; &#125; public void setPhone(String phone) &#123; this.phone = phone; &#125; public String getOrderID() &#123; return orderID; &#125; public void setOrderID(String orderID) &#123; this.orderID = orderID; &#125; public String getPrice() &#123; return price; &#125; public void setPrice(String price) &#123; this.price = price; &#125; public String getDate() &#123; return date; &#125; public void setDate(String date) &#123; this.date = date; &#125; @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeUTF(userID); dataOutput.writeUTF(uName); dataOutput.writeUTF(phone); dataOutput.writeUTF(orderID); dataOutput.writeUTF(price); dataOutput.writeUTF(date); dataOutput.writeUTF(flag); &#125; @Override public void readFields(DataInput dataInput) throws IOException &#123; userID = dataInput.readUTF(); uName = dataInput.readUTF(); phone = dataInput.readUTF(); orderID = dataInput.readUTF(); price = dataInput.readUTF(); date = dataInput.readUTF(); flag = dataInput.readUTF(); &#125; @Override public int compareTo(MapJoinBean o) &#123; if(o == null) &#123; throw new RuntimeException(); &#125; if(userID.compareTo(o.getUserID()) == 0) &#123; return orderID.compareTo(o.getOrderID()); &#125; return userID.compareTo(o.getUserID()); &#125;&#125; MapJoinDriver public class MapJoinDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException, URISyntaxException &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://localhost:9000"); Job joinJob = Job.getInstance(conf); // 使用Map端join的Mapper和Reducer joinJob.setMapperClass(MapJoinMapper.class); joinJob.setReducerClass(MapJoinReducer.class); joinJob.setJarByClass(MapJoinDriver.class); joinJob.setMapOutputKeyClass(MapJoinBean.class); joinJob.setMapOutputValueClass(NullWritable.class); joinJob.setOutputKeyClass(MapJoinBean.class); joinJob.setOutputValueClass(NullWritable.class); // 向各节点"投放"User数据文件 joinJob.addCacheFile(new URI("/join/input/user.txt")); FileInputFormat.setInputPaths(joinJob, new Path("/join/input")); FileOutputFormat.setOutputPath(joinJob, new Path("/join/output")); System.exit(joinJob.waitForCompletion(true) ? 0 : 1); &#125;&#125; 总结 利用Map-Reduce可以完成数据文件的join操作。不需要先将数据结构化，导入MySQL。 自定义JavaBean类，用于集成多个属性的思路仍然很有用。 当join的双方是一张大表和一张小表时，可考虑将小表分发到所有map端，在本地将小表加载到内存，从而实现map端join。 代码具体代码请点击Join操作与Map端join操作代码]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈倒排索引，升级版“WordCount”]]></title>
    <url>%2F2018%2F02%2F25%2Fii-wc%2F</url>
    <content type="text"><![CDATA[全文为本人（Haoxiang Ma）原创内容，转载请标明出处。 问题背景：All About Search在数据库领域和搜索引擎领域，倒排索引是一种很重要的数据结构。在大部分的应用场景中，文本型数据（Text）是主流，依靠倒排索引这种数据结构，可以显著提高文本数据项（Term）的搜索速度（无论是理论还是实际应用中）。 假设现在我们拥有以下数据。 // Document AMy name is superman.// Document BI love my cat whose name is Kitty!// Document CPlease name my car. 如果不对以上原始数据作额外处理，径直将3个文件保存至磁盘的同一目录下，分别命名为Document A、Document B、Document C。那么当需要查找car这个数据项时，我们需要遍历文件目录下的所有文件，才能输出搜索结果：Found in Document C!。若将此场景扩展至N个数据文件，平均每个数据文件的Term Count为K，那么此搜索算法的时间复杂度将为O(N*K)，随着N和K的增长，这个时间复杂度无疑是灾难性的。 也许有人会说，既然每个文件是独立的，那么我们可以将目录下的数据文件进行“Partition”，分成M块，每一块包含N/M个文件，同时启动M个线程独立负责搜索自己块内的文件，不就能够大幅提高速度，解决搜索的性能问题了吗？先撇开硬盘的I/O性能不谈，启动、协同M个线程，合并多个线程的计算结果所需要的系统开销将会是一个天文数字，很有可能就因为这个搜索任务导致整个节点崩溃。再者，当系统的用户数快速增长，同时执行多个用户的搜索请求时根本无法保证搜索的及时性，难以并发。 所以这个时候我们就要转变思路，不要傻傻地把真正的“搜索”放在用户发送搜索请求时执行，而应该早早地为数据文件里的每个数据项（Term）建立起索引（Term Index），到用户需要搜索时就可以通过已建立好的索引快速定位并返回结果，不需要一次又一次地扫描磁盘文件。 倒排索引（Inverted Index)这种数据结构就是基于以上需求而来到了这个世界上的。 // 正常情况下，我们第一反应下的索引应该是以下结构DocumentID Terms A [My, name, is, superman] B [I, love, my, cat, whose, name, is, Kitty] C [Please, name, my, car] 如果我们按照以上的结构建立索引，仍需要逐个ID扫描其对应的Terms，搜索的时间复杂度仍然是O(N*K)，压根没有提升任何性能。 // 所以我们需要通过“倒排”的方式改变索引结构Term DocumentIDsMy [A, B, C]name [A, B, C]is [A, B]superman [A]I [B]love [B]cat [B]whose [B]Kitty [B]Please [C]car [C] 所谓“倒排”，无非就是将原来的Key（DocuementID）和Value（Terms）颠倒过来，用Term作为Key，DocumentIDs作为Value。通过构建这样的索引结构，当我们需要搜索car这一数据项时，我们只需从头开始线性扫描一遍索引表，定位到car那一行并直接取出其对应的DocumentIDs，单次搜索的时间复杂度降低到了O(N)。 当然在数据量特别大时，O(N)仍然不是一个理想的指标，仍然有进步的空间。我们可以对每个Term进行Hash得到h = Hash(Term)，然后记录h与行号的映射表H_TABLE。那么每次搜索时根据搜索项的Hash可以查H_TABLE快速定位到具体的某一行，直接就可取出其对应的DocumentIDs，总的时间复杂度理论上是O(1)。（关于Hash冲突与优化的问题本文暂时不予探讨） 实现方案：WordCount Again？当系统中有海量的数据文件时，第一反应肯定就是使用Hadoop以及Hadoop生态中的工具帮助我们处理数据。那么我们是不是可以用Map-Reduce来构建倒排索引表呢？ 答案是肯定的，确实可以使用Map-Reduce。而且仔细一想，这不就是Hadoop中的“HelloWorld”——WordCount的翻版吗？？？确实也可以这么说，处理逻辑跟WordCount非常类似，只是我们需要在Reducer中稍微多做一点点工作，所以我称之为升级版WordCount哈哈😄😄😄。 // 如果100%照搬WordCount的逻辑，那么最终产出的结果文件会是My AMy BMy Cname Aname Bname C...... 显然我们想要的倒排表格式是需要把同一个Term下的所有DocumentID合并到一行里。所以为了数据格式的正确性，我们需要对输出做点小处理。于是我们要设置一个自定义的DocumentBean类，逻辑上可以简单看作DocumentBean = List&lt;DocumentID&gt;。最终我们需要通过Reducer输出&lt;Term, DocumentBean&gt;，那么结果里的每一行自然就是我们想要的格式了。 /** * 集成多个DocumentID的Bean */public class DocumentBean implements Writable &#123; private int documentCount; private List&lt;String&gt; documentIDs; public DocumentBean() &#123; documentCount = 0; documentIDs = new ArrayList&lt;&gt;(); &#125; public void set(Iterable&lt;Text&gt; documentIDs) &#123; this.documentIDs.clear(); for(Text documentID : documentIDs) &#123; this.documentIDs.add(documentID.toString()); &#125; documentCount = this.documentIDs.size(); &#125; @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeInt(documentCount); for(String documentID : documentIDs) &#123; dataOutput.writeUTF(documentID); &#125; &#125; @Override public void readFields(DataInput dataInput) throws IOException &#123; documentCount = dataInput.readInt(); for(int i = 0;i &lt; documentCount;i++) &#123; documentIDs.add(dataInput.readUTF()); &#125; &#125; @Override public String toString() &#123; StringBuilder sb = new StringBuilder(); for(String documentID : documentIDs) &#123; sb.append(documentID).append(", "); &#125; return sb.substring(0, sb.length() - 2); &#125;&#125; /** * 倒排索引的Mapper */public class InvertedMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; private Text outputKey = new Text(); private Text outputValue = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; FileSplit split = (FileSplit) context.getInputSplit(); String fileName = split.getPath().getName(); outputValue.set(fileName); String line = value.toString(); String[] terms = line.split(" "); for(String term : terms) &#123; outputKey.set(term); context.write(outputKey, outputValue); &#125; &#125;&#125; /** * 倒排索引的Reducer */public class InvertedReducer extends Reducer&lt;Text, Text, Text, DocumentBean&gt; &#123; private DocumentBean outputValue = new DocumentBean(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; outputValue.set(values); context.write(key, outputValue); &#125;&#125; /** * 程序入口 */public class InvertedDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://localhost:9000"); Job invertedJob = Job.getInstance(conf); invertedJob.setJarByClass(InvertedDriver.class); invertedJob.setMapperClass(InvertedMapper.class); invertedJob.setReducerClass(InvertedReducer.class); invertedJob.setMapOutputKeyClass(Text.class); invertedJob.setMapOutputValueClass(Text.class); invertedJob.setOutputKeyClass(Text.class); invertedJob.setOutputValueClass(DocumentBean.class); FileInputFormat.setInputPaths(invertedJob, new Path("/inverted_index/data/")); FileOutputFormat.setOutputPath(invertedJob, new Path("/inverted_index/output/")); System.exit(invertedJob.waitForCompletion(true) ? 1 : 0); &#125;&#125; 总结：What Should Be Remembered 利用自定义的Bean类来辅佐Map-Reduce，实现各种复杂功能的思路已经很普遍了，例如自定义输出输出格式／实现两表join／二次排序等等等等。 倒排索引表的设计与优化其实很复杂，本文谈到的内容只是管中窥豹。如上图所示，在倒排索引表中甚至可以存储每个Term在不同文件中的出现次数／文件更新（插入）时间／Term的TF-IDF值等等等等。通过存储这些文本数据可以帮助搭建高效的推荐系统，或者对搜索排序进行优化。]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>bigdata</tag>
        <tag>hadoop</tag>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[海量小文件优化之自定义InputFormat]]></title>
    <url>%2F2018%2F02%2F17%2Fcustomizeinputformat%2F</url>
    <content type="text"><![CDATA[全文为本人（Haoxiang Ma）原创内容，转载请标明出处。 问题背景HDFS作为分布式文件存储系统，极其适用于海量大文件的存储场景。每个大文件在系统底层会被切分成多个Block（Block Size默认为128MB），且每个Block会自动被冗余处理（默认备份数为3份）以保证一定程度上的数据安全。 HDFS对大文件友好，但是世事往往不尽如人意。“存储”对于整个业务系统而言只是其中一环，在存储之前必须先有数据收集的流程。假如前端进行数据收集时raw data为海量的小数据文件（从1KB到10MB不等），且没有经过合并就直接通过HDFS的上传API写到HDFS中，那么NameNode上就会保存大量的Block元数据记录（即使单个小文件的大小远远不到一个Block的容量，但在逻辑上也会被切分为一个Block，虽然物理上并没有占用一个真正的Block的物理容量）。 相比NameNode中存储大量Block元数据带来的影响，更值得关注的是海量小文件输入对Map-Reduce任务的影响。 理想情况下，假设Map-Reduce任务的文件输入是/xx/xx/xx.dat，为了方便，我们假设xx.dat文件的大小刚好是3个Block的大小（3 * 128MB），暂且忽略备份情况。如图所示，在以该文件作为输入启动Map-Reduce任务时，系统getSplit()自然而然地将该文件分成3个Split，定位到各Split所在的DataNode并在上面启动Map任务。3个Map任务各自处理Local Data（Hadoop提倡的Data Locality），然后Map端输出会被相应的Reducer拉取进行Reduce操作。在以上理想情况中，各个Map任务负载基本均衡，每个Map处理的都是本地128MB的数据，系统运行状态良好。 然而，如果Map端输入为海量的小文件，那么默认每个小文件本身就会被当作一个Split（小到不可再划分了）。如上图所示，假如系统中小文件的分布比较均匀，在每个存放有小文件的DataNode上都会启动一个Map任务。但是假如小文件的数目远远大于系统中DataNode的个数（100万个小文件，100个DataNode），也就是说一个DataNode上可能存放有N个小文件时，那么DataNode就要不断启动一个又一个的Map任务直到N个小文件都被处理完。更极端的情况是小文件分布不均匀，在某个特定的DataNode上存放了80%的小文件，剩余DataNode上只存放了20%的小文件，那么其他DataNode在结束自己本地的计算后，还得“默默等待”那个存放了80%小文件的DataNode完成它的所有map工作，严重影响了整个Map-Reduce任务的效率。 所以对于海量小文件数据，我们在启动Map-Reduce任务前必须对其进行优化，常见的优化思路有 在系统前沿就把多个小文件合并，将合并后的大文件写入HDFS 小文件已大量分布在HDFS中，通过一定手段在HDFS中合并它们 在启动正式的Map-Reduce任务前，先预处理合并大量小文件 其中思路1的逻辑无关HDFS，不在本文讨论范围。思路2与思路3本质上一样，是通过一定的手段让小文件合并为大文件，以大文件作为正式的Map-Reduce任务的数据输入。 源码分析默认情况下，Map-Reduce使用的默认输入格式（InputFormat）是TextInputFormat，通过查看源码可知TextInputFormat实现的泛型是\&lt;LongWritable, Text>，这也就解释了为什么默认情况下我们Map任务的输入是\&lt;LongWritable, Text>。 调用getRecordReader方法会返回一个真正的RecordReader\&lt;LongWritable, Text>对象（实现了具体读文件的逻辑）供外部使用。每次我们把Map-Reduce的作业提交之后，系统会根据设定的InputFormat（默认/自定义）建立一个RecordReader对象来读取Split，并按照一定格式对Split进行预处理。如图所示，默认的RecordReader为LineRecordReader，也就是按行读取Split中的数据内容，每读取一行都会生成一个\&lt;LongWritable, Text>（行起始偏移量, 文本内容)的K-V对。利用RecordReader不断地读取，就可以完成map端的输入。 基于以上原理，实现小文件合并为大文件的逻辑其实很简单。我们完全可以自定义InputFormat，对每个小文件不再逐行读取，而是将整个小文件的内容全部读取以生成\&lt;小文件名, 文件全部内容>的K-V对，然后将N个小文件的N个K-V对通过1个Reducer最终合并成一个大文件(本例使用SequenceFile)。 所以，关键就在于实现自定义的InputFormat类与自定义的RecordReader类。 具体实现简要原理网上的各种自定义InputFormat教程中总是先贴一大段代码，让人很难一下子搞清楚InputFormat、RecordReader和Map-Reduce任务的关系。简单来说，InputFormat和RecordReader的功能如下 自定义的InputFormat为本次Map-Reduce任务提供了一个自定义的RecordReader 自定义的RecordReader实现具体的如何从Split中读取数据的逻辑 Java代码首先实现自定义的RecordReader类。 /** * 自定义的RecordReader, 用于将Split中的内容转换为自定义的K-V对形式 */public class MyRecordReader extends RecordReader&lt;Text, BytesWritable&gt; &#123; private FileSplit split; // 读入的文件split private boolean isFinished; // 转换是否完成的标志位 private Text key; // 输出Key private BytesWritable value; // 输出Value private JobContext context; // 作业内容 /** * 初始化RecordReader方法 * @param inputSplit 读入的文件split * @param taskAttemptContext 作业内容 * @throws IOException * @throws InterruptedException */ @Override public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; this.isFinished = false; this.key = new Text(); this.value = new BytesWritable(); this.split = (FileSplit) inputSplit; this.context = taskAttemptContext; &#125; /** * 告诉调用者是否还有未读的下一个K-V对 * @return true / false * @throws IOException * @throws InterruptedException */ @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; if(!isFinished) &#123; String fileName = this.split.getPath().getName(); this.key.set(fileName); int contentLength = (int)split.getLength(); byte[] content = new byte[contentLength]; FileSystem fs = FileSystem.get(context.getConfiguration()); FSDataInputStream inputStream = null; try &#123; inputStream = fs.open(this.split.getPath()); IOUtils.readFully(inputStream, content, 0, contentLength); value.set(content, 0, contentLength); &#125; catch (Exception e) &#123; System.out.println(e.toString()); &#125; finally &#123; if(inputStream != null) &#123; try &#123; IOUtils.closeStream(inputStream); &#125; catch (Exception e) &#123; System.out.println(e.toString()); &#125; &#125; &#125; isFinished = true; return true; &#125; return false; &#125; /** * 返回当前读到的Key * @return key * @throws IOException * @throws InterruptedException */ @Override public Text getCurrentKey() throws IOException, InterruptedException &#123; return this.key; &#125; /** * 返回当前读到的Value * @return * @throws IOException * @throws InterruptedException */ @Override public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123; return this.value; &#125; /** * 返回转换过程的状态 (是否转换完成) * @return true / false * @throws IOException * @throws InterruptedException */ @Override public float getProgress() throws IOException, InterruptedException &#123; return this.isFinished ? 1.0f : 0.0f; &#125; @Override public void close() throws IOException &#123; &#125;&#125; 然后实现自定义的InputFormat类。 /** * 自定义InputFormat, 创建一个自定义的RecordReader(以实现自定义的读取输入文件逻辑) */public class MyInputFormat extends FileInputFormat&lt;Text, BytesWritable&gt;&#123; @Override public RecordReader&lt;Text, BytesWritable&gt; createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; // 初始化一个自定义的RecordReader并返回 RecordReader&lt;Text, BytesWritable&gt; recordReader = new MyRecordReader(); recordReader.initialize(inputSplit, taskAttemptContext); return recordReader; &#125;&#125; 实现了自定义的MyInputFormat类和MyRecordReader类后，Mapper和Reducer并没有什么可做的，map端只是简单地将数据输入直接输出，reduce端收集到K-V对后将其直接输出。 /** * Mapper, 负责处理从Split中读取的K-V型数据 */public class MyMapper extends Mapper&lt;Text, BytesWritable, Text, BytesWritable&gt; &#123; @Override protected void map(Text key, BytesWritable value, Context context) throws IOException, InterruptedException &#123; context.write(key, value); &#125;&#125; /** * Reducer, 负责接收K-V对后输出为结果文件 */public class MyReducer extends Reducer&lt;Text, BytesWritable, Text, BytesWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;BytesWritable&gt; values, Context context) throws IOException, InterruptedException &#123; for(BytesWritable value : values) &#123; context.write(key, value); &#125; &#125;&#125; 最后把Job信息设置好后就可以提交到集群上运行。 /** * 程序入口 */public class Driver &#123; private static final String HDFS_HOST = "hdfs://localhost:9000"; private static final int NUM_REDUCE_TASKS = 1; private static final String FILE_INPUT_PATH = "/small_files/data"; private static final String RESULT_OUTPUT_PATH = "/small_files/output"; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", HDFS_HOST); Job myJob = Job.getInstance(conf); // 通过类名设置运行的jar包 myJob.setJarByClass(Driver.class); // 设置Mapper Class与Reducer Class myJob.setMapperClass(MyMapper.class); myJob.setReducerClass(MyReducer.class); // 设置Mapper与Reducer的输出数据类型 myJob.setMapOutputKeyClass(Text.class); myJob.setMapOutputValueClass(BytesWritable.class); myJob.setOutputKeyClass(Text.class); myJob.setOutputValueClass(BytesWritable.class); // 设置InputFormat与OutputFormat // 使用自定义的InputFormat作为Input格式, Sequence文件作为Output格式 myJob.setInputFormatClass(MyInputFormat.class); myJob.setOutputFormatClass(SequenceFileOutputFormat.class); // 自定义Reduce Task数量, 决定最终输出多少个结果文件 myJob.setNumReduceTasks(NUM_REDUCE_TASKS); // 设置数据所在目录, 结果输出目录 FileInputFormat.setInputPaths(myJob, new Path(FILE_INPUT_PATH)); FileOutputFormat.setOutputPath(myJob, new Path(RESULT_OUTPUT_PATH)); System.exit(myJob.waitForCompletion(true) ? 1 : 0); &#125;&#125; 总结当HDFS中存储了海量小文件时，利用自定义的InputFormat和RecordReader启动Map-Reduce任务可将小文件合并为大文件。这个合并的Job可以放在正式的Map-Reduce任务前，利用ControlledJob对合并Job与正式Job进行控制，合并结束后才启动正式Job；也可以定时地对HDFS上的文件进行合并，避免需要用时才进行合并。 当然，最优方案还是在系统前沿就把数据合并成尽量大的文件，再把大文件写入HDFS。 完整代码完整代码请查看：我的Github目录]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>bigdata</tag>
        <tag>hadoop</tag>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OS X下MapReduce程序运行的几种模式]]></title>
    <url>%2F2018%2F02%2F13%2Fosxmr%2F</url>
    <content type="text"><![CDATA[全文为本人（Haoxiang Ma）原创内容，转载请标明出处。 1.MapReduce程序运行的模式简介 程序运行模式 本地模式 利用本地的JVM运行，使用本地的IDE进行debug 远程模式 提交至远程的集群上运行，使用本地的IDE进行debug 提交至远程的集群上运行，不使用本地IDE进行debug 数据存放路径 远程文件系统（hdfs) 本地文件系统（local file system) 2.开发环境简介 操作系统：macOS Sierra 10.12.6 Java版本：1.8.0_131-b11 Hadoop版本：hadoop-2.7.4 IDE：IntelliJ IDEA 3.MapReduce程序运行例子3.1 程序需求 学校里开设了多门课程，有语文（chinese）、数学（math）、英语（english）等。经过了一次年级统考后，每个学生的成绩都被记录在多个文本文件中，文本文件格式如下。 math.txt Ben 75Jack 60May 85Tom 91 english.txt Jack 72May 60Tom 62Ben 90 chinese.txt Ben 79May 88Tom 68Jack 70 现需要根据以上的文本文件，算出每个学生在本次统考中的平均分，并将结果用一个总的文件averageScore.txt进行存储。averageScore.txt的格式如下。 averageScore.txt #name #scoreBen 0.0May 0.0Tom 0.0Jack 0.0 3.2 程序设计思路3.2.1 Mapper的处理逻辑Mapper每次从文本文件中读取1行内容，即调用1次map方法。Mapper需要把原始数据中一行的内容拆分成学生姓名（student name）和该门课程的分数（score）。按照需求，本程序最终要算出每一个学生的平均分，所以学生姓名应作为一个key，对应的value即为该生的平均分（实际上是不严谨的，因为在实际环境中会出现多个学生重名的现象，若不作特殊处理，key是不允许重复的。最根本的解决方案是采用学号作为key，但为了演示直观，仅采用学生姓名作为key）。 Mapper读完一行的数据后，把{student name，score}这个key-value写入中间结果，准备传送给Reducer作下一步的运算。 3.2.2 Reducer的处理逻辑Reducer接收到的数据，实际上是一个key与该key对应的value的一个集合（并不仅仅是一个value）。在本需求中，传入reduce方法的参数是学生姓名，以及该生多门课程分数的集合，类似于Ben,[60,70,80,...]。所以Reducer需要将集合中的分数求和，然后求出平均值，最终得到一个{student name, average score}的key-value对。 3.2.3 具体代码设计 AVGMapper类用于实现map方法 package mr; import org.apache.hadoop.io.DoubleWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException; /** Created by marco on 2017/8/17.*/public class AVGMapper extends Mapper&lt;LongWritable, Text, Text, DoubleWritable&gt;{ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {String line = value.toString(); if(line.length() == 0) // 文件格式错误，出现空行 return; String[] split = line.split(&quot; &quot;); String stuName = split[0]; String stuScore = split[1]; double score = Double.parseDouble(stuScore); // 转成double类型，方便后续求均值计算 context.write(new Text(stuName), new DoubleWritable(score)); }} AVGReducer类用于实现reduce方法 package mr; import org.apache.hadoop.io.DoubleWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;import java.io.IOException; /** Created by marco on 2017/8/17.*/public class AVGReducer extends Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt;{ @Override protected void reduce(Text key, Iterable values, Context context) throws IOException, InterruptedException { double sum = 0; int length = 0; for(DoubleWritable value : values) { sum += value.get(); length++; } double avgScore = sum / (double)length; context.write(key, new DoubleWritable(avgScore)); }} AVGRunner类用于关联Mapper与Reducer，并创建MapReduce任务（Job）提交运行。基本代码如下所示。 package mr; import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.DoubleWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; /** Created by marco on 2017/8/17.*/public class AVGRunner{ static public void main(String[] args) throws Exception { // 设置hdfs的handler Configuration fsConf = new Configuration(); fsConf.set(&quot;fs.default.name&quot;,&quot;hdfs://localhost:9000/&quot;); FileSystem fs = FileSystem.get(fsConf); // MapReduce的配置参数 Configuration mrConf = new Configuration(); // 新建一个求平均值的Job Job avgJob = Job.getInstance(mrConf); avgJob.setJarByClass(AVGRunner.class); // 设置Mapper类与Reducer类 avgJob.setMapperClass(AVGMapper.class); avgJob.setReducerClass(AVGReducer.class); // 设置输入输出的数据结构 avgJob.setMapOutputKeyClass(Text.class); avgJob.setMapOutputValueClass(DoubleWritable.class); avgJob.setOutputKeyClass(Text.class); avgJob.setOutputValueClass(DoubleWritable.class); // 检查结果输出目录，若已存在则删除输出目录 if(fs.exists(new Path(&quot;/avg/output&quot;))) { fs.delete(new Path(&quot;/avg/output&quot;), true); } // 设置数据目录以及结果输出目录 FileInputFormat.setInputPaths(avgJob, new Path(&quot;&quot;)); FileOutputFormat.setOutputPath(avgJob, new Path(&quot;&quot;)); // 提交任务，等待完成 System.exit(avgJob.waitForCompletion(true)?0:1); }} 3.3 MapReduce程序运行 若使用本地文件系统的数据文件，且在本地模式运行，无需配置hdfs相关的参数，数据目录以及结果输出目录填写本地路径即可。（确保结果输出文件夹未被创建，否则会报异常） // 均填写本地文件路径即可 FileInputFormat.setInputPaths(avgJob, new Path(&quot;&quot;)); FileOutputFormat.setOutputPath(avgJob, new Path(&quot;&quot;)); 若使用hdfs上的数据文件，且在本地模式运行，应配置hdfs相关参数，数据目录以及结果输出目录均填写hdfs的路径。（确保结果输出文件夹未被创建，否则会报异常） // 设置hdfs参数，并用该配置创建一个新的Job Configuration fsConf = new Configuration(); fsConf.set(&quot;fs.default.name&quot;,&quot;hdfs://localhost:9000/&quot;); Job avgJob = Job.getInstance(fsConf); // 均填写hdfs路径即可 FileInputFormat.setInputPaths(avgJob, new Path(&quot;&quot;)); FileOutputFormat.setOutputPath(avgJob, new Path(&quot;&quot;)); 3.3.1 本地模式运行本地模式运行，直接编译执行AVGRunner的main方法即可，程序运行结束后会在自行设置的结果输出目录中生成运行结果。 3.3.2 远程集群运行首先使用IDE将程序打成一个jar包，本例中命名为hadoop.jar 提交到远程集群上运行分两种情况 使用本地IDE（IntelliJ IDEA）运行，任务被提交到集群运行，但可使用IDE进行跟踪debug 新建一个MapReduce的配置对象，将已经打包好的jar包传入配置中 // MapReduce的配置参数，远程运行，本地debug Configuration mrConf = new Configuration(); mrConf.set(&quot;mapreduce.job.jar&quot;,&quot;hadoop.jar&quot;); mrConf.set(&quot;mapreduce.framework.name&quot;,&quot;yarn&quot;); //利用以上配置新建一个Job Job avgJob = Job.getInstance(mrConf); avgJob.setJarByClass(AVGRunner.class); 在终端直接使用hadoop命令将任务提交到集群运行，无法使用IDE进行跟踪debug 直接在终端中输入hadoop命令 hadoop jar $jar包名称 $待执行的类的名称 在本例中应输入 hadoop jar avg.jar mr.AVGRunner ####################### 注意⚠️ ####################### 在OS X中，使用IntelliJ IDEA打包jar包后，若在终端中直接使用hadoop jar $jar包名称 $待执行的类的名称提交MapReduce任务，会报出异常，因为OS X系统的文件系统对大小写不敏感（case-insensitive）。 经过对此异常的搜索，暂时的解决方案是通过删除jar包中的LICENSE文件，使任务顺利提交。 # 在终端中执行以下命令 zip -d $jar包名称 META-INF/LICENSE zip -d $jar包名称 LICENSE ##################################################### 可以看到使用了hadoop命令提交任务后，系统调用了RPC框架和Yarn框架中的一些服务，用于远程运行，而非使用LocalJobSubmitter于本地运行。并且在MapReduce任务管理页面可看到任务已经完成的历史记录。 4.总结MapReduce任务可在本地运行，也可提交到集群上运行。 在开发初期，需要编写Demo程序时，可在本地进行开发与测试，将数据文件放置在本地文件系统，直接使用IDE运行主类的main方法，观察运行结果。 上线前调试，可采用远程模式运行，不直接使用hadoop命令提交，而是使用IDE进行提交与debug，这样既可以保证程序运行在远处集群上（生产环境or开发环境），也可以在本地方便跟踪调试。 可上线时，使用hadoop命令直接提交到远程集群，并通过localhost:50070（默认配置）的任务管理页面进行观察。]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在Map-Reduce中实现二次排序（对Value排序）]]></title>
    <url>%2F2018%2F02%2F13%2Fssort%2F</url>
    <content type="text"><![CDATA[全文为本人（Haoxiang Ma）原创内容，转载请标明出处。 问题背景众所周知，Map-Reduce任务完成后，输出的结果文件总是按照Key进行升序排列（shuffle阶段完成）。 例如Hadoop里经典的word count程序： // File1原始数据 hello world hello apple apple apple baby // 输出结果文件，已按Key进行升序排序 apple 3 baby 1 hello 2 world 1 显然，这种默认的排序方式很多时候能帮开发者减轻负担，因为开发者不用去自行实现对Key进行排序的算法，所有的排序操作均由Hadoop帮开发者完成（详情参考Map-Reduce中的shuffle原理，具体参考map端的partition-&gt;spill-&gt;(spill.sort)-&gt;(combine)过程）。 一切似乎很美好，可是如果我们遇到了要对value排序的需求呢？ // 假设有如下电影评分数据 movies.dat // 且我们希望对rating进行降序排序, 以便分析每部电影的评分趋势 MovieID, rating 001, 75.5 001, 89 001, 60 002, 55 002, 79 003, 92.5 003, 92.8 003, 60 ...... // 期望输出值（Key有序的同时，按rating降序排序） 001 89.0 001 75.5 001 60.0 002 79.0 002 55.0 003 92.8 003 92.5 003 60.0 在“排序”的需求下，我们很自然地会想到： 利用系统默认的排序。 预处理数据，把rating当成Key，movieID当成Value。 虽然按照以上的想法，确实是对作为Key的rating排序了，但我们需要的是降序输出而非默认的升序输出，且输出格式不符合要求（第一列应为movieID）。 此时，就需要引入一种二次排序（Secondary Sort）的概念了。所谓二次排序（Secondary Sort）其实就是人工地对所需字段进行排序，在系统的默认排序基础上做第二次的排序。 解决方案二次排序（Secondary Sort）概念上不难理解，无非就是自行多做一次特定的排序。可是该如何实现呢？怎么在map-reduce的流程框框内对Value进行人工排序呢？ 其实关键技巧就是利用map-reduce会对Key排序的特点，让它“顺带”对Value进行排序。为了达到这种“顺带”的效果，我们可以将原始数据中的Key（MovieID）和Value（rating）合并到一起作为新的Key（MovieBean），同时仍然保持原Value（rating）作为Value。当系统对这个合并的Key（MovieBean）按照某种特性进行排序时，其对应的Value也会被相应地“排序”（因为map端输出时，Key和Value是一个整体数据结构），为此我们应设计一个自定义的Bean类。 /** * 自定义的MovieBean类, 将原始数据中的Key和Value合并到一个类中。 */ public class MovieBean implements WritableComparable&lt;MovieBean&gt;{ public Text movieID; public DoubleWritable score; public MovieBean() { } public MovieBean(Text movieID, DoubleWritable score) { this.movieID = movieID; this.score = score; } public void set(Text movieID, DoubleWritable score) { this.movieID = movieID; this.score = score; } public Text getMovieID() { return movieID; } public void setMovieID(Text movieID) { this.movieID = movieID; } public DoubleWritable getScore() { return score; } public void setScore(DoubleWritable score) { this.score = score; } @Override public String toString() { return &quot;movieID=&quot; + movieID + &quot;, score=&quot; + score; } /** * 重点! 利用自定义的compareTo方法实现排序效果! * @param o object of MovieBean * @return result of comparison */ @Override public int compareTo(MovieBean o) { if(o == null) { throw new RuntimeException(); } // movieID相同时, 按照score进行降序排序 if(this.movieID.compareTo(o.getMovieID()) == 0) { return -score.compareTo(o.getScore()); } // movieID不相同时, 直接按照MovieID排序 return this.movieID.compareTo(o.getMovieID()); } /** * @param dataOutput 序列化输出 */ @Override public void write(DataOutput dataOutput) throws IOException { dataOutput.writeUTF(movieID.toString()); dataOutput.writeDouble(score.get()); } /** * @param dataInput 序列化输入 */ @Override public void readFields(DataInput dataInput) throws IOException { movieID = new Text(dataInput.readUTF()); score = new DoubleWritable(dataInput.readDouble()); } } 有了这个自定义的MovieBean类作为新的Key后，Mapper端的输出就从原来的 \&lt;MovieID, Rating&gt;键值对转换成了\&lt;MovieBean, Rating&gt;键值对。 &lt;MovieID, Rating&gt; =&gt; &lt;(MovieID, Rating), Rating&gt; 那么Reducer端接收到的将是大量的\&lt;MovieBean, Rating&gt;数据。此时问题就来了，当我们的Key是简单类型时（如IntWritable，Text，DoubleWritable），很自然就能将多个K-V对中相同的Key提取出来，且将多个Value合并成一个集合，构成Reducer端的输入数据结构\&lt;Key, List>。但是当我们的Key是复合类型，例如MovieBean是MovieID和Rating的复合结构时，即使两个MovieBean对象的MovieID相同，但这两个MovieBean却是不会被认为是同一个对象的。 // 1号K-V对 &lt;(&quot;0001&quot;, 85.0), 85.0&gt; // 2号K-V对 &lt;(&quot;0001&quot;, 68.0), 68.0&gt; // Reducer接收到以上两个K-V对后，并不会把它们合并成&lt;MovieBean, List&lt;Value&gt;&gt;的数据结构 // 因为两个K-V对的Key（MovieBean）并不相同 为了解决这个问题，让Reducer把相同MovieID的MovieBean当成是一样的Key，继而把相同MovieID所对应的Ratings合并成\&lt;MovieBean, List>结构，我们需要通过实现自定义的GroupingComparator来 _欺骗_ Reducer。 /** * 自定义的GroupingComparator */ public class MovieGroupingComparator extends WritableComparator { /** * 构造函数, 告知自定义Bean类 */ protected MovieGroupingComparator() { super(MovieBean.class, true); } /** * 重写WritableComparator接口的compare方法(类似于普通Comparator接口) * @param a movieA * @param b movieB * @return result of comparison */ @Override public int compare(WritableComparable a, WritableComparable b) { MovieBean movieA = (MovieBean) a; MovieBean movieB = (MovieBean) b; // 只比较两个MovieBean的MovieID, 忽略其他属性 return movieA.getMovieID().compareTo(movieB.getMovieID()); } } 实现以上的自定义GroupingComparator时，我们在compare方法中只考虑MovieID这一个属性，等同于_欺骗_了Reducer。Reducer判断两个Key是否相同时只考虑MovieID是否相同，从而将不同的MovieBean对象抽取成一个统一的MovieBean作为Reducer的输入Key，即可顺利合并出\&lt;MovieBean, List>这样的数据结构。 // example // 假设Reducer0接收到了以下K-V对 &lt;(&quot;0001&quot;, 89.0), 89.0&gt; &lt;(&quot;0001&quot;, 76.8), 76.8&gt; &lt;(&quot;0001&quot;, 69.5), 69.5&gt; &lt;(&quot;0001&quot;, 69.3), 69.3&gt; // 由于compare方法只比较MovieBean中的MovieID属性, 完全忽略Rating, 所以 // 以上4个K-V对中的MovieBean均会被视作一样的Key，最终合并成的数据结构如下 // (为什么是有序的, 因为在map端的spill过程中已经依照rating降序排列了,参考MovieBean // 类中重写的compareTo方法) &lt; Key, List&lt;Value&gt;&gt; &lt;(&quot;0001&quot;, 89.0), [89.0, 76.8, 69.5, 69.3]&gt; 作业提交至此，二次排序中所有自定义的工作已经完成。但是千万不要忘记在提交Job之前，给Job设置以上自定义GroupingComparator，否则Job会使用内置默认的GroupingComparator，那我们的二次排序就无法生效了。 movieJob.setGroupingComparatorClass(MovieGroupingComparator.class); 另外，如果需要自定义Reducer数量（例如有时希望输出N个结果文件，则需要N个Reducer），还要自定义Partitioner。Partitioner的作用简单来说就是给Mapper端产生的K-V对打上一个partition id烙印，让系统知道这个K-V对应该被哪个Reducer取走。在本例中，如有需要（不是必须），我们可以按照MovieID进行划分，不同MovieID的K-V对划分到不同的Reducer上进行处理。 /** * 自定义Partitioner, 用于划分K-V对被哪个Reducer取走 */ public class MoviePartitioner extends Partitioner&lt;MovieBean, DoubleWritable&gt; { @Override public int getPartition(MovieBean movieBean, DoubleWritable doubleWritable, int numReducers) { // 相同MovieID的必定会到同一个Reducer上 return movieBean.getMovieID().hashCode() % numReducers; } } 最后给Job设定自定义的Reducer数，即可启动N个Reducer进行数据处理。 final int NUM_REDUCE_TASK = 5; movieJob.setNumReduceTasks(NUM_REDUCE_TASK); 单Reducer运行结果 总结使用SecondarySort可以使我们在Map-Reduce框架内完成自定义排序。依托Map-Reduce会对Key进行排序的特性，可以将需要排序的字段（Value）与原始Key合成为自定义的Bean作为新的Key，原Value保持不动。有了SecondarySort，我们就不必在框架外做额外的工作进行排序，干扰程序的可读性；也不必将原始Key和Value对换，影响输出格式。 代码地址Github-Secondary Sort]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>bigdata</tag>
        <tag>hadoop</tag>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用ZooKeeper开发服务器上下线感知程序]]></title>
    <url>%2F2018%2F02%2F13%2Fzkserver%2F</url>
    <content type="text"><![CDATA[全文为本人（Haoxiang Ma）原创内容，转载请标明出处。 What is ZooKeeperZooKeeper是一个分布式的分布式应用程序协调服务。简单地来说，就是用于协调管理多个分布式应用程序的一个工具，扮演着一个第三方管理者的角色。 问题背景分析假设现在有10个应用程序(App#0 - App#9)，运行在由10台服务器(Server#0 - Server#9)组成的集群上（假设平均分配，每台服务器上运行一个程序）。此时由于某个热门线上活动的开始（如抢票or低价秒杀等），突然间有数以百万计的用户访问服务器上的资源，等待服务器处理并应答（如下图所示）。 很不幸10台服务器中有K台受不住负载压力，导致服务器崩溃。在这种情况下，如果客户端无法感知服务器的状态（在线／离线），部分向已经崩溃的服务器发送请求的客户端将会有长时间无法获得应答，它们只能一直重复地向已经崩溃的服务器地址重发请求，无法切换至另外（10-K）台完好的服务器进行交互。 其实在这种场景下，如果客户端能够及时地感知到集群中哪些节点已经崩溃，哪些节点仍然完好，是可以切换至完好的节点并向其发送请求的。理论上只要集群中仍有1个节点是完好的，它即能向客户端提供服务。 所以整个问题的症结就在于，如何让客户端感知到服务器上下线状态，以便切换请求发送的地址。 重新参考ZooKeeper的功能描述，ZooKeeper可以用来协调管理多个分布式应用程序，那其实可以用于管理我们的分布式机器集群。如上图所示，在用户和服务器集群中间可设置ZooKeeper层，让ZooKeeper实时感知每一个节点的状态，然后客户端并不直接向具体节点发起请求，而应先向ZooKeeper询问当前仍然存活的服务器节点，然后再从中挑选一个负载较低的服务器节点进行交互。由于ZooKeeper本身的高可用性（本身也可拓展为分布式架构），所以就能大大地提高整个系统的可用性。 ZooKeeper数据结构ZooKeeper数据结构采用了树状结构（在文件系统中被广泛使用），且不是简单的二叉树，而是多叉树。在ZooKeeper的树结构中，每一个节点被称为znode，可通过控制台命令或者Java的SDK对内部数据进行管理。 znode的类型有2*2=4种，分别是： PERSISTENT PERSISTENT_SEQUENTIAL EPHEMERAL EPHEMERAL_SEQUENTIAL 其中PERSISTENT和EPHEMERAL的区别正如其名，在无外力影响下PERSISTENT节点不会被改变和删除，而EPHEMERAL节点在创建节点的session结束后会自动从树中删除。至于SEQUENTIAL与非SEQUENTIAL则影响了节点id自增，SEQUENTIAL节点的id会自动遵循父节点下的自增规则进行命名。 如图所示，在本问题中我们可以把一台服务器看作树中的一个节点，我们可以利用EPHEMERAL节点的这一特性进行服务器状态的监听。服务器上线时创建与zk之间的session并向zk注册节点，只要服务器不崩溃，session便不会结束，即EPHEMERAL节点会一直存在，可被客户端感知；当服务器崩溃时，其与zk之间保持的session自然也会结束，EPHEMERAL节点会自动被删除，客户端查询服务器列表时绝对无法获得已删除的节点信息。 Demo程序 Server.java (服务器端代码) package my.bigdata.zk;import org.apache.zookeeper.*;public class Server &#123; private static final String HOST_ADDRESS = "localhost:2181"; private static final int DEFAULT_TIMEOUT = 2000; private static final String DEFAULT_SERVER_PARENT = "/servers"; private ZooKeeper zkConnect = null; /** * 连接至ZooKeeper * @throws Exception */ public void connect() throws Exception&#123; zkConnect = new ZooKeeper(HOST_ADDRESS, DEFAULT_TIMEOUT, new Watcher() &#123; @Override public void process(WatchedEvent watchedEvent) &#123; System.out.println("Type:" + watchedEvent.getType() + " Path:" + watchedEvent.getPath()); &#125; &#125;); &#125; /** * 向ZooKeeper注册本服务器节点 * @param data 服务器信息 * @throws Exception */ public void register(String data) throws Exception&#123; String create = zkConnect.create(DEFAULT_SERVER_PARENT + "/server", data.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); // 注册成ephemeral节点以便自动在zk上注销 System.out.println(create + " is registered!"); &#125; /** * 通过sleep模拟服务器在线 */ public void sleep() &#123; try &#123; Thread.sleep(20000); &#125; catch (Exception e) &#123; System.out.println(e.toString()); &#125; &#125; public static void main(String[] args) throws Exception &#123; //连接至zk Server server = new Server(); server.connect(); //向zk注册服务器信息 String data = args[0]; server.register(data); server.sleep(); &#125;&#125; 服务器端的重点在于，程序启动时向ZooKeeper的指定节点下注册服务器信息，相当于通知ZooKeeper这个第三方：“服务器已上线”。其次，注册的节点类型必须是ephemeral节点，为了实现节点id自增(auto-increment)还可以使用ephemeral_sequential节点。 Client.java (客户端代码) package my.bigdata.zk;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import java.util.ArrayList;import java.util.Arrays;import java.util.List;public class Client &#123; private static final String HOST_ADDRESS = "localhost:2181"; private static final int DEFAULT_TIMEOUT = 2000; private static final String DEFAULT_SERVER_PARENT = "/servers"; private ZooKeeper zkConnect = null; private List&lt;String&gt; availableServers; /** * 连接至ZooKeeper * @throws Exception */ public void connect() throws Exception &#123; zkConnect = new ZooKeeper(HOST_ADDRESS, DEFAULT_TIMEOUT, new Watcher() &#123; @Override public void process(WatchedEvent watchedEvent) &#123; try &#123; updateServerCondition(); // 重复注册 &#125; catch (Exception e) &#123; System.out.println(e.toString()); &#125; &#125; &#125;); &#125; /** * 向zk查询服务器情况, 并update本地服务器列表 * @throws Exception */ public void updateServerCondition() throws Exception &#123; List&lt;String&gt; children = zkConnect.getChildren(DEFAULT_SERVER_PARENT, true); List&lt;String&gt; servers = new ArrayList&lt;&gt;(); for(String child : children) &#123; byte[] data = zkConnect.getData(DEFAULT_SERVER_PARENT + "/" + child, false, null); servers.add(new String(data)); &#125; availableServers = servers; System.out.println(Arrays.toString(servers.toArray(new String[0]))); &#125; /** * 通过sleep让客户端持续运行，模拟"监听" */ public void sleep() throws Exception&#123; System.out.println("client is working"); Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 连接zk Client client = new Client(); client.connect(); // 获取servers节点信息（并监听），从中获取服务器信息列表 client.updateServerCondition(); client.sleep(); &#125;&#125; 客户端的重点在于，它不断地向ZooKeeper某个特定节点（此处是servers节点）注册了一个Watcher，那么一旦该节点下的结构发生改变，ZooKeeper会向注册了Watcher的客户端发送“状态变化”的消息，那么客户端即可动态地从ZooKeeper中获取最新的服务器节点信息，甚至无需“主动”询问。 当然，ZooKeeper的应用场景还有很多，考虑到它本身也可拓展为一个分布式应用，在这种高可用性保证下它简直就是多个分布式应用的万能管家和协调者😊。]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>bigdata</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
</search>
