<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>[Raft]Leader Election(选主)笔记</title>
      <link href="/2018/12/09/raft-leader-election/"/>
      <url>/2018/12/09/raft-leader-election/</url>
      
        <content type="html"><![CDATA[<ol><li><p>Raft采用Leader - Follower的架构，主要的工作都由Leader来主动调度和完成，所以选出一个合法的Leader是首要任务。</p></li><li><p>为了保证不丢数据（<strong>特指已经commit且成功通知client的数据</strong>），Raft对Leader有严格的要求，不是集群里随便一台服务器都能够当上Leader，这种“严格的要求”大多体现在选主过程中。</p></li><li><p>选主过程最重要的限制是：</p><p><strong><em>Leader必须存有所有已经commit过的log entry，这样才能保证整个分布式系统的一致性。</em></strong></p><p><strong>在外部用户看来，已经commit成功的就必然是100%正常存储好了的。</strong>如果选出一个Leader，结果它上面缺了几条已经commit过的数据，那这个分布式系统就没有任何意义了。几分钟前跟用户说“我已经commit了，你放心吧”，过一会用户想查一查数据却发现“尼玛，怎么丢数据了，说好的已经commit了呢？！”</p></li><li><p>为了满足这个条件（Leader上必须存有所有已经commit过的数据），Raft中提出了两个限制条件，<strong>如果严格遵守以下两个条件，是能保证选出合法Leader的</strong>。</p><ul><li>在选主过程中，每台服务器只会投票给拥有比自己更新的log entry的服务器。</li><li>Leader在commit log entry的时候，只允许<strong>直接</strong>commit当前term（任期）的log entry，决不允许<strong>直接</strong>commit“历史”log entry。（历史log entry只能<strong>间接被动</strong>commit）</li></ul><p>（1）先来看看第一个条件：每台server只会投票给拥有比自己更新的log entry的server。这句话里最关键的是如何定义<strong>“更新的log entry”</strong>。按照Raft论文里给出的定义：</p><blockquote><p>Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs. If the logs have last entries with different terms, then the log with the later term is more up-to-date. If the logs end with the same term, then whichever log is longer is more up-to-date.</p></blockquote><ul><li>两台服务器上的<strong>最后一条</strong>log entry，如果它们的term不同，则term更大者是“更新”的。</li><li>两台服务器上的<strong>最后一条</strong>log entry，如果它们的term相同，则index更大者是“更新”的。</li></ul><p>（这里我觉得论文里漏了一个小细节，如果两条log entry的term和index都相同，<strong>应该会默认把发起投票的服务器当成更新的</strong>，term和index都一样的情况下不存在所谓的“更新”，一切为了选主的顺利进行，能快速有效选出leader才是最重要的。） </p><p><img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/raft_latest_1.png" alt=""> </p><p>如上图所示，根据Raft的判断标准，<strong>下者的记录比上者更新，因为下者最后一条log entry的term为5，5比4大</strong>。 </p><p><img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/raft_latest_2.png" alt=""> </p><p>如上图所示，根据Raft的判断标准，<strong>上者的记录比上者更新，因为上者最后一条log entry的index为4，下者最后一条log entry的index为3，4比3大</strong>。 </p><p>（2）再研究第二个条件：Leader只允许<strong>直接</strong>commit当前term（任期）的log entry，决不允许<strong>直接</strong>commit“历史”log entry。这条前提一看上去有点云里雾里的，感觉并没有什么必要性。<strong>实则不然，这条前提非常重要，是用来避免异常情况下频繁换Leader可能造成的commited log entry被覆盖的问题。</strong> </p><p>Raft原版论文里也给出了一个经典场景： </p><p><img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/raft_election.png" alt=""></p><ul><li>在(a)阶段，已经处于<code>term:2</code>了，<code>S1</code>是本轮（<code>term:2</code>）的leader。在<code>S1</code>上任后没多久，收到了客户端的请求，写入一条<code>{term:2, index:2}</code>的log，并将此log成功复制到<code>S2</code>这个小兄弟上。<strong>结果天不如人愿，还没来得及把log复制到其他小兄弟上，<code>S1</code>自己就宕机了。</strong></li><li>到了(b)阶段，<code>S1</code>宕机后，整个集群处于无领导状态。经过了一小会，<code>S5</code>很幸运率先结束timeout，自增term号（从<code>term:2</code>自增到<code>term:3</code>），发起了选主。由于此时<code>S3</code>和<code>S4</code>上的记录都和<code>S5</code>一样新，所以都愿意投票给<code>S5</code>，<code>S5</code>成功当选，成为leader。<code>S5</code>成为leader后马上又收到了客户端的请求，于是在本机上写入一条<code>{term:3, index:2}</code>的log。<strong>然而<code>S5</code>的命运比<code>S1</code>更惨，只来得及存在本地，来不及把log entry复制给任何一个小兄弟，直接就挂了。</strong></li><li>此时来到了(c)状态，<code>S5</code>挂了之后，<code>S1</code>成功重启恢复运转。<code>S1</code>自告奋勇，自增全局term号（从<code>term:3</code>自增到<code>term:4</code>），请求大家选主。<code>S2 ~ S4</code>都会投给<code>S1</code>，<code>S1</code>顺利当选leader。<code>S1</code>成了leader后做了第一件事情，发现之前<code>{term:2, index:2}</code>的log还没有成功复制给大部分兄弟，于是开始复制工作，把log复制到了<code>S3</code>上。突然<code>S1</code>又收到了客户端的请求，写入一条<code>{term:4, index:3}</code>的log到本机。</li></ul><p>以上3步都非常理所当然，没有任何的争议点，最大的争议点就出现在了(d)和(e)上。<strong>(d)和(e)实际上是(c)发生之后的两种互斥的可能情况，(d)是忽视第二条前提会发生的情况，(e)是满足第二条前提会发生的情况。</strong></p><ul><li>先看(d)。<strong>假设我们忽略第二条前提，也就是说leader可以随意commit任何term的log entry。</strong>那么在(c)结束之后，作为<code>term:4</code>的leader的<code>S1</code><strong>可以commit掉<code>{term:2, index:2}</code>的log，并且把结果返回给客户端。</strong>结果刚完成以上步骤，<code>S1</code>又倒霉地宕机了，<code>{term:4, index:3}</code>的log没来得及复制给任何一个小兄弟。过了一会，<code>S5</code>恢复正常，自增全局term号（从<code>term:4</code>自增到<code>term:5</code>），要求选主。由于此时<code>S5</code>上的最后一条log是<code>{term:3, index:2}</code>，比<code>S2 ~ S4</code>的都更新，大家都会投票给<code>S5</code>，<code>S5</code>成功当上<code>term:5</code>的leader。当上leader后，<code>S5</code>的第一件事情就是把它还没来得及复制给多个小兄弟的log复制出去，所以就造成了(d)状态，所有服务器上的log记录都被<code>S5</code>自己的log记录覆盖了。<strong>之前已经成功commit的<code>{term:2, index:2}</code>的log直接被覆盖，消失无踪⚠️！</strong></li><li>再看(e)。<strong>假设我们一定要坚守第二条前提，也就是说leader只能直接commit当前term的log entry，不能直接commit历史log entry。</strong>那么在(c)结束后，<code>S1</code>也不能commit<code>{term:2, index:2}</code>的log，因为<code>S1</code>此时是<code>term:4</code>的leader，而不是<code>term:2</code>的leader。只有如(e)所示，之后<code>S1</code>有机会commit属于当前term的log entry（<code>{term:4, index:3}</code>）时，才有机会<strong>间接地</strong>把之前<code>term:2</code>的历史记录也commit掉。即使此时<code>S1</code>宕机，<code>S5</code>也绝对不可能选上下一轮的leader，因为<code>S5</code>上最新的log<code>{term:3, index:2}</code>已经不如<code>S1 ~ S3</code>的新了，他拿不到过半的选票，做不了leader，也就不会发生已经commit的记录被覆盖的错误了。</li></ul><p>经过这一轮例子分析，可以很清晰地看到第二条前提的重要性了。如果允许leader随便直接commit历史记录的话，极端情况下很可能会造成数据丢失的系统错误（客户端知道你commit成功了，他就应该能放心了，结果过了一会你跟客户说commit也不算数，我搞丢了。。。。。。）<strong>所以，对于历史记录的commit只能被动触发！！！在commit当前term的log entry时顺便把之前未处理的log entry给commit掉。</strong></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 分布式 </category>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
            <tag> 算法 </tag>
            
            <tag> 分布式 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>[Golang]groupcache项目解析——Part2</title>
      <link href="/2018/12/06/groupcache-part2/"/>
      <url>/2018/12/06/groupcache-part2/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 本文写于2018/12/05，基于<code>Go 1.11</code>。 至于其他版本的Go SDK，如有出入请自行查阅其他资料。</p></blockquote><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>本文是《GroupCache项目解析》系列文章的第二篇，在上一篇(<a href="http://www.marcoma.xyz/index.php/2018/11/30/groupcache_part1/" target="_blank" rel="noopener">[Golang]groupcache项目解析——Part1</a>)中已经对<code>GroupCache</code>的背景、架构、代码结构作了介绍，也提供了一个简单的用例。现在来到了第二篇，主要是对<code>GroupCache</code>中的几个辅助性的模块作详细的讲解与分析，其中包括</p><ul><li>Consistent Hash模块</li><li>LRU模块</li><li>SingleFlight模块</li></ul><h4 id="Consistent-Hash（一致性哈希）"><a href="#Consistent-Hash（一致性哈希）" class="headerlink" title="Consistent Hash（一致性哈希）"></a>Consistent Hash（一致性哈希）</h4><p>所谓的<code>一致性哈希</code>，根本目的就是将数据打散，均匀地分布到集群上多个不同的节点。它和普通哈希最不同的地方在于，<strong>除了数据外，它把节点本身（ip地址或者节点id）也进行了哈希，放到和数据同一个哈希空间内。</strong>（具体可参考本人之前的文章<a href="http://www.marcoma.xyz/index.php/2018/03/17/consistent_hash/" target="_blank" rel="noopener">《一致性Hash算法——分析与模拟》</a>） 接下来看看<code>groupcache</code>里面的具体代码。 先看数据结构的定义。</p><pre><code>// Hash就是一个返回unit32的哈希方法type Hash func(data []byte) uint32  // Map就是一致性哈希的高级封装type Map struct {    hash     Hash       // 哈希算法    replicas int        // replica参数，表明了一份数据要冗余存储多少份    keys     []int  // 存储hash值，按hash值升序排列（模拟一致性哈希环空间）    hashMap  map[int]string     // 记录hash值 -&gt; 节点ip地址的映射关系}</code></pre><p>接下来看看工厂方法。</p><pre><code>// 一致性哈希的工厂方法func New(replicas int, fn Hash) *Map {    m := &amp;Map{        replicas: replicas,        hash:     fn,        hashMap:  make(map[int]string),    }    if m.hash == nil {        m.hash = crc32.ChecksumIEEE // 不指定自定义Hash方法的话，默认用ChecksumIEEE    }    return m}</code></pre><p>最后分析最关键的<code>Add</code>和<code>Get</code>方法。</p><pre><code>// Add方法，参数为...string，一般就是多个节点的ip地址（或者节点id）func (m *Map) Add(keys ...string) {    for _, key := range keys {        // 每一个key都会冗余多份（每份冗余就是一致性哈希里的虚拟节点 v-node）        for i := 0; i &lt; m.replicas; i++ {            // 1. 先算出当前冗余的hash值            // 2. 把hash值塞进哈希环里            // 3. 记录下hash值 -&gt; 节点ip地址的映射，之后可以凭借hash值找到具体服务器地址            hash := int(m.hash([]byte(strconv.Itoa(i) + key)))            m.keys = append(m.keys, hash)             m.hashMap[hash] = key           }    }    sort.Ints(m.keys)   // 一致性哈希要求哈希环是升序的，最后执行一次排序操作}// Get方法，输入一个key，找到该key应该存于哪个节点，返回该节点的地址func (m *Map) Get(key string) string {    if m.IsEmpty() {        return &quot;&quot;    }    // 1. 算出key的hash值    // 2. 二分查找大于等于该key的第一个hash值的下标（哈希环是升序有序的，所以可以二分查找）    hash := int(m.hash([]byte(key)))    idx := sort.Search(len(m.keys), func(i int) bool { return m.keys[i] &gt;= hash })    // 下标越界，循环找到到0号下标    if idx == len(m.keys) {        idx = 0    }    // 通过查询记录了hash -&gt; 节点地址的hashMap，得到节点地址，返回    return m.hashMap[m.keys[idx]]}</code></pre><p>通过上述代码可以看到，<code>groupcache</code>中的一致性哈希非常简单清晰。在<code>groupcache</code>里用到一致性哈希的地方，就是多节点部署时，要把多个节点地址用一致性哈希管理起来，从而让缓存数据能够均匀分散，降低单台服务器的压力。 <strong>但是这里实现的一致性哈希还比较粗糙，没有实现动态删除节点，还不支持节点宕机后自动数据迁移，这两个功能是一致性哈希的另一大精髓。（感兴趣的可参考我之前的文章）</strong></p><h4 id="LRU"><a href="#LRU" class="headerlink" title="LRU"></a>LRU</h4><p>第二个模块我们来研究下<code>LRU</code>。所谓<code>LRU</code>其实就是操作系统里那个内存页管理的经典算法——最近最少被使用（Least Recently Used Algorithm）。<strong>其实除了操作系统底层，很多数据库或者缓存产品里都实现了<code>LRU</code>，例如<code>Innodb</code>存储引擎的<code>buffer pool</code>里的LRU List就是一个关键数据结构。</strong> <code>LRU</code>的思想非常朴素，基本都是基于一条双向链表，无非就是热门的、经常被访问的数据就放到链表头部，久而久之冷门数据就会被“排挤”到链表尾部，当内存不够时把尾部的数据移除，清理出更多空间来存新的数据。 在<code>groupcache</code>里，<code>LRU</code>用来存最底层的K-V数据，先来看看数据结构的定义。</p><pre><code>// Key是任意可比较（Comparable）类型type Key interface{}// entry是一个K-V对，value也是任意类型（不必Comparable）type entry struct {    key   Key    value interface{}}// LRU的高层封装（非并发安全！）type Cache struct {    MaxEntries int  // 最多允许存多少个K-V entry    OnEvicted func(key Key, value interface{})  // 回调函数，当一个entry被移除后回调    ll    *list.List    // LRU链表    cache map[interface{}]*list.Element // 记录Key -&gt; entry的映射关系，O(1)时间得到entry}</code></pre><p>接下来看看关键的<code>Add</code>和<code>Get</code>方法。</p><pre><code>// Add方法，插入一个K-V对func (c *Cache) Add(key Key, value interface{}) {    if c.cache == nil {        c.cache = make(map[interface{}]*list.Element)        c.ll = list.New()    }    // 如果该key已存在，更新entry里的value值，并将entry挪到链表头部    if ee, ok := c.cache[key]; ok {        c.ll.MoveToFront(ee)        ee.Value.(*entry).value = value        return    }    // 如果该key不存在，新建一个entry，插到链表头部    ele := c.ll.PushFront(&amp;entry{key, value})    c.cache[key] = ele    // 如果超出链表允许长度，移除链表尾部的数据    if c.MaxEntries != 0 &amp;&amp; c.ll.Len() &gt; c.MaxEntries {        c.RemoveOldest()    }}// Get方法，通过Key来拿对应的valuefunc (c *Cache) Get(key Key) (value interface{}, ok bool) {    if c.cache == nil {        return    }    // 如果该key存在，获取对应entry的value，将该entry挪到链表头部，返回    if ele, hit := c.cache[key]; hit {        c.ll.MoveToFront(ele)        return ele.Value.(*entry).value, true    }    return}</code></pre><h4 id="SingleFlight"><a href="#SingleFlight" class="headerlink" title="SingleFlight"></a>SingleFlight</h4><p><code>SingleFlight</code>是一个非常重要的模块，看它的名字里有一个<code>Single</code>有一个<code>Flight</code>，其实<code>Single</code>指的是N条对同一个key的查询命令中<strong>只有1条被真正执行</strong>，而<code>Flight</code>大家就把它等价于<code>Execution</code>就行了。 先来看看<code>SingleFlight</code>里的数据结构的定义。</p><pre><code>// call等价于一条被真正执行的对某个key的查询操作type call struct {    wg  sync.WaitGroup  // 用于阻塞对某个key的多条查询命令，同一时刻只能有1条真正执行的查询命令    val interface{}     // 查询结果，也就是缓存中某个key对应的value值    err error}// Group相当于一个管理每个key的call请求的对象type Group struct {    mu sync.Mutex       // 并发情况下，保证m这个普通map不会有并发安全问题    m  map[string]*call // key为数据的key，value为一条call命令，记录下某个key当前时刻有没有客户端在查询}</code></pre><p>接下来看看<code>SingleFlight</code>里面唯一一个，也是最重要的一个方法——<code>Do()</code></p><pre><code>// Do里面是查询命令执行的逻辑。// 当客户端想查询某个key对应的值时会调用Do方法来执行查询。// 参数传入一个待查询的key，还有一个对应的查询方法，返回key对应的value值func (g *Group) Do(key string, fn func() (interface{}, error)) (interface{}, error) {    // 为了保证普通map的并发安全，要先上锁    g.mu.Lock()    // 检查map有无初始化    if g.m == nil {        g.m = make(map[string]*call)    }    // 检查当前时刻，该key是否已经有别的客户端在查询    // 如果有别的客户端也正在查询，map里肯定存有该key，以及一条对应的call命令    if c, ok := g.m[key]; ok {        g.mu.Unlock()   // 解锁，自己准备阻塞，此时已不存在并发安全问题，允许别人进行查询        c.wg.Wait() // 阻塞，等待别的客户端完成查询就好，不用自己再去耗费资源查询        return c.val, c.err // 阻塞结束，说明别人已经查询完成，拿来主义直接返回    }    // 如果能执行到此步，说明当前时刻没有别人在查询该key，当前客户端是    // 当前时刻第一个想要查询该key的人，就插入一条key -&gt; call记录    // 注意，此时的map仍然是上锁状态，因为还要对map进行插入，有并发安全问题    c := new(call)    c.wg.Add(1)    g.m[key] = c    g.mu.Unlock()    // 执行作为参数传入的查询方法    // **同一时刻对于同一个key只可能有一个客户端执行到此处**    c.val, c.err = fn()    c.wg.Done()    // 执行完查询方法，把map中的key -&gt; call删掉    g.mu.Lock()    delete(g.m, key)    g.mu.Unlock()    return c.val, c.err}</code></pre><p>结合上述代码注释里的分析，<code>SingleFlight</code>的逻辑应该很清楚了。特别提一提里面的几个思维亮点：</p><ul><li>时刻谨记go里面的普通map不是并发安全的，要在有并发安全隐患的地方手动上锁和解锁。</li><li>用一个map来记录key与查询请求，可以迅速得知（理想情况下O(1)）当前时刻某个key是否有人在执行查询。</li><li>本来用<code>set</code>类容器来存当前正被人查询的key也可以完成以上需求。但是第二个亮点就是<code>call</code>结构，<code>call</code>里面封装了一个<code>WaitGroup</code>和一个<code>val</code>。当某一时刻有N个对某个key的查询请求，通过<code>WaitGroup</code>来阻塞其中的N-1个，只执行1次查询方法，然后把查询结果塞到<code>call.val</code>中，通知<code>WaitGroup</code>完成任务。这样做，不仅执行查询的那一个“天选之子”可以返回该值，而且那N-1个被阻塞的也可以直接取<code>call.val</code>作为结果返回。</li></ul><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p><code>groupcache</code>这个项目的代码量虽不多，但有很多精华的地方。如</p><ul><li>实现<strong>一致性哈希</strong>来管理多节点</li><li>实现<code>LRU</code>算法来管理底层K-V数据</li><li>实现<code>SingleFlight</code>来提高并发查询效率</li></ul><p>其中，<code>SingleFlight</code>的逻辑最让我开了眼界。之前对于“并发查询”的优化方面，我考虑的可能也就是如何优化存储的数据结构，或者类似于把请求分发到多台机器上处理，用多机的计算能力来抗。但是这些都不如<code>SingleFlight</code>里的逻辑这么粗暴明了，同时又高效。</p>]]></content>
      
      
      <categories>
          
          <category> Go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Go </tag>
            
            <tag> 笔记 </tag>
            
            <tag> 源码 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>[Golang]groupcache项目解析——Part1</title>
      <link href="/2018/11/30/groupcache-part1/"/>
      <url>/2018/11/30/groupcache-part1/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。 本文写于2018/11/29，基于<code>Go 1.11</code>。 至于其他版本的Go SDK，如有出入请自行查阅其他资料。</p></blockquote><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>本文是《groupcache项目解析》系列文章的第一篇，在本文中主要是对<code>groupcache</code>这个项目作简单的介绍，包括其</p><ul><li>架构</li><li>代码文件结构</li><li>简单用例</li></ul><h4 id="groupcache简介"><a href="#groupcache简介" class="headerlink" title="groupcache简介"></a>groupcache简介</h4><p><code>groupcache</code>(<a href="https://github.com/golang/groupcache" target="_blank" rel="noopener">on Github</a>)是一个用<code>Go</code>实现的K-V cache的库，可以起到<code>memcached</code>的<strong>部分功能</strong>。它支持单节点部署，也支持多节点部署。 其中最值得一提的两个特性是：</p><ul><li>不支持update和delete（基本只能用于静态资源缓存）</li><li>热门缓存自动镜像（auto mirroring）</li></ul><p><strong>⚠️注意，<code>groupcache</code>并不是一个可直接运行的存储组件，不像MySQL或者Redis之类那样提供编译后的可运行程序。<code>groupcache</code>只是一个K-V cache的第三方库，基于它的代码可以自己写代码实现一个cache层。</strong></p><h4 id="groupcache架构"><a href="#groupcache架构" class="headerlink" title="groupcache架构"></a>groupcache架构</h4><ol><li><p>节点管理 以上提到，<code>groupcache</code>是一个支持多节点部署的K-V cache。当有多个存储节点时，内部会以<code>consistent hash</code>（一致性哈希）的方式管理多个节点。关于一致性哈希的解析，详情可参考我之前的文章<a href="http://www.marcoma.xyz/index.php/2018/03/17/consistent_hash/" target="_blank" rel="noopener">《一致性Hash算法——分析与模拟》</a>。 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/hash.png" alt=""> 通过一致性哈希，可以统一管理多个节点。如果哈希算法设计得比较好，可以把大量K-V数据均匀打散，存储到不同的节点上。</p></li><li><p>group（存储组） 在<code>groupcache</code>里，<code>group</code>是一个相对独立的存储容器，每个<code>group</code>都有自己的名字，多个<code>group</code>之间不共享数据。然而<code>group</code>只是一个<strong>逻辑概念</strong>，一个<code>group</code>里存的K-V数据是可以存在多个分散的物理节点上的（<strong>分散的策略依赖于一致性哈希算法</strong>）。也就是说每个物理节点上实际上存了多个<code>group</code>的K-V数据，组与组之间的访问隔离全靠<code>groupcache</code>的代码逻辑来实现。 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/group.png" alt=""></p></li><li><p>缓存系统</p><ul><li><p>LRU 在缓存系统的底层，每个K-V Entry都是通过一条LRU链表来管理的。经常被访问的数据会被放置在LRU链表的前端，久而久之冷数据会下沉到链表尾端，甚至直接被移出链表。</p></li><li><p>并发查询优化 在<code>groupcache</code>中，如果某节点同时收到N个对于同一个key的查询请求，但是请求的key不在当前节点上，<code>groupcache</code>会自动<strong>阻塞</strong>N-1个请求，只执行其中一个请求，去其他节点或者数据库中fetch数据。最后才恢复N个请求，把数据放到N个请求中返回。因为无论多少个对同一个key的查询请求并发到达，只执行一次查询，所以并发查询效率很高。</p></li><li><p>热门缓存自动镜像 每个节点都包含了两类缓存：<code>main cache</code>（属于本节点的数据）和<code>hot cache</code>（不属于本节点但是全局热门的数据）。当节点收到了对某个key的查询请求，它首先会检查本地<code>hot cache</code>中有没有，如果没有就再看看该key是不是属于本节点的数据，如果不是就向兄弟节点请求。所谓的<strong>自动镜像</strong>，指的是从兄弟节点处返回的数据可以缓存在本节点的<code>hot cache</code>里，虽然自身<strong>没有那个数据的存储权限</strong>，但是可以存储成一份热门数据的镜像，以后再收到对该key的请求，无需再向兄弟节点请求，浪费网络资源。</p></li></ul></li></ol><h4 id="groupcache代码模块"><a href="#groupcache代码模块" class="headerlink" title="groupcache代码模块"></a>groupcache代码模块</h4><ul><li><p><code>consistenthash</code> <code>consistenthash</code>模块实现了简单的一致性哈希算法。数据（一般是节点地址）进入一致性哈希后，会被自动冗余得到多个备份（取决于<code>replica</code>的设定值），然后插到一致性哈希环上。</p></li><li><p><code>groupcachepb</code> <code>groupcachepb</code>模块里，用了第三方库<code>protobuf</code>生成了统一的<code>Request</code>和<code>Response</code>结构，供节点间网络通信使用。</p></li><li><p><code>lru</code> <code>lru</code>模块实现了经典的LRU算法，用<code>container/list</code>里的链表实现。</p></li><li><p><code>singleflight</code> <code>singleflight</code>模块非常重要。正如它名字里的<code>single</code>，它是用来保证多个对同一个key的请求不被多次执行的。也就是上面简介所说的<strong>并发查询优化</strong>。</p></li><li><p><code>testpb</code> <code>testpb</code>，测试<code>protobuf</code>结构。</p></li><li><p><code>byteview</code> <code>byteview</code>是一个对byte数组或者字符串的封装，在外部看来，<code>groupcache</code>里的所有K-V数据最终都是落盘到byte上，都是对<code>byteview</code>的读写操作。</p></li><li><p><code>groupcache</code> 核心代码文件，其中定义了<code>Group</code>、<code>GetterFunc</code>、<code>Stats</code>等多个关键数据结构，以及对应的方法。</p></li><li><p><code>http</code> 核心代码文件，定义了<code>HTTPPool</code>以及对应的方法，包含了各种网络通信的逻辑。</p></li><li><p><code>peers</code> 定义了节点的相关操作。</p></li><li><p><code>sinks</code> <code>sinks</code>里定义了<code>Sink</code>接口以及多种不同的sink。其实sink可以理解为一种特殊容器，当节点收到对某个key的查询请求，但是本地没有数据，需要到远程数据库里读取时，会把读取回来的数据下沉到sink容器里面，最后再把数据转成byteview塞到本地缓存里。</p></li></ul><h4 id="Quick-Start-Example"><a href="#Quick-Start-Example" class="headerlink" title="Quick Start Example"></a>Quick Start Example</h4><p>以下提供一个简单的<code>groupcache</code>使用例子。</p><pre><code>package mainimport (    &quot;github.com/golang/groupcache&quot;    &quot;io/ioutil&quot;    &quot;log&quot;    &quot;net/http&quot;    &quot;os&quot;)var (    // 简单起见，hardcode一段兄弟节点的地址    peers = []string{&quot;http://127.0.0.1:8001&quot;, &quot;http://127.0.0.1:8002&quot;, &quot;http://127.0.0.1:8003&quot;})func main() {    // 先建好http连接池，表明当前节点的兄弟节点有哪些    host := os.Args[1]    localAddr:= &quot;http://&quot; + host    localHttpPool := groupcache.NewHTTPPool(localAddr)    localHttpPool.Set(peers...)    // 定义一个逻辑上的分组，叫fileCacheGroup，用来缓存文件内容，缓存大小为64MB    // 当本地缓存miss时，直接读取磁盘上的文件    var fileCacheGroup = groupcache.NewGroup(&quot;file&quot;, 64&lt;&lt;20, groupcache.GetterFunc(        func(ctx groupcache.Context, key string, dest groupcache.Sink) error {            result, err := ioutil.ReadFile(key)            if err != nil {                log.Println(&quot;Get file error.&quot;)                return err            }            log.Printf(&quot;Trying to get %s\n&quot;, key)            dest.SetBytes([]byte(result))            return nil        }))    // 为了测试，建立一个对外的http服务，路由是host:port/file_cache?fname={}    http.HandleFunc(&quot;/file_cache&quot;, func(rw http.ResponseWriter, r *http.Request) {        var value []byte        key := r.URL.Query().Get(&quot;fname&quot;)        fileCacheGroup.Get(nil, key, groupcache.AllocatingByteSliceSink(&amp;value))        rw.Write([]byte(value))    })    // 启动http服务    log.Fatal(http.ListenAndServe(host, nil))}</code></pre><p>如需测试上述代码，可编译后直接命令行执行（记得带上host参数如<code>127.0.0.1:8001</code>）。由于cache miss的逻辑是在本地磁盘上读取文件，所以可以先在目录下新建几个垃圾文本文件，里面随便填充一些内容，进行测试。 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/groupcache_test.png" alt=""> <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/postman_cache.png" alt=""> 测试结果如上图所示。</p><ol><li>先在本地随便新建一个<code>hh.txt</code>文件，里面写上<code>This is hh.txt!</code>。</li><li>然后编译完上述例子程序后，直接传入参数<code>127.0.0.1:8001</code>以命令行启动程序。</li><li>用postman访问代码中定义好的服务路由（<code>host:port/file_cache?fname={}</code>），测试缓存服务，得到response结果为<code>hh.txt</code>的文件内容。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Go </tag>
            
            <tag> 源码 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>[Golang]浅析几种并发模式</title>
      <link href="/2018/11/25/go-concurrency-pattern/"/>
      <url>/2018/11/25/go-concurrency-pattern/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。</p><p>本文写于2018/11/24，基于Go 1.11。<br>至于其他版本的Go SDK，如有出入请自行查阅其他资料。</p></blockquote><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>最近读完了整本《Go in Action》，给我印象最深的几章是讲</p><ul><li>多态</li><li>高级并发模式</li><li>常用工具包（<code>http</code>, <code>json</code>, <code>log</code>……）</li></ul><p>本文基于《Go in Action》里介绍的3种高级并发模式进行浅析，主要起到解释和笔记的作用，也会简单地讲讲我个人对这几种模式的理解。</p><h4 id="并发模式"><a href="#并发模式" class="headerlink" title="并发模式"></a>并发模式</h4><ol><li><p>任务计时器</p><p> 何谓“任务计时器”？**其实就是一个包装了多个要执行的<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+ 所有```Task```执行完毕，正常退出</span><br><span class="line">+ 收到外部中断信号（interrupt），退出</span><br><span class="line">+ ```Timer```超过设定的时长，退出</span><br><span class="line"></span><br><span class="line">先来直接看看《Go in Action》中给出的实现代码。</span><br></pre></td></tr></table></figure></p><p> package runner</p><p> import (</p><pre><code>&quot;errors&quot;&quot;os&quot;&quot;os/signal&quot;&quot;time&quot;</code></pre><p> )</p><p> // 任务计时器<br> type Runner struct {</p><pre><code>interrupt chan os.Signal    // 接收中断信号的channelcomplete chan error            // 接收任务完成信号的channeltimeout &lt;-chan time.Time    // 接收超时信号的channeltasks []func(int)            // 存放要执行的多个任务的切片                            // 其中每个任务是一个以int为形参的方法</code></pre><p> }</p><p> // 定义了两种异常退出的错误<br> var ErrTimeout = errors.New(“received timeout”)<br> var ErrInterrupt = errors.New(“received interrupt”)</p><p> // 容器工厂，通过new直接得到一个容器实例<br> func New(d time.Duration) *Runner {</p><pre><code>return &amp;Runner{    interrupt: make(chan os.Signal, 1),    complete:  make(chan error),    timeout:   time.After(d),}</code></pre><p> }</p><p> // 往容器里添加任务的方法<br> func (r *Runner) Add(tasks …func(int)) {</p><pre><code>r.tasks = append(r.tasks, tasks...)</code></pre><p> }</p><p> // 容器启动的方法<br> func (r *Runner) Start() error {</p><pre><code>// We want to receive all interrupt based signals.signal.Notify(r.interrupt, os.Interrupt)// Run the different tasks on a different goroutine.go func() {    r.complete &lt;- r.run()}()select {// Signaled when processing is done.case err := &lt;-r.complete:    return err// Signaled when we run out of time.case &lt;-r.timeout:    return ErrTimeout}</code></pre><p> }</p><p> // 容器内部执行任务的方法<br> func (r *Runner) run() error {</p><pre><code>for id, task := range r.tasks {    // Check for an interrupt signal from the OS.    if r.gotInterrupt() {        return ErrInterrupt    }    // Execute the registered task.    task(id)}return nil</code></pre><p> }</p><p> // 检查是否发生中断信号<br> func (r *Runner) gotInterrupt() bool {</p><pre><code>select {case &lt;-r.interrupt:    // Stop receiving any further signals.    signal.Stop(r.interrupt)    return truedefault:    return false}</code></pre><p> }</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">上述代码的逻辑很清晰，但仍有几个细节需要特别强调一下。</span><br><span class="line"></span><br><span class="line">+ 在容器```Runner```里，```timeout```是一个接收超时信号（```time.Time```）的channel，一旦该channel接收到**一个**超时信号，将通知```Runner```退出。而在new一个新的```Runner```时，我们用的是```time.After(duration)```来生成一个```timeout```管道，为什么不直接```make(chan time.Time)```呢？因为用```time.After(duration)```生成管道时，会潜在自动触发一个机制：经过```duration```后该管道会收到一个```time.Time```信号，不需要自己额外去做发送超时信号这一套逻辑。</span><br><span class="line">+ 使用```select```语句。可以看到在```Start()```中，开了一个goroutine去执行任务后，我们写了一个```select```语句，其中两个分支分别是所有任务正常完成且收到complete信号，还有任务超时收到超时信号。**其实```select```语句可以简单地看成是一个定制版的```epoll```机制**，它可以同时监听多个```case```。如果所有的```case```分支都不能执行，将阻塞在此；如果有一个```case```可以执行，一定会执行该```case```；和```epoll```唯一不同的是，如果同时有多个```case```可以执行，```select```会**随机**执行其中一个```case```。同理，在```gotInterrupt()```里，我们也用了```select```语句监听有没有中断信号，如果在执行```gotInterrupt()```的那个时刻没有收到中断信号，那绝对会直接执行```default```分支（永远都能执行的分支），返回```false```告知调用者没有收到中断信号。</span><br><span class="line">+ 在容器```Runner```里，```tasks```是一个装了多个待执行任务的切片，定义里说明了每个任务都是一个```func(int)```。**但是这并不具有普适性，这不是必须的，设计者可以根据自己的需求来定义每个任务是什么样的方法。**可以是```func(interfact&#123;&#125;) interface&#123;&#125;```，可以是任意的方法。</span><br><span class="line">+ ```run()```方法用于在容器内部执行多个任务。最奇怪的是，在```run()```里面其实是**遍历了```tasks```，逐个逐个串行地执行任务，只有上一个任务完成了下一个任务才会开始。**个人认为，也许这种设计迎合了一定的场景需求（上下游任务间存在依赖），但是有些时候我们确实是需要并发地执行多个任务。**建议改成多个```goroutine```并发执行```tasks```中的任务，然后用```WaitGroup```来阻塞，等待所有任务完成。**</span><br></pre></td></tr></table></figure><pre><code>func (r *Runner) run() error {    wg := sync.WaitGroup{}    wg.Add(len(r.tasks))    for id, task := range r.tasks {        if r.gotInterrupt() {            return ErrInterrupt        }        // 开多个goroutine并发执行多个任务，用WaitGroup来统一        go func() {            defer wg.Done()            task(id)        }()    }    wg.Wait()    return nil}<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">2. 资源池</span><br><span class="line"></span><br><span class="line">什么是**资源池**，顾名思义，就是一个放满了各种资源的池子。讲得“专业”一点，就是一个管理着多个可用资源的容器，外部的线程/协程可向资源池申请资源，使用完后可以把资源放回资源池，重复利用。</span><br><span class="line"></span><br><span class="line">直接来看看《Go in Action》中的代码。</span><br></pre></td></tr></table></figure></code></pre><p> package pool</p><p> import (</p><pre><code>&quot;errors&quot;&quot;io&quot;&quot;log&quot;&quot;sync&quot;</code></pre><p> )</p><p> // 资源池<br> type Pool struct {</p><pre><code>m         sync.Mutex        // mutex用于控制同步resources chan io.Closer    // 存放资源的管道factory   func() (io.Closer, error) // 新建资源的工厂closed    bool                // 资源池是否关闭的标志</code></pre><p> }</p><p> // 错误信号<br> var ErrPoolClosed = errors.New(“Pool has been closed.”)</p><p> // 资源池工厂，用于新建资源池<br> func New(fn func() (io.Closer, error), size uint) (*Pool, error) {</p><pre><code>if size &lt;= 0 {    return nil, errors.New(&quot;Size value too small.&quot;)}return &amp;Pool{    factory:   fn,    resources: make(chan io.Closer, size),}, nil</code></pre><p> }</p><p> // 从资源池中拿资源<br> func (p *Pool) Acquire() (io.Closer, error) {</p><pre><code>select {case r, ok := &lt;-p.resources:    log.Println(&quot;Acquire:&quot;, &quot;Shared Resource&quot;)    if !ok {        return nil, ErrPoolClosed    }    return r, nildefault:    log.Println(&quot;Acquire:&quot;, &quot;New Resource&quot;)    return p.factory()}</code></pre><p> }</p><p> // 把资源放回资源池<br> func (p *Pool) Release(r io.Closer) {</p><pre><code>p.m.Lock()defer p.m.Unlock()if p.closed {    r.Close()    return}select {case p.resources &lt;- r:    log.Println(&quot;Release:&quot;, &quot;In Queue&quot;)default:    log.Println(&quot;Release:&quot;, &quot;Closing&quot;)    r.Close()}</code></pre><p> }</p><p> // 关闭资源池<br> func (p *Pool) Close() {</p><pre><code>p.m.Lock()defer p.m.Unlock()if p.closed {    return}p.closed = trueclose(p.resources)for r := range p.resources {    r.Close()}</code></pre><p> }</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">以下讲讲几个资源池设计中的细节。</span><br><span class="line">+ 关于```factory```。```factory```是一个用来新建资源的工厂，当资源池内没有资源时，上述代码的逻辑是通过事先定义好的```factory```来新建资源。**当然，也可以不这么干。根据不同的场景和需求，当资源池内没有资源时可以选择阻塞，而非新建资源。**</span><br><span class="line">+ 用```mutex```来同步```Release()```和```Close()```。在同一时刻，不能同时有多个协程进入```Release()```和进入```Close()```。也就是说有协程在关闭资源池时，不允许别的协程放回资源；有协程在放回资源时，不允许别的协程关闭资源池。如果不用```mutex```进行同步，假设同时有协程A进入了```Release()```，协程B进入了```Close()```。特殊情况下，当协程A运行到select语句前失去了cpu资源，协程B正常运行关闭了```resources```管道，协程A再想往一个已关闭了的```resources```管道里插数据，会直接引起错误。</span><br><span class="line">+ ```resources```管道是一个```缓冲管道```。通过工厂new一个资源池时，```resources```被定义成了一个大小为```size```的```缓冲管道```。用```缓冲管道```的好处是，只有当管道**全空**或者**全满**时才会对生产者/消费者进行阻塞，其他情况下正常生产/消费资源。</span><br><span class="line"></span><br><span class="line">3. 并发池</span><br><span class="line"></span><br><span class="line">所谓**并发池**，就是一个放了N个待执行任务的池子，或者可以看成是一个**可并发执行任务，却不带有定时功能的任务计时器。**</span><br></pre></td></tr></table></figure><p> package work</p><p> import “sync”</p><p> // 任务接口<br> type Worker interface {</p><pre><code>Task()</code></pre><p> }</p><p> // 并发池<br> type Pool struct {</p><pre><code>work chan Workerwg   sync.WaitGroup</code></pre><p> }</p><p> // 并发池工厂，用于新建并发池<br> func New(maxGoroutines int) *Pool {</p><pre><code>p := Pool{    work: make(chan Worker),}p.wg.Add(maxGoroutines)for i := 0; i &lt; maxGoroutines; i++ {    go func() {        for w := range p.work {            w.Task()        }        p.wg.Done()    }()}return &amp;p</code></pre><p> }</p><p> // 提交任务<br> func (p *Pool) Run(w Worker) {</p><pre><code>p.work &lt;- w</code></pre><p> }</p><p> // 关闭并发池<br> func (p *Pool) Shutdown() {</p><pre><code>close(p.work)p.wg.Wait()</code></pre><p> }<br> <code>`</code></p><p> 基本上实现的功能跟<strong>优化后</strong>的任务计时器一样（除了不支持定时），支持多<code>goroutine</code>并发执行。</p></li></ol><h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><ol><li>任务计时器适用于各种监控任务，或者对执行时间有限制的任务。</li><li>资源池多用于管理各种连接，例如数据库连接，提高连接的复用性。</li><li>并发池多用于计算密集型任务，需要多个<code>goroutine</code>并发执行多个小任务。</li></ol><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>《Go in Action》中介绍的3种并发模式都非常实用，也有很强的普适性。但是在实际项目中还是需要搞清楚具体的需求，要清楚它们的assumption和它们的缺点，并不是100%地适用于所有场景，要基于这几种基本的模式作更深层次的自定义开发。</p>]]></content>
      
      
      <categories>
          
          <category> Go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Go </tag>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>[Golang]更正“神贴”《如何优雅地关闭Go Channel》</title>
      <link href="/2018/11/20/close-channel/"/>
      <url>/2018/11/20/close-channel/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。</p><p>本文写于2018/11/19，基于<figure class="highlight go"><figcaption><span>1.11```。</span></figcaption><table><tr><td class="code"><pre><span class="line">&gt; 至于其他版本的Go SDK，如有出入请自行查阅其他资料。</span><br><span class="line"></span><br><span class="line">#### Introduction</span><br><span class="line"></span><br><span class="line">现在在网上搜索<span class="string">``</span><span class="string">`Go`</span><span class="string">``</span>，<span class="string">``</span><span class="string">`Channel`</span><span class="string">``</span>，<span class="string">``</span><span class="string">`关闭`</span><span class="string">``</span>等关键词时，一定会搜到一篇好几年前的“神贴”**《How To Gracefully Close Channels》**（[原文链接请点击](https:<span class="comment">//go101.org/article/channel-closing.html)），或者各种国人执笔的中文直译版**《如何优雅地关闭Go Channel》**。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">基于<span class="string">``</span><span class="string">`Go`</span><span class="string">``</span>本身对并发的强支持，不断地有开发者需要学习<span class="string">``</span><span class="string">`Channel`</span><span class="string">``</span>这个被设计为多个<span class="string">``</span><span class="string">`goroutine`</span><span class="string">``</span>间**安全**传递数据的内置数据结构，自然也有很多人学习过以上这篇文章。**然而不知道是受当时写作的时间背景限制，还是其他原因，实际上这篇文章里有一些比较严重的缺陷，必须得到纠正。**</span><br><span class="line"></span><br><span class="line">我这么一个<span class="string">``</span><span class="string">`Go`</span><span class="string">``</span>的初学者，研究过后暂时**没有**发现网上对原贴进行优化或者更正的博客，所以我决定写下这篇更正“神贴”的博客，并提供一些我的关闭<span class="string">``</span><span class="string">`Go Channel`</span><span class="string">``</span>的解决方案。</span><br><span class="line"></span><br><span class="line">#### 驳斥理由</span><br><span class="line"></span><br><span class="line">+ 原文标题的核心是<span class="string">``</span><span class="string">`Close Channels`</span><span class="string">``</span>，然而除了第一个最简单的例子（M receivers，one sender）里有关闭数据channel的逻辑外，其他几个复杂例子中**压根没有<span class="built_in">close</span> channel，仅仅只是退出sender**</span><br><span class="line">+ 即使我们把“退出sender”等价于“<span class="built_in">close</span> channel”。但是同样除了第一个最简单的例子（M receivers，one sender）里是sender主动关闭外，其他几个例子中退出sender都是由receiver触发的，类似receiver读到一个什么特殊值就提示sender停止生产。**我不否认在某些场景里确实需要receiver提示sender何时结束**（例如receiver发生了异常，无法继续处理，可通知sender赶紧结束）。**但是在很多场景里是需要sender自己触发停止生产的，而不是让receiver告知才停止**（例如<span class="number">100</span>个sender分别读<span class="number">100</span>台机器上的文件，然后把文件数据怼到一个channel里，此时肯定不可能让receiver来主导sender的数据读取何时停止，对吧）。</span><br><span class="line">+ 最后一个问题，原贴例子里receiver读到一个特殊值导致退出后，并没有安排别的<span class="string">``</span><span class="string">`goroutine`</span><span class="string">``</span>去读完channel中可能剩下的数据，直接导致数据丢失。（当然作者也意识到这一点了，所以在最后提了一下，说读完剩下的数据很简单blabla，我就不写了你们自己去实现就好了）</span><br><span class="line"></span><br><span class="line">#### 解决方案</span><br><span class="line">其实很明显，关闭channel最主要的麻烦在于sender端如何控制，既不能不去关闭，也不能重复关闭（<span class="string">``</span><span class="string">`panic`</span><span class="string">``</span>）。所以接下来就讨论两种在sender端关闭channel的解决方案：单个sender和多个sender的应用场景。</span><br><span class="line"></span><br><span class="line">+ 单个sender</span><br></pre></td></tr></table></figure></p></blockquote><pre><code>package mainimport (    &quot;fmt&quot;    &quot;sync&quot;)// 一个sender，一个receiverfunc main() {    dataChannel := make(chan int, 100)    done := make(chan interface{})    go sender(dataChannel)    go receiver(dataChannel, done)    // 阻塞直到receiver完成，避免主线程马上退出    &lt;-done    fmt.Println(&quot;Done.&quot;)}func sender(dataChannel chan int) {    defer close(dataChannel)    for i := 0;i &lt; 1000;i++ {        dataChannel &lt;- i    }}func receiver(dataChannel chan int, done chan interface{}) {    for data := range dataChannel {        fmt.Printf(&quot;Receive data %d\n&quot;, data)    }    done &lt;- nil}<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">在单个sender的场景下，没有什么好说的。其实就是当sender把所有数据都塞到channel之后主动关闭该channel。**显式close channel的好处是，当receiver端使用```for range```从channel中读数据，读到close标识后会自动结束循环。（或者是普通循环里的ok标识为false，用来结束循环）**</span><br><span class="line"></span><br><span class="line">+ 多个sender</span><br></pre></td></tr></table></figure>package main// 多个sender，一个receiverconst (    SENDER_COUNT = 5)func sender(id int, dataChannel chan string, wg *sync.WaitGroup) {    defer wg.Done()    for i := 0;i &lt; 100;i++ {        dataChannel &lt;- fmt.Sprintf(&quot;Sender %d is sending %d&quot;, id, i)    }}func receiver(dataChannel chan string, done chan interface{}) {    for data := range dataChannel {        fmt.Printf(&quot;Receive data: %s\n&quot;, data)    }    done &lt;- nil}func monitor(dataChannel chan string, wg *sync.WaitGroup) {    wg.Wait()    close(dataChannel)}func main() {    dataChannel := make(chan string, 100)    done := make(chan interface{})    wg := &amp;sync.WaitGroup{}    wg.Add(SENDER_COUNT)    go monitor(dataChannel, wg)    for i := 0;i &lt; SENDER_COUNT;i++ {        go sender(i, dataChannel, wg)    }    go receiver(dataChannel, done)    &lt;-done    fmt.Println(&quot;Done.&quot;)}```参照以上代码，核心的思想是利用```sync```包自带的```WaitGroup```（类似于```Java```里```J.U.C```包的```CountdownLatch```），用来统计已完成工作的sender数。除了sender和receiver外，我们还定义了一个monitor协程，用来关闭channel。可以看到我们定义了1个monitor，1个receiver，5个sender，并且在启动sender之前先启动了monitor，传入```waitGroup```，让其等待所有sender协程完成工作。在sender里，通过```defer```来保证sender在完成作业（或者发生异常）之后能够通知```waitGroup```。当所有sender都完成工作后，```waitGroup```计数自然减为0，monitor协程主动关闭了数据channel，所以receiver端的```for range```循环在读完所有数据后就能正常退出。</code></pre><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>本文是对《How To Gracefully Close Channels》的核心内容表示质疑，并且提出了我自己的解决方案。<strong>其实原文里作者提供的解决方案并不是错误的，里面的方案对部分场景肯定是适用的。只是对sender端主动关闭的场景而言有一定的纰漏。</strong></p><p>希望读者能理解不同场景下应该有不同的解决方案，具体还是要结合实际项目来分析。</p>]]></content>
      
      
      <categories>
          
          <category> Go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Go </tag>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>[Golang]奇怪的“类”和多态（Polymorphism）</title>
      <link href="/2018/11/15/polymorphism/"/>
      <url>/2018/11/15/polymorphism/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。</p><p>本文写于2018/11/14，基于<figure class="highlight go"><figcaption><span>1.11```。</span></figcaption><table><tr><td class="code"><pre><span class="line">&gt; 至于其他版本的Go SDK，如有出入请自行查阅其他资料。</span><br><span class="line"></span><br><span class="line">#### Introduction</span><br><span class="line"><span class="string">``</span><span class="string">`多态`</span><span class="string">``</span>这个词语每个开发者都一定不会感到陌生。因为在很多不同的编程语言里面，多态都是有具体实现的。例如在C++里面我们可能会说父类的指针可以指向子类的对象，在Java里面虽然没有指针这样的概念，但是我们一般会说，如果某个子类实现了一个接口，那么这个接口的引用就可以引用这个子类的一个实例对象。</span><br><span class="line"></span><br><span class="line">但是类似于C++和Java里面的实现基本上都是基于接口和类的相互合作去构建的。而在Go里面，是有接口的定义，但是并不存在严格意义上的类（Class），只是单纯的结构体（Struct），而且实现方法时还有略显奇葩的<span class="string">``</span><span class="string">`Receiver`</span><span class="string">``</span>机制。所以本文的目标就是来探索一下在Go语言里面多态是如何实现的，以及在Go语言时实现多态的过程中有哪些奇奇怪怪的坑。</span><br><span class="line"></span><br><span class="line">#### 类、方法、多态</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> 基本接口实现</span><br><span class="line"></span><br><span class="line">首先看看一下在Go语言里面如何去实现一个接口，其实就是让一个结构体去实现接口里定义的所有方法。但是在Go里面结构体（Struct）并不是我们在Java中认知的类（Class），所以我们所谓的实现一个方法是要写在结构体的外部，并且为每个方法定义一个接收者（Receiver）。</span><br></pre></td></tr></table></figure></p></blockquote><pre><code>// Human接口，内含breathe()方法，因为所有人类都能呼吸type Human interface {    breathe()}// 学生结构体，内含姓名和年龄type Student struct {    name string    age int}// 实现Student类的breathe方法，实现了Human接口func (s Student) breathe() {    fmt.Println(&quot;I can breathe.&quot;)}<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">2. Receiver的选择</span><br><span class="line"></span><br><span class="line">在为每个方法定义Receiver的时候，不仅可以把Receiver定义为一个结构体的对象（值），还可以定义为该结构体的一个指针。当你需要对某个对象（值）里面的属性做修改的时候，例如更新或者删除原值，一般情况下会把Receiver定义为结构体的**指针**。</span><br></pre></td></tr></table></figure>// Driver接口，内含updateLicense()方法，因为司机有时需要更新驾照信息type Driver interface {    updateLicense()}// Man结构体type Man struct {    name string    age int    license string}// 实现Man的updateLicense方法，实现了Driver接口func (m *Man) updateLicense() {    m.license = &quot;new license&quot;    fmt.Println(&quot;License is updated.&quot;)}<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">3. 方法调用</span><br><span class="line"></span><br><span class="line">无论你是用指针还是用某个结构体具体的值，都可以直接对该结构体所实现的方法进行直接的调用。当你使用指针调用某个接收者是值的方法时，Go语言的内部会帮你把指针指向的对象（值）找出来，然后再进行调用。如果某个方法的接收者是一个指针，同样也可以用对象（值）来进行调用。这是因为go语言内部会自动把对象（值）的地址找到，然后构建出指向该对象（值）的指针，就可以顺利进行调用。</span><br></pre></td></tr></table></figure>// 学生结构体，内含姓名和年龄type Student struct {    name string    age int}// Receiver为对象（值）的方法func (s Student) breathe() {    fmt.Println(&quot;I can breathe.&quot;)}// Receiver为指针的方法func (s *Student) grow() {    s.age += 1    fmt.Println(&quot;Grow older.&quot;)}func main() {    student := Student{name: &quot;hh&quot;, age: 18} //学生对象（值）    pointer := &amp;student //指针    student.breathe() //直接调用正常，打印出I can breathe.    pointer.breathe() //通过指针调用也正常，也打印出I can breathe.    student.grow() //直接调用正常，打印出Grow older    pointer.grow() //通过指针调用也正常，也打印出Grow older}<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">**从上述代码可以看出，无论某个方法的Receiver是对象（值）还是指针，都可以通过对象（值）或者指针来进行方法调用。**再仔细想一想也是非常合理的，因为Go内部可以轻松地找到一个对象（值）的地址，自然就能构建出指向它的指针（```&amp;obj```），调用Receiver为指针的方法；另外也很容易通过指针找到其指向的对象（值）（```*p```），自然也能轻松调用Receiver为对象（值）的方法。</span><br><span class="line"></span><br><span class="line">4. 最简单的多态示例</span><br></pre></td></tr></table></figure>// Human接口，内含breathe()方法，因为所有人类都能呼吸type Human interface {    breathe()}// 学生结构体，内含姓名和年龄type Student struct {    name string    age int}// 实现Student类的breathe方法，实现了Human接口func (s Student) breathe() {    fmt.Println(&quot;I can breathe.&quot;)}// 测试多态的Test方法func Test(h Human) {    h.breathe()}func main() {    student := Student{name: &quot;hh&quot;, age: 18}    Test(student) //打印出I can breathe.}<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">上述是最简单直接描述```多态```的代码例子，```Test```方法的入参是一个接口类型（```Human```），**只要实现了```Human```接口的任一类型的对象都可以传进去，可以是这里的```Student```，也可以是```Driver```，也可以是```Teacher```。无所谓，只要实现了```Human```接口就没问题。**</span><br><span class="line"></span><br><span class="line">5. 实现多态时诡异的报错</span><br><span class="line"></span><br><span class="line">在上面最简单的多态例子里，```Student```实现了```Human```接口，实现了一个以值为Receiver的```breathe()```方法，便可成功传入```Test```方法里。**那假如接口中声明了不止一个方法，且实现时Receiver不一定是值，还可能是指针呢？那样可以吗？**</span><br></pre></td></tr></table></figure>// Human接口，内含breathe()和grow()type Human interface {    breathe()    grow()}// 学生结构体，内含姓名和年龄type Student struct {    name string    age int}// 实现Human接口中的breathe()func (s Student) breathe() {    fmt.Println(&quot;I can breathe.&quot;)}// 实现Human接口中的grow()，因为要改变age属性，所以Receiver为指针func (s *Student) grow() {    s.age += 1    fmt.Println(&quot;I can grow.&quot;)}// 测试多态的Test方法func Test(h Human) {    h.breathe()    h.grow()}func main() {    student := Student{name: &quot;hh&quot;, age: 18}    Test(student) // 此处报错！！！    // 报错信息如下    // cannot use student (type Student) as type Human in argument to Test:    // Student does not implement Human (grow method has pointer receiver)}<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">从上述代码的报错中看到，```Student```没有实现```Human```接口，因为```grow```方法的Receiver是指针？？？**可是我明明实现了```grow```方法啊，只是它的Receiver是指针而已！难道```*Student```和```Student```居然被认为是两种不同的类型？？？**</span><br><span class="line"></span><br><span class="line">**没错！在Go的设计理念中，```type pointer```和```type value```确实就是两种不同的类型！**</span><br><span class="line"></span><br><span class="line">**所以如果想让某个结构体实现一个接口，必须要分离开来思考，你到底是想让```type pointer```实现还是想让```type value```实现？**</span><br><span class="line"></span><br><span class="line">这个时候，正常人都会想：我要实现一些需要改变对象属性值的方法（像上面的```grow()```），当然需要让这些方法的Receiver为指针，不然怎么改变对象内部的属性值啊？而对于那些不需要改变对象属性值的方法，Receiver为指针也不会出错，顶多就是看着不规范而已。**好，那就把所有实现的方法的Receiver都改成指针，肯定能正常实现那个接口~** 于是便有了下面的代码👇</span><br></pre></td></tr></table></figure>// Human接口，内含breathe()和grow()type Human interface {    breathe()    grow()}// 学生结构体，内含姓名和年龄type Student struct {    name string    age int}// 全改成指针Receiver，美滋滋func (s *Student) breathe() {    fmt.Println(&quot;I can breathe.&quot;)}// 全改成指针Receiver，美滋滋func (s *Student) grow() {    s.age += 1    fmt.Println(&quot;I can grow.&quot;)}// 测试多态的Test方法func Test(h Human) {    h.breathe()    h.grow()}func main() {    student := Student{name: &quot;hh&quot;, age: 18}    Test(student) // 此处还是报错！！！    // 报错信息如下    // Cannot use &apos;student&apos; (type Student) as type Human     // Type does not implement &apos;Human&apos; as &apos;breathe&apos; method has a pointer receiver less... (⌘F1)     //Inspection info: Reports incompatible types.}<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">**又出错了？！甚至都不用run，IDE直接报错？！** **之前说好的指针和对象在调用方法时可以随便互换使用呢？？？为什么这里不行了？？？**</span><br><span class="line"></span><br><span class="line">此时就要搬出```Go specification```里的经典表格来解释这个表层现象了👇👇👇。</span><br><span class="line"></span><br><span class="line">![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/gospec.png)</span><br><span class="line"></span><br><span class="line">注意看第二个表格。第一行：当```Methods Receivers```为对象（值）（```t T```）的时候，可用对象（值）或者指针作为多态方法的接口参数；第二行：当```Methods Receivers```为指针时，只能用指针作为多态方法的接口参数。我们实现的方法的Receiver全都是指针，所以我们传一个对象（值）进去，像```Test(student)```就会报错。**如果我们把main中的代码改成```Test(&amp;student)```，用指针作为方法的多态接口参数，自然就不会报错了。**</span><br><span class="line"></span><br><span class="line">**但这些只是表层现象，再研究得深入一点，为什么会这样呢？原来我们一直认为“有了对象（值）就一定能找到它的地址，从而构建出指向它自己的指针”，其实这种想法在一定程度上是有问题的。**在```Go In Action```一书中给出了一段示意代码，展示了为什么有的时候是找不到对象（值）的地址的👇👇👇。</span><br></pre></td></tr></table></figure>01 // Sample program to show how you can&apos;t always get the02 // address of a value.03 package main0405 import &quot;fmt&quot;0607 // duration is a type with a base type of int.08 type duration int0910 // format pretty-prints the duration value.11 func (d *duration) pretty() string {12     return fmt.Sprintf(&quot;Duration: %d&quot;, *d)13 }1415 // main is the entry point for the application.16 func main() {17     duration(42).pretty()1819     // cannot call pointer method on duration(42)20     // cannot take the address of duration(42)21 }```**当你拥有一个对象（值），你有可能拿不到它的地址，那就没有办法构建出指向它的指针，自然也就没有办法访问到Receiver为指针的那些方法。所以该对象所拥有的方法集合（```Method Set```）中只包含Receiver为对象（值）的那部分方法****而当你拥有一个指针的时候，你肯定、必然、100%能拿到它指向的对象（值）。那你既能访问到Receiver为指针的方法，也能访问到Receiver为对象（值）的方法。所以该指针所拥有的方法集合（```Method Set```）中包含了Receiver为指针和Receiver为对象（值）的所有方法****这也就是上面第一个表格的含义。然后我们试着把第一个表格转置一下，也就能得到第二个表格。**解释了为什么我们把```Test(student)```改成```Test(&amp;student)```就能通过的原因。另外如果把那两个方法改成```func (s Student) breathe()```和```func (s Student) grow()```，那无论是```Test(student)```还是```Test(&amp;student)```都可以正常运行，因为参照第二个表格，当实现接口中的方法的Receiver为对象（值）时，以接口类型作为参数的多态方法可以接收对象（值）也可以接收指针，无所谓。</code></pre><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>本人文笔水平有限，在文字解释部分可能稍显混乱，如有疑问请反复参照示意代码、表格、图片，也欢迎留言讨论。</p><p>在Go实现多态这一部分，最麻烦的莫过于同一个结构体的<code>指针类型</code>和<code>值类型</code>，在实现接口时被认为是不一样的类型。当某个结构体想实现一个接口，统一了所有方法的Receiver后，在传参给接口参数时又出现了类型不匹配方面的小坑。表面上fix掉报错很容易，但其底层的原理掰开来还是有点小复杂的。</p>]]></content>
      
      
      <categories>
          
          <category> Go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Go </tag>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>[Golang]奇葩数据结构之Slice（切片）</title>
      <link href="/2018/11/10/go-slice/"/>
      <url>/2018/11/10/go-slice/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。</p><p>本文写于2018/11/09，基于<figure class="highlight go"><figcaption><span>1.11```。</span></figcaption><table><tr><td class="code"><pre><span class="line">&gt; 至于其他版本的Go SDK，如有出入请自行查阅其他资料。</span><br><span class="line"></span><br><span class="line">#### Introduction</span><br><span class="line"></span><br><span class="line">最近新接触<span class="string">``</span><span class="string">`Golang`</span><span class="string">``</span>，个人有一种习惯，在粗略过完基础语法后就开始深入研究一番语言内部built-in的数据结构。想当年学<span class="string">``</span><span class="string">`Java`</span><span class="string">``</span>时，就抠了不少<span class="string">``</span><span class="string">`JDK7`</span><span class="string">``</span>和<span class="string">``</span><span class="string">`JDK8`</span><span class="string">``</span>里面的集合类源码（**特别是Map相关和List相关的实现**）。</span><br><span class="line"></span><br><span class="line">类似的，在<span class="string">``</span><span class="string">`Golang`</span><span class="string">``</span>里面，built-in类型的几种经典数据结构（或者说是容器／集合）有</span><br><span class="line"></span><br><span class="line">+ <span class="string">``</span><span class="string">`channel</span></span><br></pre></td></tr></table></figure></p></blockquote><ul><li><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+ ```slice</span><br></pre></td></tr></table></figure></li><li><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">对于```array```，没什么好说的，各语言中都一样，就是一片</span><br><span class="line"></span><br><span class="line">+ 连续的</span><br><span class="line">+ 可通过index快速定位的</span><br><span class="line">+ 存储相同数据类型</span><br><span class="line"></span><br><span class="line">的内存区域。</span><br><span class="line"></span><br><span class="line">**```array```相当的方便简单好用，但是最大的问题就是严重缺乏动态性，在```array```的领域里你很难看到随意扩容或者随意缩减**。</span><br><span class="line"></span><br><span class="line">于是，在其他部分语言里（如```Java```），就提供了很多基于```LinkedList```构建的容器，根据需求随意动态扩容缩减、头插入尾插入、随意删除中间元素，这种**动态性**用起来非常爽。**但是却又失去了数组那种连续空间所支持的最爽的一点 —— 用index直接定位到具体的元素（API上支持，但实际上还是一路遍历过去）。**</span><br><span class="line"></span><br><span class="line">所以，基于这些问题和背景，在```Golang```中我们就见到了一个“奇葩”的数据结构 —— ```slice```（切片）。</span><br><span class="line"></span><br><span class="line">#### 底层实现</span><br><span class="line"></span><br><span class="line">+ 代码层面</span><br><span class="line"></span><br><span class="line">根据官方的说法，或者很多Go开发者的说法，```slice```最直接的定义就是```dynamic array```。**所以其实它的底层真的是基于```array```实现的。**同时它还能支持**一定程度上比较良好的**动态性。</span><br><span class="line"></span><br><span class="line">先来看看```Go 1.11```中的```slice```源码。</span><br><span class="line"></span><br><span class="line">```go</span><br><span class="line">type slice struct &#123;</span><br><span class="line">array unsafe.Pointer</span><br><span class="line">len   int</span><br><span class="line">cap   int</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><pre><code>**看到了这个数据结构的定义，瞬间让我想起了<figure class="highlight plain"><figcaption><span>Dynamic String(SDS)```的实现。**同样是封装了一个底层数组（此处```array```指针指向的是一片连续内存区域），同样是维护了一个```len```属性，同样有一个```cap```（```free```）属性来指明容器的使用情况。（**感兴趣的可以去查阅一下```Redis```里的字符串（```SDS```）实现**）</span></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">解释一下结构体里各成员的含义：</span><br><span class="line"></span><br><span class="line">+ ```array unsafe.Pointer```：一个指向连续内存区域（数组）的指针</span><br><span class="line">+ ```len   int```：此```slice```中已存了多少个元素</span><br><span class="line">+ ```cap   int```：此```slice```中最多能存多少个元素（最大容量）</span><br><span class="line"></span><br><span class="line">+ 逻辑层面</span><br><span class="line"></span><br><span class="line">假设我们新建了一个```slice</span><br></pre></td></tr></table></figure>+ <figure class="highlight plain"><figcaption><span>:</span></figcaption><table><tr><td class="code"><pre><span class="line">+ 或者```slice := []string&#123;&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;&#125;</span><br></pre></td></tr></table></figure>将会得到如下图所示的逻辑结构，<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/basic.png)</span><br><span class="line"></span><br><span class="line">若我们不是从头新建，而是从某个已有的```array```中建出一个```slice</span><br></pre></td></tr></table></figure><figure class="highlight go"><table><tr><td class="code"><pre><span class="line">strs := [<span class="number">4</span>]<span class="keyword">string</span>&#123;<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span>&#125;</span><br><span class="line">slice := strs[<span class="number">0</span>:<span class="number">2</span>] <span class="comment">// 取strs下标为[0,2)的元素构造一个slice</span></span><br></pre></td></tr></table></figure>将会得到如下图所示的逻辑结构，<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/basic2.png)</span><br><span class="line"></span><br><span class="line">**所以，基于底层的数组，本质上```slice```不过是一个数组的```wrapper```（包装类），并提供一些动态性的操作（```append()```），让使用者感受不到它是一个数组罢了。**</span><br><span class="line"></span><br><span class="line">#### 对slice的操作，以及其中的坑</span><br><span class="line"></span><br><span class="line">对```slice```操作，无非就5种：</span><br><span class="line"></span><br><span class="line">+ 新建（无需多讲）</span><br><span class="line">+ 直接新建</span><br><span class="line">+ 从已有的数组中截取</span><br><span class="line">+ 插入</span><br><span class="line">+ 截断（所谓的删除）-&gt; 用index截取</span><br><span class="line">+ 更新（无需多讲）</span><br><span class="line">+ 读取（无须多讲）</span><br><span class="line"></span><br><span class="line">**首先明确第一个坑，在同一个数组上构造出来的多个```slice```都是共享同一个底层数组的。所以你对某一个```slice```进行写或者更新操作后，很有可能就会影响到基于同一个底层数组的其他```slice```。**详见下述代码与示意图。</span><br><span class="line"></span><br><span class="line">![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/basic3.png)</span><br><span class="line"></span><br><span class="line">```go</span><br><span class="line">strs := [4]string&#123;&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;&#125;</span><br><span class="line">slice1 := strs[0:2] // 取strs下标为[0,2)的元素构造一个slice1</span><br><span class="line">slice2 := strs[0:3] // 取strs下标为[0,3)的元素构造一个slice2</span><br><span class="line"></span><br><span class="line">slice2[0] = &quot;hhhhh&quot;</span><br><span class="line">fmt.Println(slice1) // [hhhhh, b] slice1被改变了</span><br></pre></td></tr></table></figure></code></pre><p>**由此可见，当你的代码里有多个<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">接下来着重讲讲**插入（append()）**操作，以及其中奇葩的几个点。</span><br><span class="line"></span><br><span class="line">一般执行```append(slice, value)```时会有以下两种情况</span><br><span class="line"></span><br><span class="line">+ ```slice```的```len```小于```cap```，顺理成章可以直接插入（**注意会直接改变底层数组!**）</span><br><span class="line">+ ```slice```的```len```等于```cap```，当前可用空间不足</span><br><span class="line"></span><br><span class="line">1. ```len```小于```cap</span><br></pre></td></tr></table></figure></p><pre><code><figure class="highlight go"><table><tr><td class="code"><pre><span class="line">strs := [<span class="number">4</span>]<span class="keyword">string</span>&#123;<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span>&#125;</span><br><span class="line">slice1 := strs[<span class="number">0</span>:<span class="number">2</span>] <span class="comment">// 取strs下标为[0,2)的元素构造一个slice1, len=2, cap=4</span></span><br><span class="line">slice1 = <span class="built_in">append</span>(slice1, <span class="string">"hh"</span>) <span class="comment">// append后len=3, cap=4</span></span><br></pre></td></tr></table></figure>![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/basic4.png)如上图所示，在append了<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">2. ```len```等于```cap</span><br></pre></td></tr></table></figure><figure class="highlight go"><table><tr><td class="code"><pre><span class="line">strs := [<span class="number">4</span>]<span class="keyword">string</span>&#123;<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span>&#125;</span><br><span class="line">slice1 := strs[<span class="number">0</span>:<span class="number">1</span>:<span class="number">1</span>] <span class="comment">// 取strs下标为[0,1)的元素构造一个slice1, len=1, cap=1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 为了说明问题, 此处用特殊的写法</span></span><br><span class="line">slice2 := <span class="built_in">append</span>(slice1, <span class="string">"hh"</span>)</span><br><span class="line">fmt.Println(<span class="built_in">len</span>(slice2), <span class="built_in">cap</span>(slice2)) <span class="comment">// len=2, cap=2</span></span><br><span class="line">fmt.Println(<span class="built_in">len</span>(slice1), <span class="built_in">cap</span>(slice1)) <span class="comment">// len=1, cap=1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 关键点在此</span></span><br><span class="line">slice2[<span class="number">0</span>] = <span class="string">"hello"</span></span><br><span class="line">fmt.Println(slice1) <span class="comment">// [a]</span></span><br></pre></td></tr></table></figure>当执行了```slice2[0] = &quot;hello&quot;```后，```slice1```居然还是```[a]```而不是```[hello]```，说好的**基于同一个底层数组的多个slice会共享该数组呢？？？**在这里怎么出问题了？？？**其实关键就在于我们append时，```slice1```的```len``` == ```cap```**，在原```slice1```中没有空间可以允许插入新元素了。**此时就会触发slice扩容，然而此扩容并不是对原底层数组进行操作，而是新开辟一段更长的数组空间，把原数组的值copy过来，再让slice中的指针指向新开辟的长数组！**详细流程请参考下图。![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/basic5.png)由图可见，原本```slice1```里的指针指向的数组是位于```0x34```处的。当尝试```append()```并触发扩容时，**底层直接在另外的地址(```0x88```）处开辟了（或者称之为```deep clone```吧）一段全新的数组。**并把值都copy过去，然后才真正把需要append的```&quot;hh&quot;```塞进去。很自然，**之后我们执行的```slice2[0] = &quot;hello&quot;```，只是对```0x88```起始的那个数组进行了修改，完全没有影响到```slice1```指向的那个从```0x34```起始的数组。**把```slice1```打印出来，自然也就还是```[a]```了。所以在很多Go项目的代码里，或者官方教程里，**最常见的append用法都是```slice = append(slice, value)```，而不是```newSlice := append(slice, value)```。**当把append完成后的结果再一次赋给原slice，无论发生了什么(无论```len```和```cap```是什么关系都无所谓），总不会造成runtime时的歧义。如果在一些特定的场景下，必须要使用```newSlice := append(slice, value)```的写法，那就要格外小心！**如果在append过程中触发了扩容，```newSlice```和原```slice```将不再指向同一段数组空间，在这种情况下对```newSlice```的修改也丝毫不会影响```slice```**（也有可能这恰好就是你的逻辑需要的）。</code></pre><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p><code>Golang</code>中的<code>slice</code>兼顾了传统数组的优点（连续内存空间），也支持动态扩容／截断，确实是在实际开发中非常有用的容器。<strong>但建议每个Go开发者都详细了解其底层实现，因为为了支持其动态性，append和append相关的操作会带来很多奇奇怪怪的坑</strong></p>]]></content>
      
      
      <categories>
          
          <category> Go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Go </tag>
            
            <tag> 笔记 </tag>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>[Google]记一道不错的电面题</title>
      <link href="/2018/09/28/google-interview0/"/>
      <url>/2018/09/28/google-interview0/</url>
      
        <content type="html"><![CDATA[<h2 id="Google-记一道不错的电面题"><a href="#Google-记一道不错的电面题" class="headerlink" title="[Google]记一道不错的电面题"></a>[Google]记一道不错的电面题</h2><blockquote><p>最近一位朋友跟我分享了一道Ta在google电面中被问到的算法题，一阵研究后觉得题目出得很不错，不是那种傻X的tricky题目，故分享之，并附上我实现的Java代码。</p></blockquote><h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p>根据输入的有序字符串，构造出目标字符串，保证每个输入的串均服从目标串的原顺序。</p><ul><li>target: <code>3-5-7-1-9</code></li><li>input: <code>3-5-7, 5-1-9, 7-1, 1-9, 5-7</code></li></ul><p>可以看到输入的每个字符串均服从目标串的顺序，只不过每个输入串都缺失了一些元素。 本题的前提条件：</p><ul><li>保证根据<code>input</code>能够生成唯一的目标串</li><li><code>target</code>中的元素取值范围是<code>[1, 9]</code>，且不重复</li></ul><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>拿到题目后，最直观的思路是：每处理一个输入串，把里面的数字抽取出来，建立一个<code>ListNode</code>，然后根据该串中的先后顺序把多个<code>ListNode</code>连接起来。同时维护一个<code>Map</code>记录已经遇到过的<code>ListNode</code>，之后再遇到该数字就不用新建，而是直接从<code>Map</code>中取出数字对应的<code>ListNode</code>。 例如，先建立一个<code>Map&lt;Integer, ListNode&gt;</code>，然后开始处理第一个输入串<code>3-5-7</code>，</p><ul><li>先遇到<code>3</code>，<code>Map</code>中没有<code>Node3</code>，建立<code>Node3</code></li><li>然后遇到<code>5</code>，<code>Map</code>中没有<code>Node5</code>，建立<code>Node5</code>，然后让<code>Node3</code>的<code>next</code>指向<code>Node5</code></li><li>最后遇到<code>7</code>，<code>Map</code>中没有<code>Node7</code>，建立<code>Node7</code>，然后让<code>Node5</code>的<code>next</code>指向<code>Node7</code></li><li>接着处理下一个输入串，循环</li></ul><p>大多数人第一反应都会想到这样的处理逻辑，非常直观。<strong>但是这样的逻辑存在细节问题。</strong>因为每个输入串只是保留了<code>target</code>的相对顺序，中间缺失了一些元素，所以在构建链表时，凭借局部相对顺序直接插入<code>ListNode</code>可能会造成全局顺序的错误。<strong>例如在得到<code>3-&gt;5-&gt;7</code>这个链表后，再处理第二个输入串<code>5-1-9</code>，根据以上的逻辑就会把<code>Node1</code>直接插到<code>Node5</code>身后，得到<code>3-&gt;5-&gt;1-&gt;9-&gt;7</code>,显然是不对的，主要原因就是局部的相对顺序不一定能直接映射全局顺序。</strong> <strong>所以，这道题的核心就在于记录顺序，并且是全局顺序。</strong>由之前的想法拓展开来，我们已经有了<strong>为每个数字建立对应的<code>ListNode</code></strong>的想法，那么这个<code>Node</code>是否可以是一个<code>GraphNode</code>呢？我们既然可以用<code>LinkedList</code>存这些数据，是否也可以拓展成用<code>Graph</code>存这些数据呢？ 如果我们用<code>Graph</code>存储这些数据，那么在<code>Graph</code>中能体现<strong>顺序</strong>的无非就是<strong>有向边</strong>（若存在边<code>A ---&gt; B</code>，代表得先到达<code>A</code>才能到达<code>B</code>）。<strong>有向边 + 点，我们就得到了有向图</strong>，结合条件中说明的<code>target</code>中不存在重复元素，我们就得到了<strong>有向无环图（<code>DAG</code>）</strong>。那么在<code>DAG</code>中，能找出所谓的<strong>全局顺序</strong>的，就是<code>Topological Sort</code>。</p><h3 id="具体代码"><a href="#具体代码" class="headerlink" title="具体代码"></a>具体代码</h3><pre><code>package google;import java.util.*;public class Solution {    public static void main(String[] args) {        Solution solution = new Solution();        List&lt;String&gt; stream = new ArrayList&lt;&gt;();        stream.add(&quot;3-5-7&quot;);        stream.add(&quot;3-1&quot;);        stream.add(&quot;5-1-9&quot;);        stream.add(&quot;7-1&quot;);        stream.add(&quot;1-9&quot;);        solution.reconstruct(stream);    }    /**     * Reconstruct an ordered sequence from the given stream.     * @param stream stream of sequences     */    public void reconstruct(List&lt;String&gt; stream) {        if (stream == null || stream.size() == 0) {            return;        }        // 1. extract nodes, edges from the input stream        Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;();    // mapping between node_num and node_index        Map&lt;Integer, String&gt; outputMapping = new HashMap&lt;&gt;();   // mapping between node_index and output_str        List&lt;Integer[]&gt; edges = new ArrayList&lt;&gt;();  // record the edges [A, B] means an edge pointing from A to B        int nodeIndex = 0;        for (String s : stream) {            String[] nums = s.split(&quot;-&quot;);            for (int i = 0;i &lt; nums.length;i++) {                int nodeNum = Integer.parseInt(nums[i]);                if (!map.containsKey(nodeNum)) {                    map.put(nodeNum, nodeIndex++);                    outputMapping.put(nodeIndex - 1, nums[i]);                }                if (i != nums.length - 1) {                    edges.add(new Integer[]{nodeNum, Integer.parseInt(nums[i + 1])});                }            }        }        // 2. construct the graph structure        int nodeCount = map.keySet().size();        boolean[][] graph = new boolean[nodeCount][nodeCount];  // adjacency matrix        int[] indegrees = new int[nodeCount];   // record the indegree of each node        for (Integer[] edge : edges) {            int from = edge[0];            int to = edge[1];            if (!graph[map.get(from)][map.get(to)]) {                // avoid duplicated edges in the input stream                graph[map.get(from)][map.get(to)] = true;                indegrees[map.get(to)]++;            }        }        // 3. finding out nodes with 0 indegree        Queue&lt;Integer&gt; queue = new LinkedList&lt;&gt;();        StringBuilder result = new StringBuilder();        for (int i = 0;i &lt; indegrees.length;i++) {            if (indegrees[i] == 0) {                queue.add(i);                result.append(outputMapping.get(i) + &quot;-&quot;);            }        }        // 4. topological sort        while (!queue.isEmpty()) {            int index = queue.poll();            for (int j = 0;j &lt; graph[index].length;j++) {                if (graph[index][j]) {                    graph[index][j] = false;                    indegrees[j]--;                    if (indegrees[j] == 0) {                        queue.add(j);                        result.append(outputMapping.get(j) + &quot;-&quot;);                    }                }            }        }        System.out.println(result.substring(0, result.length() - 1));    }}</code></pre><h3 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h3><p>在上述代码中，我们主要做了以下几件事情</p><ol><li>处理每一个输入串，将其内部的数字解析出来，并为每个数字“生成”一个对应的<code>Node</code>和其<code>NodeIndex</code>。</li><li>根据第一步得到的各个<code>Node</code>和其<code>NodeIndex</code>，结合输入串，“生成”各点之间的有向边，即我们的<strong>图结构</strong>，此处用<code>adjacency matrix</code>实现。</li><li>开始执行<code>topological sort</code>的逻辑，先构建<code>indegree</code>数组，找到入度为<code>0</code>的点。</li><li>基于入度为<code>0</code>的点，利用<code>Queue</code>实现<code>topological sort</code>。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
            <tag> Java </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>《推荐系统实践》笔记 #4</title>
      <link href="/2018/06/01/recsys-note4/"/>
      <url>/2018/06/01/recsys-note4/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。</p></blockquote><h3 id="利用上下文信息进行推荐"><a href="#利用上下文信息进行推荐" class="headerlink" title="利用上下文信息进行推荐"></a>利用上下文信息进行推荐</h3><p>在推荐时，应该考虑<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">1. 时间上下文信息</span><br><span class="line">+ 时间效应</span><br><span class="line">+ 用户兴趣持续变化</span><br><span class="line">+ 物品本身有生命周期</span><br><span class="line">+ 季节／节日效应（圣诞节，奥斯卡......）</span><br><span class="line"></span><br><span class="line">+ 时间上下文推荐算法</span><br><span class="line">+ 最近最热门算法</span><br><span class="line"></span><br><span class="line">![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/recentlyhot.png)</span><br><span class="line"></span><br><span class="line">+ 结合时间上下文的```Item-based CF</span><br></pre></td></tr></table></figure></p><pre><code>        **用户在相隔时间很短内喜欢的物品具有更高相似度**，**近期行为相比很久之前的行为更能体现用户现在的兴趣，应加强用户近期行为的权重。**        ![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/time1.png)        ![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/time2.png)        ![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/time3.png)    + 结合时间上下文的<figure class="highlight plain"><figcaption><span>CF```</span></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/time_ubcf1.png)</span><br><span class="line"></span><br><span class="line">![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/time_ubcf2.png)</span><br><span class="line"></span><br><span class="line">+ 时间段图模型</span><br><span class="line"></span><br><span class="line">2. 地点上下文信息</span><br><span class="line"></span><br><span class="line">**不同地区的用户兴趣有所不同（距离很重要）**</span><br><span class="line"></span><br><span class="line">+ 基于位置的推荐算法</span><br><span class="line">+ 物品／用户均可分为有空间属性的和无空间属性的，例如餐馆／商店带有空间属性，图书／电影则没有。</span><br><span class="line">+ 数据结构：```(user, user_location, item, item_location, rating)</span><br></pre></td></tr></table></figure>    + 可将位置信息数据集划分成树状结构        ![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/country.png)    + 只有<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+ 只有```item_location```时，先忽略物品位置信息，直接用```Item-based CF```计算出```p(u, i)```，最后再引入距离代价作惩罚。</span><br><span class="line"></span><br><span class="line">![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/penalty.png)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 利用社交网络数据</span><br><span class="line"></span><br><span class="line">1. 获取社交网络数据的途径</span><br><span class="line">+ e-mail</span><br><span class="line">+ 用户注册信息</span><br><span class="line">+ 用户的位置数据</span><br><span class="line">+ 论坛 &amp; 讨论组</span><br><span class="line">+ 即时聊天工具</span><br><span class="line">+ 社交网站</span><br><span class="line"></span><br><span class="line">2. 社交网络数据简介</span><br><span class="line">+ 图结构```G = (V, E, W)```, 其中```V```是用户节点，```E```是用户关系边，```W```是边权重</span><br><span class="line">+ 单向关注 =&gt; 有向图（如Twitter）；互相关注 =&gt; 无向图（如Facebook）；基于社区关系，无明确方向</span><br><span class="line"></span><br><span class="line">3. 基于社交网络的推荐</span><br><span class="line">+ 基于邻域的社会化推荐算法</span><br><span class="line"></span><br><span class="line">![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/familiarity.png)</span><br><span class="line"></span><br><span class="line">熟悉程度 = 双方共同好友的比例</span><br><span class="line"></span><br><span class="line">相似度 = ```User-based CF```中余弦相似度</span><br><span class="line"></span><br><span class="line">优化：</span><br><span class="line"></span><br><span class="line">+ 不需要取所有的好友进行计算，只取相似度高的前```K```个</span><br><span class="line">+ 只取最近短时间内发生的操作记录进行计算</span><br><span class="line"></span><br><span class="line">+ 基于图的社会化推荐</span><br><span class="line"></span><br><span class="line">+ 结合社交网络图 &amp; 用户物品二分图（也可加入社区的顶点）</span><br><span class="line">+ 用户与用户之间的权重 = ```a * (相似度 + 熟悉度)</span><br></pre></td></tr></table></figure>    + 用户与物品之间的权重 = ```b * (用户对物品的喜爱程度)```    + 通过调节```a```和```b```参数确定哪部分对系统影响较大+ 信息流（Feed）推荐    帮助用户从信息墙上挑选有用的信息，综合考虑信息流中每个会话的长度、时间、用户兴趣间的相似度等。</code></pre><ol start="4"><li><p>好友推荐系统</p><ul><li><p>基于内容的匹配</p><p>  给用户推荐和他们有相似内容属性的用户（人口统计学属性、用户兴趣、用户位置……）</p></li><li><p>基于共同兴趣的好友推荐</p><p>  利用<code>User-based CF</code>计算用户之间的兴趣相似度</p></li><li><p>基于社交网络图</p><p>  <strong>推荐好友的好友 =&gt; 推荐熟悉的好友的好友</strong></p><p>  <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/w_out.png" alt=""></p><p>  <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/w_in.png" alt=""></p><p>  <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/w_out_in.png" alt=""></p><p>  <strong>算法时间复杂度不高，适合在线应用</strong></p></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
            <tag> 推荐 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>《推荐系统实践》笔记 #3</title>
      <link href="/2018/05/24/recsys-note3/"/>
      <url>/2018/05/24/recsys-note3/</url>
      
        <content type="html"><![CDATA[<h3 id="推荐系统冷启动问题"><a href="#推荐系统冷启动问题" class="headerlink" title="推荐系统冷启动问题"></a>推荐系统冷启动问题</h3><p><strong><em>如何在没有大量用户数据的情况下设计推荐系统</em></strong> =&gt; <strong><em>冷启动问题</em></strong></p><ol><li><p>冷启动问题</p><ul><li>用户冷启动 =&gt; 如何给全新的用户作推荐</li><li>物品冷启动 =&gt; 如何把新加入的物品推荐给用户</li><li>系统冷启动 =&gt; 如何在一个新开发的网站上设计开发个性化推荐系统</li></ul></li><li><p>常见的冷启动解决方案</p><ul><li>作<strong>非个性化</strong>的推荐：直接按热门排行榜进行推荐</li><li><p>利用用户注册时提供的个人信息：<figure class="highlight plain"><figcaption><span>```sex```......</span></figcaption><table><tr><td class="code"><pre><span class="line">+ 利用用户的社交网络信息：```Facebook```, ```微博```......</span><br><span class="line">+ 用户注册／登陆时先让其对一些给定物品进行反馈，采集其兴趣爱好</span><br><span class="line">+ 对于新物品，可从**内容相似性**的方向进行推荐，不一定只考虑行为相似性</span><br><span class="line">+ 事先引入、建立专家知识库，建立物品相关度表</span><br><span class="line"></span><br><span class="line">3. 详细解决方案</span><br><span class="line">+ 利用用户注册信息</span><br><span class="line">+ ```sex```, ```age```, ```DOB```, ```job```, ```ethnic```, ```edu```, ```location```等人口统计学信息 &amp; 用户兴趣描述...</span><br><span class="line"></span><br><span class="line">+ 推荐流程</span><br><span class="line">+ 获取用户注册信息</span><br><span class="line">+ 根据信息给用户分类</span><br><span class="line">+ 推荐其所属分类中用户喜欢的物品</span><br><span class="line"></span><br><span class="line">+ 理论依据</span><br><span class="line"></span><br><span class="line">```p(f, i)```为物品```i```在```f```特征人群中受喜爱的程度。</span><br><span class="line"></span><br><span class="line">![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/p(f,i).png)</span><br><span class="line"></span><br><span class="line">+ 启动初期先让用户给部分物品评分</span><br><span class="line"></span><br><span class="line">待评分的物品要</span><br><span class="line">+ 比较热门</span><br><span class="line">+ 具有代表性 &amp; 区分性</span><br><span class="line">+ 具有多样性</span><br><span class="line"></span><br><span class="line">可用一个```Decision Tree```来选择启动评分物品集合</span><br><span class="line"></span><br><span class="line">+ 利用物品的内容属性</span><br><span class="line"></span><br><span class="line">在```Item-based CF```中，新```item```加入时，不会立刻更新物品相关性矩阵，因为计算耗时特别大，所以要利用物品的内容属性进行冷启动推荐。</span><br><span class="line"></span><br><span class="line">对于文本数据而言，计算内容相似度前，需要利用NLP相关技术转化为向量（```keyword vector```），但向量空间模型丢失了多个```keyword```之间的关联和位置信息。**而且很多时候，两篇文本没有（或很少）直接相同的关键词，但是主题却高度相关。**在这种情况下，可使用```LDA（Latent Dirichlet Allocation）```来挖掘文本主题。</span><br><span class="line"></span><br><span class="line">LDA核心元素</span><br><span class="line">+ 文档```D```: ```D[i]```代表文档集合中第```i```篇文档</span><br><span class="line">+ 话题```Z```: ```Z[i][j]```代表```i```文档中```j```词所属的话题</span><br><span class="line">+ 词语```W```: ```W[i][j]```代表```i```文档中的```j```词</span><br><span class="line"></span><br><span class="line">计算步骤</span><br><span class="line"></span><br><span class="line">+ 先利用```LDA```挖掘出两篇文本的话题分布</span><br><span class="line">+ 再通过```KL-Divergence（KL散度）```比较两个分布的相似度</span><br><span class="line"></span><br><span class="line">+ 利用专家的作用 =&gt; 让专家手动／半手动地打标签</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 利用用户标签数据</span><br><span class="line"></span><br><span class="line">1. ```UGC``` = User Generated Content 用户生成数据，即让普通用户给物品打标签。</span><br><span class="line"></span><br><span class="line">2. 标签系统的推荐问题</span><br><span class="line">+ 如何利用用户打标签的行为为其推荐物品</span><br><span class="line">+ 如何在用户给物品打标签时为其推荐适合该物品的标签</span><br><span class="line"></span><br><span class="line">3. 用户标签行为</span><br><span class="line"></span><br><span class="line">数据结构：```behavior = (u, i, b)```, ```u``` = 用户，```i``` = 物品，```b``` = 标签。</span><br><span class="line"></span><br><span class="line">4. 基于标签的简单推荐算法</span><br><span class="line">+ 统计每个用户最常用的标签</span><br><span class="line">+ 对于每个常用标签，统计被打过该标签次数最多的物品</span><br><span class="line"></span><br><span class="line">![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/tag_rec1.png)</span><br><span class="line"></span><br><span class="line">![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/tag_rec2.png)</span><br><span class="line"></span><br><span class="line">![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/tag_rec3.png)</span><br><span class="line"></span><br><span class="line">5. 基于邻域的标签扩展</span><br><span class="line"></span><br><span class="line">**若两个标签同时出现在很多物品的标签集合中，这两个标签就具有较大的相似度**，所以可以基于邻域原理计算出相似标签。</span><br><span class="line"></span><br><span class="line">6. 标签清理</span><br><span class="line">+ 去除词频很高的```stopword</span><br></pre></td></tr></table></figure></p></li><li><p>去除因词根不同造成的同义词</p></li><li>去除因分隔符造成的同义词</li><li>让用户主动反馈不合适的词</li></ul></li><li><p>基于图的标签推荐算法</p><p> 数据结构：<figure class="highlight v"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">建立图结构之后，可用PersonalRank算法进行随机游走。</span><br><span class="line"></span><br><span class="line"><span class="number">8</span>. 给用户推荐标签</span><br><span class="line">+ 为什么要给用户推荐标签</span><br><span class="line">+ 方便用户输入标签，降低用户打标签的难度</span><br><span class="line">+ 提高标签质量，减少冗余的同义词</span><br><span class="line">+ 如何给用户推荐标签</span><br><span class="line">+ 直接推荐整个系统里最热门的标签</span><br><span class="line">+ 给用户推荐物品<span class="meta">```i```上的最热门标签```item[i][b]</span></span><br></pre></td></tr></table></figure></p><pre><code>+ 给用户推荐他自己常用的标签+ 融合以上两个方法，进行线性加权+ 或者基于图作标签推荐</code></pre></li></ol>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
            <tag> 推荐 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>《推荐系统实践》笔记 #2</title>
      <link href="/2018/05/19/recsys-note2/"/>
      <url>/2018/05/19/recsys-note2/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文100%由本人（Haoxiang Ma）原创，如需转载请注明出处。</p></blockquote><h3 id="利用用户行为数据"><a href="#利用用户行为数据" class="headerlink" title="利用用户行为数据"></a>利用用户行为数据</h3><ol><li>用户行为数据类别<ul><li>session log</li><li>impression log</li><li>click log</li></ul></li><li>用户行为类型<ul><li>显性反馈 => 明确选择<code>喜欢</code> or <code>不喜欢</code>，数量较少</li><li>隐性反馈 => 没有明确选择，多数为<code>浏览</code> or <code>点击</code>，数据极多</li></ul></li><li><p>用户行为记录的数据结构</p><p>item</p><p>remark</p><p>user_id</p><p>用户id</p><p>item_id</p><p>物品id</p><p>behavior_type</p><p>行为类型（购买／浏览／点赞／点灭。。。）</p><p>context</p><p>上下文信息（时间／地点。。。）</p><p>behavior_weight</p><p>权重（视频观看时长／文章评分。。。）</p><p>behavior_content</p><p>内容（评论的文本／打标签中的标签。。。）</p></li><li><p>基于用户行为数据的算法</p><ul><li>用户协同过滤算法（User-based CF）</li><li>物品协同过滤算法（Item-based CF）</li></ul></li><li><p>用户协同过滤算法</p><ul><li><p>算法步骤</p><ol><li>找到和目标用户相似的用户集合<code>U</code></li><li>找到<code>U</code>中每个用户<code>u</code>喜欢的物品<code>i</code>，且当前目标用户未对<code>i</code>有过任何行为</li><li>利用余弦相似度计算用户间的相似度 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/cosine_similarity.png" alt=""></li></ol></li><li><p>三个中间结果矩阵</p><ul><li><code>W[][]</code>：<code>W[u][v]</code>代表了用户<code>u</code>和用户<code>v</code>间的相似度</li><li><code>C[][]</code>：<code>C[u][v]</code>等于<code>|N(u)|∩|N(v)|</code></li><li><code>N[]</code>：<code>N[u]</code>等于用户<code>u</code>发生过行为的物品数</li></ul></li><li>优化改进<ul><li>为了提高算法效率，其实当<code>|N(u)|∩|N(v)| = 0</code>的时候（即两个用户间没有任何交集），压根不用去计算<code>u</code>和<code>v</code>间的相似度。所以可以建立<code>物品-用户倒排表</code>，没有交集的用户<strong>绝对不会</strong>出现在同一个物品所属的链中。</li><li>从理论上讲，应该<strong>多考虑冷门物品的贡献度，适当惩罚热门物品带来的相关性</strong>，因为热门物品可能每个人都会买，不应该带来太多的个性化相关性。所以要对原始的公式引入惩罚机制。 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/user_penalty.png" alt=""></li></ul></li></ul></li><li><p>物品协同过滤算法 <strong>物品相似度并不是利用物品的内容属性计算其相似度，而是从用户行为记录的角度计算</strong></p><ul><li>算法步骤<ol><li>计算物品间的相似度（矩阵）</li><li>根据相似度矩阵和用户的历史行为记录生成推荐列表</li></ol></li><li>三个中间结果矩阵<ul><li><code>W[][]</code>：<code>W[i][j]</code>代表了物品<code>i</code>和物品<code>j</code>间的相似度</li><li><code>C[][]</code>：<code>C[i][j]</code>等于<code>|N(i)|∩|N(j)|</code>, 就是同时对<code>i</code>和<code>j</code>发生过行为的用户数</li><li><code>N[]</code>：<code>N[i]</code>等于对物品<code>i</code>发生过行为的用户数</li></ul></li><li><p>算法优点 能够提供推荐解释，利用用户历史上喜欢的物品为现在的推荐结果进行解释。不像<code>User-based CF</code>那样无法提供合理的解释。</p></li><li><p>优化改进</p><ul><li><p>引入Inverse User Frequency对活跃用户进行惩罚 =&gt; 活跃用户对物品相似度的贡献应小于非活跃用户 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/IUF.png" alt=""> <strong>在实际生产环境中，可直接忽略特别大的兴趣列表，提高算法效率。</strong></p></li><li><p>将相似度矩阵按行归一化 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/normalize.png" alt=""> Example: <code>A</code>类类内物品相似度0.5，<code>B</code>类类内物品相似度0.6，AB类间物品相似度0.2。由于<code>B</code>类类内物品相似度最高，所以推荐10个物品时10个都会是<code>B</code>类物品。进行归一化后，<code>A</code>类与<code>B</code>类类内物品相似度均为1，推荐10个物品时会有5个<code>A</code>物品&amp;5个<code>B</code>物品</p></li></ul></li></ul></li><li><p><code>User-based CF</code>与<code>Item-based CF</code>综合比较</p><ul><li><code>User-based CF</code>着重于反映和用户兴趣相似的小群体的热点</li><li><code>Item-based CF</code>着重于维护目标用户的历史兴趣</li><li><code>User-based CF</code>维护用户相似度矩阵，<code>Item-based CF</code>维护物品相似度矩阵。需要考虑数据存储的代价和矩阵计算的代价 => 用户数多 or 物品数多</li></ul></li><li><p>哈利波特问题 《哈利波特》很热门，几乎买了任何书的人都会去买《哈利波特》，所以<strong>对热门物品要进行惩罚</strong>。 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/harryporter.png" alt=""> 通过提高alpha的取值（<code>[0.5, 1.0]</code>），可惩罚热门物品。</p></li><li><p>隐语义模型（<code>Latent Factor Model</code>)</p><ul><li><p>模型背景 <strong>仅靠用户行为数据无法解决跨领域的问题</strong>，例如很多人看完7点的新闻联播会继续开着电视看8点的电视剧，但给看了电视剧的人推荐新闻联播显然是不合理的。<strong>两个不同领域的最热门物品间往往会存在较高的相似度</strong>，不是因为它们真的相似，只是因为它们都很热门，所以大家都会看。</p></li><li><p>需要解决的问题 隐语义模型（<code>Latent Factor Model</code>) => <strong>基于用户行为数据的聚类</strong>，解决</p><ul><li>如何给物品分类（基于用户行为，而不是内容属性）</li><li>如何确定用户对哪些类的物品感兴趣，以及感兴趣的程度</li><li>对于一个给定的泪，选择其中哪些物品作推荐，推荐的权重是多少？</li></ul></li><li><p>理论 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/LFM1.png" alt=""> <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/LFM2.png" alt=""> 后面带<code>lambda</code>的两项为正则项，防止过拟合。 使用（随机）梯度下降，最小化损失函数<code>C</code>，计算得到<code>p</code>和<code>q</code>。 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/LFM3.png" alt=""> <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/LFM4.png" alt=""></p></li><li><p>关键参数</p><ul><li>隐特征个数<code>F</code>(或者叫<code>K</code>)</li><li>梯度下降的步长／学习速率<code>alpha</code></li><li>正则化参数<code>lambda</code></li><li>正／负样本的比例<code>ratio</code></li></ul></li><li>常见问题 隐语义模型（<code>Latent Factor Model</code>)在<strong>显式评分数据集</strong>上表现很好，而对于<strong>隐式评分数据集</strong>而言重点在于如何生成负样本。生成负样本的原则有<ul><li>对于每个用户，要保证正／负样本的平衡（相近或相等）</li><li>将原数据集中非常热门，但用户却没有过行为的物品当作负样本 => 不将冷门物品作为负样本是因为用户可能压根没有发现冷门物品，而不是对冷门物品不感兴趣。</li></ul></li><li>模型特点<ul><li>隐语义模型（<code>Latent Factor Model</code>)有较好的理论基础，是一种学习方法，有学习过程</li><li>中间结果的存储空间只需要<code>O(F * (M + N))</code>，（<code>F</code>为隐特征个数，<code>M</code>为用户数，<code>N</code>为物品数），而协同过滤则需要<code>O(N * N)</code></li><li>时间复杂度为<code>O(K * F *S)</code>，（<code>F</code>为隐特征个数，<code>K</code>为行为记录数，<code>S</code>为迭代次数）</li><li>不适合用于实时推荐，用户发生了新行为后，推荐列表不会发生变化</li><li>很难像<code>Item-based CF</code>一样用自然语言解释推荐原因</li></ul></li></ul></li><li><p>基于<strong>图</strong>的推荐模型 <strong>用户行为容易用图结构表示</strong>，<code>G = (V, E)</code>，<code>V = V(u) ∩ V(i)</code>。用户和物品均是图中的顶点，若用户<code>u</code>对物品<code>i</code>发生过行为，则存在边<code>e(u, i)</code>。 所以给用户<code>u</code>推荐物品 = 找到跟<code>V(u)</code>无<strong>直接边</strong>相连的顶点中相关性最高的顶点。 顶点两两间的相关性取决于</p><ul><li>两点间的路径数</li><li>两点间路径的长度</li><li>两点间路径所经过的点</li></ul><p><strong>如果两点间有很多条不同路径／两点间每条路径的长度较短／两点间的路径不会经过出度较大的点，那么它们的相关性相对而言就比较高。</strong> <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/PR.png" alt=""></p><p><strong><em>PersonalRank算法</em></strong>，采用了<strong>随机游走</strong>的概念，图中每个物品顶点<code>V(i)</code>被访问的概率<code>PR(v(i))</code>即为该物品<code>i</code>最后在推荐列表中的权重。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
            <tag> 推荐 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>《推荐系统实践》笔记 #1</title>
      <link href="/2018/05/12/recsys-note1/"/>
      <url>/2018/05/12/recsys-note1/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文100%由本人（Haoxiang Ma)原创，如需转载请注明出处</p></blockquote><h2 id="概念与定义"><a href="#概念与定义" class="headerlink" title="概念与定义"></a>概念与定义</h2><ol><li>推荐系统是一个复合系统，用于将<code>User</code>和<code>Item</code>互相关联，给<code>User</code>推荐“合适的”<code>Item</code>，同时给<code>Item</code>找到潜在的买家／用户。系统起源于<strong>信息过载</strong>问题，随着互联网信息爆炸性增长，用户没有办法短时间内接受完所有信息。</li><li>一般情况下，推荐系统的结构为<ul><li>前端页面</li><li>后端服务（日志系统）</li><li>推荐算法系统</li></ul></li><li>如何评价一个推荐系统的优劣？要从多个角度入手进行考察<ul><li>离线实验<ol><li>首先从海量日志文件中收集、清洗所需的用户数据，生成标准数据集</li><li>将数据集随机分成<code>训练集</code> &amp; <code>测试集</code></li><li>用<code>训练集</code>训练出兴趣模型，在<code>测试集</code>上进行预测测试</li><li>建立一个评价公式去评估测试结果</li></ol></li><li>用户调查（问卷）<ol><li>设计问卷，按照一定的规则选取用户进行调查</li><li><strong>成本较高，难以设计</strong></li></ol></li><li>在线实验（<strong><em>A/B Test</em></strong>）<ol><li>将用户随机／按照特定规则分成<code>K</code>组，在每组用户上各自应用不同的算法，然后比较不同组用户的评价指标（点击率，转化率等等）</li><li>需要在前端的流量入口就将用户分组，并打上组别标签，各组分别收集日志</li></ol></li><li>设计新系统时需要结合以上3种方法<ol><li>用<code>离线实验</code>证明各个离线指标优于当前系统</li><li>用<code>用户调查（问卷）</code>证明用户满意度高于当前系统</li><li>用<code>在线实验（A/B Test）</code>确定商业指标&amp;其他所关心的指标上优于当前系统</li></ol></li></ul></li><li><p>评价指标</p><ul><li>用户满意度 => 通过问卷调查或前端反馈页面</li><li><p>预测准确度 => 通过<code>离线实验</code></p><ol><li><p>评分预测型系统（预测用户给他没有见过的<code>Item</code>打多少分）</p><ul><li><p>RMSE（均方根误差） <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/RMSE.png" alt=""></p></li><li><p>MAE（平均绝对误差） <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/MAE.png" alt=""></p></li></ul></li><li><p>TopN推荐型系统（给用户推荐<code>N</code>个他没见过的<code>Item</code>)</p><ul><li><p>Precision（准确率） 简单来说就是<strong>推荐出来的结果中有多少个是当前用户真正感兴趣的</strong> <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/precision.png" alt=""></p></li><li><p>Recall（召回率） 简单来说就是<strong>有多少当前用户真正感兴趣的物品被成功推荐出来</strong> <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/recall.png" alt=""></p></li><li><p>为了全面一点，可以对TopN中的<code>N</code>取多个值，绘制出一条<code>p/r curve</code></p></li></ul></li><li><p>在实际生产环境中更多会使用<strong>TopN</strong>模型，因为用户给一个<code>Item</code>打高分不等于他很想购买／阅读该物品</p></li></ol></li><li><p>覆盖率 => 系统能够推荐出来的物品占总物品集合的比例 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/coverage.png" alt=""></p></li><li><p>多样性</p><ul><li><p>推荐结果列表中物品两两之间的<strong>不相似性</strong> <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/diversity1.png" alt=""></p></li><li><p>系统总体的<strong>不相似性</strong> <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/diversity2.png" alt=""></p></li></ul></li><li><p>新颖性 => 给用户推荐他们从来未听说过的，千万不能推荐他们已经看过／购买过的</p></li><li><p>惊喜度</p></li><li>信任度</li><li>实时性</li><li>健壮性</li><li>商业目标</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
            <tag> 推荐 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>[Leetcode题解] - Construct Binary Tree from XX-Order</title>
      <link href="/2018/04/22/leetcode-105106/"/>
      <url>/2018/04/22/leetcode-105106/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文行文由本人(Haoxiang Ma)原创，思路借鉴了Leetcode高票答案，加以个人分析、实现、总结。如需转载请注明出处。</p></blockquote><h2 id="题目背景"><a href="#题目背景" class="headerlink" title="题目背景"></a>题目背景</h2><ul><li><p><strong>[Leetcode 105]</strong> <code>Construct Binary Tree from Preorder and Inorder Traversal</code></p><p>Given preorder and inorder traversal of a tree, construct the binary tree.</p><p>Note:<br>You may assume that duplicates do not exist in the tree.</p><p>For example, given</p><p>preorder = [3,9,20,15,7]<br>inorder = [9,3,15,20,7]<br>Return the following binary tree:</p><pre><code>3</code></pre><p>   / \<br>  9  20</p><pre><code>/  \</code></pre><p>   15   7</p></li></ul><p>简单来说，就是给定二叉树的先序遍历结果+二叉树的中序遍历结果，重构二叉树。</p><ul><li><p><strong>[Leetcode 106]</strong> <code>Construct Binary Tree from Inorder and Postorder Traversal</code></p><p>Given inorder and postorder traversal of a tree, construct the binary tree.</p><p>Note:<br>You may assume that duplicates do not exist in the tree.</p><p>For example, given</p><p>inorder = [9,3,15,20,7]<br>postorder = [9,15,7,20,3]<br>Return the following binary tree:</p><pre><code>3</code></pre><p>   / \<br>  9  20</p><pre><code>/  \</code></pre><p>   15   7</p></li></ul><p>简单来说，就是给定二叉树的中序遍历结果+二叉树的后序遍历结果，重构二叉树。</p><h2 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h2><p><code>105</code>和<code>106</code>两题的题目背景类似，都是给定某两种二叉树的遍历结果，要求重构二叉树，<strong>且两题均提供了中序遍历结果</strong>。 首先，对题例中的二叉树结构以及其对应的三种遍历结果进行对比和分析。</p><pre><code>    3   / \  9  20    /  \   15   7preorder = [3,9,20,15,7]inorder = [9,3,15,20,7]postorder = [9,15,7,20,3]</code></pre><p>仔细观察以上三种遍历结果，不难发现以下规律：</p><ol><li><p><strong>从左到右</strong>遍历<code>preorder</code>序列，对于每一个元素，均能在<code>inorder</code>序列中找到。且该元素在二叉树中的<strong>左子树</strong>中的所有元素都在<code>inorder</code>序列里该元素的<strong>左边</strong>，该元素在二叉树中的<strong>右子树</strong>中的所有元素都在<code>inorder</code>序列里该元素的<strong>右边</strong>。例如对于<code>3</code>，其在二叉树中左子树里有<code>9</code>，正好在<code>inorder</code>序列里<code>9</code>在<code>3</code>的左边；其右子树里有<code>20</code>,<code>15</code>,<code>7</code>，正好在<code>inorder</code>序列里<code>20</code>,<code>15</code>,<code>7</code>都在<code>3</code>的右边。</p></li><li><p><strong>从右到左</strong>遍历<code>postorder</code>序列，对于每一个元素，均能在<code>inorder</code>序列中找到。且该元素在二叉树中的<strong>左子树</strong>中的所有元素都在<code>inorder</code>序列里该元素的<strong>左边</strong>，该元素在二叉树中的<strong>右子树</strong>中的所有元素都在<code>inorder</code>序列里该元素的<strong>右边</strong>。例如对于<code>20</code>，其在二叉树中左子树里有<code>15</code>，正好在<code>inorder</code>序列里<code>15</code>在<code>20</code>的左边；其右子树里有<code>7</code>，正好在<code>inorder</code>序列里<code>7</code>在<code>20</code>的右边。</p></li></ol><p>根据以上观察到的规律，我们可以大致得出以下思路： 对于<code>105</code>这道题，给定<code>preorder</code>和<code>inorder</code>，我们可以<strong>从左到右</strong>遍历<code>preorder</code>序列，对于每一个元素<strong>Ei</strong>，我们：</p><ol><li>选定<strong>Ei</strong>，构造当前节点。</li><li>在<code>inorder</code>序列中找到<strong>Ei</strong>，假定其index为<code>j</code>。</li><li>在<code>inorder</code>序列中，从<code>inorder_start</code>到<code>j-1</code>的元素肯定在<strong>Ei</strong>的左子树里，从<code>j+1</code>到<code>inorder_end</code>的元素肯定在<strong>Ei</strong>的右子树里。</li><li>继续递归构造左子树和右子树</li></ol><p>对于<code>106</code>这道题，给定<code>postorder</code>和<code>inorder</code>，我们可以<strong>从右到左</strong>遍历<code>postorder</code>序列，对于每一个元素<strong>Ei</strong>，我们：</p><ol><li>选定<strong>Ei</strong>，构造当前节点。</li><li>在<code>inorder</code>序列中找到<strong>Ei</strong>，假定其index为<code>j</code>。</li><li>在<code>inorder</code>序列中，从<code>inorder_start</code>到<code>j-1</code>的元素肯定在<strong>Ei</strong>的左子树里，从<code>j+1</code>到<code>inorder_end</code>的元素肯定在<strong>Ei</strong>的右子树里。</li><li>继续递归构造左子树和右子树</li></ol><p>可以看到，<code>105</code>和<code>106</code>的思路基本一致，差别仅仅在于<code>preorder</code>和<code>postorder</code>本身的性质差异，一个先访问<code>root</code>，另一个后访问<code>root</code>。所以对于<code>preorder</code>序列我们要<strong>从左到右</strong>遍历，而对于<code>postorder</code>我们要<strong>从右到左</strong>遍历。</p><h2 id="具体代码"><a href="#具体代码" class="headerlink" title="具体代码"></a>具体代码</h2><ul><li><p><strong>[Leetcode 105]</strong> <code>Construct Binary Tree from Preorder and Inorder</code></p><p>class Solution {</p><pre><code>public TreeNode buildTree(int[] preorder, int[] inorder) {    return build(preorder, inorder,                  0, preorder.length - 1,                  0, inorder.length - 1);}public TreeNode build(int[] pre, int[] in,                       int preStart, int preEnd,                       int inStart, int inEnd) {    if(pre == null || in == null || preStart &gt; preEnd || inStart &gt; inEnd) {        // corner case        return null;    }    // select current root element    int curVal = pre[preStart];    TreeNode cur = new TreeNode(curVal);    // search for current root element in inorder sequence    int index = -1;    for(int i = inStart;i &lt;= inEnd;i++) {        if(in[i] == curVal) {            index = i;            break;        }    }    if(index == -1) {        // invalid sequence        return null;    }    // count how many elements are in left &amp; right subtree    int left = index - inStart;    int right = inEnd - index;    // build recursively    cur.left = build(pre, in, preStart + 1, preStart + left, inStart, index - 1);    cur.right = build(pre, in, preStart + left + 1, preEnd, index + 1, inEnd);    return cur;}</code></pre><p>}</p></li></ul><ul><li><p><strong>[Leetcode 106]</strong> <code>Construct Binary Tree from Inorder and Postorder</code></p><p>class Solution {</p><pre><code>public TreeNode buildTree(int[] inorder, int[] postorder) {    return build(inorder, postorder,                  0, inorder.length - 1,                  0, postorder.length - 1);}public TreeNode build(int[] in, int[] post,                       int inStart, int inEnd,                       int postStart, int postEnd) {    if(in == null || post == null || inStart &gt; inEnd || postStart &gt; postEnd) {        // corner case        return null;    }    // select current root element    int curVal = pre[preStart];    TreeNode cur = new TreeNode(curVal);    // search for current root element in inorder sequence    int index = -1;    for(int i = inStart;i &lt;= inEnd;i++) {        if(in[i] == curVal) {            index = i;            break;        }    }    if(index == -1) {        // invalid sequence        return null;    }    // count how many elements are in left &amp; right subtree    int left = index - inStart;    int right = inEnd - index;    // build recursively    cur.left = build(in, post, inStart, index - 1, postStart, postStart + left - 1);    cur.right = build(in, post, index + 1, inEnd, postStart + left, postEnd - 1);    return cur;}</code></pre><p>}</p></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>对于这两道题，需要注意以下几点</p><ol><li><code>preorder</code>序列里<code>root</code>排在前面，<code>postorder</code>序列里<code>root</code>排在后面。</li><li>从<strong>数组</strong>中重构二叉树，往往就是<strong>类似于先序遍历的递归写法</strong>，先在数组中定位到当前节点的值，构造好当前节点，接着<strong>确定好数组边界</strong>，递归构造左子树 &amp; 右子树。</li><li><p>类似的构造二叉树题目，可参考</p><ul><li><strong>[Leetcode 108]</strong> <code>Convert Sorted Array to Binary Search Tree</code></li></ul><p>同样是从<strong>数组</strong>中重构二叉树。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Leetcode </category>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> Leetcode </tag>
            
            <tag> 数据结构 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>[Leetcode题解] - Unique BST I &amp;&amp; II</title>
      <link href="/2018/04/05/lc-uniquebstiii/"/>
      <url>/2018/04/05/lc-uniquebstiii/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文行文由本人(Haoxiang Ma)原创，思路借鉴了Leetcode高票答案，加以个人分析与总结。如需转载请注明出处。</p></blockquote><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><ul><li><p><strong><em>Unique Binary Search Tree II</em></strong></p><p>Given an integer n, generate all structurally unique BST’s (binary search trees) that store values 1…n.</p><p>For example,<br>Given n = 3, your program should return all 5 unique BST’s shown below.</p></li></ul><pre><code>1         3     3      2      1 \       /     /      / \      \  3     2     1      1   3      2 /     /       \                 \2     1         2                 3</code></pre><ul><li><p>即给定一个正整数<code>n</code>，求出<code>1 ~ n</code>能构成的<strong>所有</strong>二叉搜索树，返回所有可能的根节点。</p></li><li><p><strong><em>Unique Binary Search Trees</em></strong></p></li></ul><pre><code>Given n, how many structurally unique BST&apos;s (binary search trees) that store values 1...n?For example,Given n = 3, there are a total of 5 unique BST&apos;s.   1         3     3      2      1    \       /     /      / \      \     3     2     1      1   3      2    /     /       \                 \   2     1         2                 3</code></pre><ul><li>即给定一个正整数<code>n</code>，求出<code>1 ~ n</code>能构成的<strong>所有</strong>二叉搜索树的总棵数，返回总棵数。</li></ul><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>以上两道题殊途同归，输入均为一个正整数<code>n</code>，一个是要生成<strong>所有</strong>合法的二叉搜索树，另一个则想求出<strong>合法</strong>二叉搜索树的总棵树。 <strong>假如我们有办法基于<code>1 ~ n</code>生成所有合法的<code>BST</code>，那么不管是题1还是题2均能被轻松解决了。</strong> 思路主要有以下两点：</p><ol><li>凡是遇到求<strong>所有</strong>，<strong>全部</strong>的题目，基本可参考<code>DFS</code>的思路。因为<code>DFS</code>就是在解集构成的图里不断走走走，一路走到黑，遇到满足条件的就塞进结果里，遇到死路就往回走试试别的路。自然就能把整个解集（所有可能性）给遍历完，<strong>所有</strong>符合条件的解也就都被找到了。</li><li>既然让我们生成<code>BST</code>，那我们就参考参考<code>BST</code>的特性。用最通俗易懂的话来讲，<code>BST</code>中的每个节点，其左子树里所有子孙的值均小于它，其右子树里所有子孙的值均大于它。</li></ol><p>基于以上两点，我们进行<code>DFS</code>的流程就很清晰了。 在<code>1 ~ n</code>中，我们随便挑一个数<code>k</code>作为当前的<code>root</code>，<code>k</code>可以是<code>1 ~ n</code>中的任意一个，为了生成所有可能性，我们可用一个<code>for</code>循环对所有可能的取值进行遍历。此外，因为以上提及的<code>BST</code>的性质，所以比<code>k</code>小的<code>1 ~ k-1</code>都要被放到左子树，比<code>k</code>大的<code>k+1 ~ n</code>都要被放到右子树。即到了构建左子树的时候，能用的数字就是<code>1 ~ k-1</code>，到了构建右子树的时候，能用的数字就是<code>k+1 ~ n</code>。一直这样<code>DFS</code>走下去构建左子树和右子树，<strong>直到没有任何可用的数字</strong>，就返回<code>null</code>。</p><h2 id="详细代码"><a href="#详细代码" class="headerlink" title="详细代码"></a>详细代码</h2><pre><code>/** * Definition for a binary tree node. * public class TreeNode { *     int val; *     TreeNode left; *     TreeNode right; *     TreeNode(int x) { val = x; } * } */class Solution {    public List&lt;TreeNode&gt; generateTrees(int n) {        // 处理corner case        if(n &lt;= 0) {            return new ArrayList&lt;&gt;();        }        return dfs(1, n);    }    public List&lt;TreeNode&gt; dfs(int start, int end) {        List&lt;TreeNode&gt; result = new LinkedList&lt;&gt;();        // 没有任何可用数字时，只能返回null        if(start &gt; end) {            result.add(null);            return result;        }        // 对于当前可选的数字start ~ end，其中任意一个都能选为当前的根节点        for(int i = start;i &lt;= end;i++) {            // 生成所有可能的左子树(start ~ i-1)和右子树(i+1 ~ end)            List&lt;TreeNode&gt; leftNodes = dfs(start, i - 1);            List&lt;TreeNode&gt; rightNodes = dfs(i + 1, end);            // 对于左子树的根节点的所有可能性和右子树的根节点的所有可能性，            // 作一个笛卡尔积，求出所有组合的可能性            for(TreeNode left : leftNodes) {                for(TreeNode right : rightNodes) {                    TreeNode cur = new TreeNode(i);                    cur.left = left;                    cur.right = right;                    result.add(cur);                }            }        }        return result;    }}</code></pre><h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>基于以上<code>DFS</code>思路的代码，我们可以用<code>1 ~ n</code>生成所有合法的<code>BST</code>，解决了问题。</p><ul><li>如果需要返回所有可能的<code>BST</code>，直接返回最终的<code>List&lt;TreeNode&gt;</code>即可。</li><li>如需返回总棵数，返回最终生成的<code>List</code>的<code>size</code>即可。</li></ul><p>⚠️<strong>但是，我们能不能再做得更好一点呢？像题目2中只要求出一个总棵数，一个正整数而已，我们需要如此“大兴土木”地把所有合法的<code>BST</code>构造出来，最后“轻描淡写”地返回<code>List</code>的<code>size</code>吗？是否存在一些数学规律，能够让我们直接求出基于<code>1 ~ n</code>的合法<code>BST</code>的总棵数呢？</strong> 首先我们可以尝试将问题公式化。 我们用<code>Sum(n)</code>表示<code>n</code>个连续数能构成的合法<code>BST</code>总棵数；用<code>F(k, n)</code>表示给定<code>n</code>个连续数时，以<code>k</code>为根的合法<code>BST</code>的棵数。显然我们能够得到以下的推导式：</p><pre><code>// 给定n个连续数，// 能够构成的合法BST总棵数 = 以1为根的BST棵数                         + 以2为根的BST棵数                         + ...                         + 以n为根的BST棵数。Sum(n) = F(1, n) + F(2, n) + F(3, n) + ... + F(n, n)     (1)</code></pre><p>同时，以<code>k</code>为根的合法<code>BST</code>的棵数：<code>F(k, n)</code>又等于多少呢？很简单，以<code>k</code>为根，根据<code>BST</code>的性质，<strong>左子树里只能存放<code>1 ~ k-1</code>这<code>k-1</code>个连续数，右子树里只能存放<code>k+1 ~ n</code>这<code>n-k</code>个连续数</strong>。所以<code>F(k, n)</code>取决于其左子树的可能性 * 其右子树的可能性：</p><pre><code>F(k, n) = Sum(k-1) * Sum(n-k)     (2)</code></pre><p>基于以上(1)和(2)推导式，我们可以改写得到：</p><pre><code>// Corner Case: Sum(0) = 1 因为0个数只能生成空树，空树永远只有1种// Corner Case: Sum(1) = 1 因为1个数只能生成单个节点的树，也永远只有1种Sum(n) = F(1, n) + F(2, n) + ... + F(n, n)       = Sum(0)*Sum(n-1) + Sum(1)*Sum(n-2) + ... + Sum(n-1)*Sum(0)     (3)</code></pre><p>用(3)式进行计算，一种非常类似于<code>DP</code>的思想，即可避免浪费大片内存和时间生成所有合法的<code>BST</code>，直接能通过<strong>数值运算</strong>得到最终结果。 代码如下</p><pre><code>class Solution {    public int numTrees(int n) {        if(n &lt;= 0) {            return 0;        }        int[] Sum = new int[n + 1];        Sum[0] = 1;    // 0个数时只有1种，空树        Sum[1] = 1;    // 1个数时只有1种，单个节点的树        for(int i = 2;i &lt;= n;i++) {            for(int j = 0;j &lt; i;j++) {                Sum[i] += Sum[j] * Sum[i - j - 1];            }        }        return Sum[n];    }}</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>此题利用了</p><ol><li><code>BST</code>的特性，“左小右大”来构建所需的结果。</li><li><code>DFS</code>的思想生成<strong>所有</strong>解。</li><li>数学推导式进行优化，<strong>当所求的解只是一个简单的整数值时，可思考是否需要真正生成所有数据，很多情况下只是需要我们进行推导，利用DP的思想进行数值运算即可。</strong></li></ol>]]></content>
      
      
      <categories>
          
          <category> Leetcode </category>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> Leetcode </tag>
            
            <tag> 数据结构 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>什么是Copy-On-Write</title>
      <link href="/2018/03/31/whatiscow/"/>
      <url>/2018/03/31/whatiscow/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文为本人（Haoxiang Ma)原创，如需转载请注明出处。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>所谓<code>Copy-On-Write</code>，简称<code>COW</code>，正如它的名字所示，即为<code>写时复制</code>。具体而言，它是容器的一种特性，或者说可以利用这种特性开发一种<code>COW容器</code>。 简单来说，<code>写时复制</code>就是在用户（线程）对某一个容器（如List）进行写操作（增／删／改）时，不直接在容器上进行操作，而是先<strong>复制</strong>一个与原容器一模一样的容器出来，然后在复制出来的容器上进行真正的写操作。 在<code>Java</code>中，常见的<code>COW容器</code>有<code>CopyOnWriteArrayList&lt;E&gt;</code>和<code>CopyOnWriteArraySet&lt;E&gt;</code>。</p><h2 id="具体解析"><a href="#具体解析" class="headerlink" title="具体解析"></a>具体解析</h2><p><img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/Timeline.png" alt=""> 如上图所示，最开始我们只有一个容器（内存地址<code>0x80</code>），和一个指向该容器的引用<code>A</code>。经过<code>T1</code>和<code>T2</code>后，没有任何用户（线程）对其进行写操作。 到达<code>T3</code>时刻时，某个用户（线程）申请向容器写入<code>123</code>这个数据。此时由于<code>Copy-On-Write</code>特性，并不是直接往<code>0x80</code>地址的容器写入，而是先复制出一个跟原容器一模一样的容器B（内存地址<code>0x95</code>），然后往<code>0x95</code>处的这个容器B写入<code>123</code>这条数据。<strong>写入完成后，关键是要将<code>引用A</code>进行重指向，指向<code>0x95</code>，不再指向原本的<code>0x80</code>。</strong>一系列动作完成后，到了<code>T4</code>时刻，<code>引用A</code>所指向的就是一个存有<code>123</code>这条数据的容器。 （⚠️如无意外，<code>0x80</code>处的原容器所占的内存空间在某个时候会被gc回收掉） 经过了以上的图解，相信读者已经对<code>Copy-On-Write</code>过程有了一定的理解。接下来谈谈其中的一些细节点。</p><ol><li><p>排他性的写操作 在对<code>COW容器</code>进行写操作时，会于内存中额外复制一个副本出来进行操作，所以<strong>写操作必然是排他性的</strong>。绝<strong>不能</strong>允许<code>N</code>个线程同时进行写操作，然后复制出<code>N</code>个副本，各线程自己对自己的那个副本进行操作，那样的话无法保证容器内的数据一致性，<code>N</code>个副本中的数据都不是完整的数据。 <strong>为了保证数据的一致性，需要对写操作加上一个<code>排他锁</code>，在某一时刻只能有一个线程对<code>COW容器</code>进行写操作。</strong></p></li><li><p>读写分离 对于<code>读</code>操作而言，允许多个线程并发读取容器内的数据，不存在任何数据一致性问题或安全问题。对于<code>写</code>操作，因为会复制一个副本容器，在副本容器上写，写成功后再修改引用，所以在某个时刻，<code>写</code>和<code>读</code>针对的并不是同一个容器，实现了<strong>读写分离</strong>。 有了<strong>读写分离</strong>，就能显著提高<code>读</code>操作的效率，因为写锁是会排斥其他所有操作的，一旦一个容器／表／文件被加上了写锁，那么任何人都无法再读／写该容器／表／文件的内容。既然<code>COW容器</code>的复制机制能保证读和写是在不同的容器上进行，也就意味着永远都不会因为写锁的存在而阻塞<code>读</code>操作，自然就能顺畅无比地并发读取了。</p></li></ol><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>基于写时复制的特点，<code>COW容器</code>特别适用于大量读取，极少量写入的应用场景。因为写操作是排他性阻塞的，所以一旦有较多的写操作需求，那<code>COW容器</code>的性能将会灾难性地下降。当写操作较多时，可将多个写操作合并成一批写操作，call一次写入方法写入多条数据，避免多次call写方法，避免多次容器复制。</p><ol><li>优点<ul><li>适用于<strong>大量读取，极少量写入</strong>的应用场景，支持高效的并发读取</li></ul></li><li>缺点<ul><li><strong>内存消耗大</strong>。当容器中的数据很多时，复制操作将会消耗大量的内存，可能会频繁引发GC</li><li><strong>无法保证数据<code>强一致性</code>，只能保证<code>最终一致性</code></strong>，因为读的时候有可能已完成了写操作，但容器指针未来得及重指向，但经过多个时间窗口后最终数据是一致的</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Java中的深复制&amp;浅复制</title>
      <link href="/2018/03/23/deepcopyinjava/"/>
      <url>/2018/03/23/deepcopyinjava/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文内容均为本人（Haoxiang Ma）原创，如需转载请注明出处。</p></blockquote><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>在程序中，出于特定的需求，往往要对一些对象进行复制，然后对复制后的对象进行操作，<strong>且不想让这些操作影响到原对象的数据</strong>。在这种情况下，搞清楚<code>深复制（Deep Copy）</code>和<code>浅复制（Shallow Copy）</code>是非常有必要的。 何谓<code>浅复制</code>？简单来说，就是对一个对象进行复制，得到一个新对象，然后“照搬”原对象中的数据来填充新对象的数据域。虽然新旧对象是两个不同的对象（在内存中的地址不同），但是它们内部的数据是一样的。 这样看来，<code>浅复制</code>不正是我们所需的<code>复制</code>嘛，可以得到一个新的对象，且新对象中的数据跟原对象一样，已经是非常完美的复制啊，为什么还要加一个<code>浅</code>字呢？ <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/shallow.png" alt=""> 原因在于，<strong>在Java中</strong>，一个对象里可能有<code>primitive</code>类型的数据如<code>int</code>,<code>long</code>等，更可能包含了<code>referrence（引用）</code>类型的数据如指向自定义类实例的引用。众所周知，<strong>引用的值仅仅是其所指向对象的内存地址</strong>。通过<code>浅复制</code>，我们会把这个内存地址“照搬”过去新对象中，那么新对象和旧对象中的一个成员引用就会指向同一块内存，操作的时候将会互相影响，与我们理想状态下的<code>复制</code>相差甚远。如上图所示，从<code>Building1</code>对象复制出来<code>Building2</code>对象后，它们本身所处的内存地址不一样，已经成为了两个独立的对象，但由于<code>Building</code>中包含了一个引用类型的数据<code>Floor</code>，而<code>浅复制</code>只将引用的值照搬过去新对象中，导致两个不同的<code>Building</code>中的<code>Floor</code>指向了内存里的同一个<code>Floor</code>。 <strong>所谓的<code>浅</code>指的就是对于<code>referrence（引用）</code>类型的数据只是“浅显”地“照搬”其值，没有深度地“复制”出一个新的对象。</strong> 所以对应<code>浅复制</code>，为了解决它的问题，我们自然就有<code>深复制</code>的概念。所谓<code>深复制</code>就是为了完完全全、彻彻底底地对一个对象进行<strong>深度</strong>的复制，避免新旧对象中的引用仍指向同一块内存区域，互相影响。 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/deep.png" alt=""> 如上图所示，从<code>Building1</code>对象复制出来<code>Building2</code>对象，两者的<code>Floor</code>引用指向的已是内存区域中不同的<code>Floor</code>对象，无论对<code>Building1</code>再怎么折腾，也不会对<code>Building2</code>产生影响，完成真正理想状态下的<code>复制</code>。 <strong>本文将探讨在<code>Java</code>中如何实现对象的<code>深复制</code>。</strong></p><h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><p>首先来看看我们的自定义类<code>Building</code>和<code>Floor</code></p><pre><code>// Building 类class Building{    public Floor floor;    public Building() {        this.floor = new Floor();    }    public Building(Floor floor) {        this.floor = floor;    }}// Floor类class Floor implements Cloneable {    public int count;    public Floor() {    }    public Floor(int count) {        this.count = count;    }}</code></pre><p>可以看到一个<code>Building</code>中包含一个<code>Floor</code>成员变量。</p><ol><li><p><code>clone</code>方法与<code>Cloneable</code>接口 <strong>实现<code>深复制</code>的第一种方法，需要复制的类实现<code>Cloneable</code>接口，并重写<code>clone</code>方法。</strong>在<code>Java</code>的上帝类<code>Object</code>中，有一个纯天然的<code>clone</code>方法，但是其中并没有具体的代码逻辑，仅仅是声明了一个<code>CloneNotSupportedException</code>异常，所以必须对其进行重写。</p><pre><code>protected native Object clone() throws CloneNotSupportedException;</code></pre></li></ol><pre><code>而`Cloneable`接口中也没有任何的方法声明，完全是一个标记性的空接口(`Mark-Interface`)。**实现该接口的作用仅仅是作一个`标记(mark)`，告诉JVM，我实现了这个接口，这个类的实例对象可被复制**。    public interface Cloneable {    }&gt; ⚠️ ⚠️ ⚠️注意，如果不实现`Cloneable`接口，即使重写了`clone`方法，在调用时也会自动抛出异常，因为没有**标记**，没有**“告诉”**JVM，这是可以被复制的。所以基于`Building`类和`Floor`类的定义，我们应该让它们都实现`Cloneable`接口，且重写`clone`方法。    // Building类实现Cloneable接口    class Building implements Cloneable {        public Floor floor;        public Building() {            this.floor = new Floor();        }        public Building(Floor floor) {            this.floor = floor;        }        @Override        public Object clone() throws CloneNotSupportedException {            // 先clone出一个Building对象            Building newBuilding = (Building) super.clone();             // 然后再手动clone出一个Floor对象            newBuilding.floor = (Floor) floor.clone();            // 返回clone出的Building对象            return newBuilding;        }    }    // Floor类实现Cloneable接口    class Floor implements Cloneable {        public int count;        public Floor() {        }        public Floor(int count) {            this.count = count;        }        @Override        public Object clone() throws CloneNotSupportedException {            return super.clone();        }    }为了测试是否真的进行了`深复制`，我们看看新旧`Building`对象的内存地址是否相同，再看看它们内部的`Floor`引用所指向的对象内存地址是否相同。    public static void main(String[] args) throws Exception{        Building b1 = new Building();        Building b2 = b1;        System.out.println(b2 == b1);   // true        Building b3 = (Building) b1.clone();         System.out.println(b3 == b1);       // false        System.out.println(b3.floor == b1.floor); // false    }通过实现`Cloneable`接口，重写`clone`方法，成功进行了`深复制`。 **⚠️然而，如下图所示，使用这种方法实现`深复制`，有着巨大的缺陷。** ![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/chain.png) 在以上例子中，我们只有2个自定义类，`Building`包含`Floor`。但是当这种包含关系变得更加复杂，有无数多个自定义类，一层又一层地包含下去时，我们需要为每个自定义类都实现`Cloneable`接口，重写`clone`方法。（例如`Building`包含`Floor`，`Floor`包含`Room`）。更麻烦的是，处于高层次的类的`clone`方法会很复杂，要一个一个地对低层次的类调用`clone`方法，一旦其中有几个类的结构发生了变化，又要重新改写多个`clone`方法，一点也不科学。</code></pre><ol start="2"><li><p>利用<code>Serializable</code>接口 首先说说什么是<code>Serialize（序列化）</code>和<code>Deserialize(反序列化）</code>。</p><ul><li><strong><code>Serialize（序列化）</code>指的是将内存中的对象转化为二进制数据，进而将对象数据存储到磁盘文件里</strong>。</li><li><p><strong><code>Deserialize(反序列化）</code>指的是从磁盘文件中读取二进制数据，根据一定的规则转化为内存中的对象</strong>。</p><p>public interface Serializable {<br>}</p></li></ul></li></ol><pre><code>在`Java`中，如果想将一个对象序列化，首先得标记其为“可序列化”，即实现`Serializable`接口。**跟`Cloneable`接口一样，`Serializable`接口也是一个空荡荡的`Mark-Interface`，它的唯一作用是用于标记该类可被序列化。** 为了便于区分，不再使用`Building`和`Floor`进行说明。以下使用`Human`类和`Head`类。    class Human implements Serializable {        public Head head;        public Human() {            this.head = new Head();        }        public Human(Head head) {            this.head = head;        }        /**         * 深复制         * @return 复制得到的Human对象         * @throws Exception         */        public Human deepClone() throws Exception {            ByteArrayOutputStream baos = new ByteArrayOutputStream();            ObjectOutputStream objectOutputStream = new ObjectOutputStream(baos);            objectOutputStream.writeObject(this);            ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());            ObjectInputStream objectInputStream = new ObjectInputStream(bais);            return (Human) objectInputStream.readObject();        }    }    class Head implements Serializable {        public int val;        public Head() {        }        public Head(int val) {            this.val = val;        }    }核心代码为`Human`类中的`deepClone()`方法。首先先将当前对象(`this`)写到一个`ObjectOutputStream`中，然后再新建一个`ObjectInputStream`，并利用该`ObjectInputStream`的`readObject()`方法从流中获得一个**新构建**的Java对象。 接下来，我们检测一下是否真正完成了`深复制`。    public static void main(String[] args) throws Exception{        Human h1 = new Human();        Human h2 = h1.deepClone();        System.out.println(h1 == h2);       // false        System.out.println(h1.head == h2.head);  // false    }⚠️通过实现`Serializable`接口，可以避免重写多个`clone()`方法，也可实现深复制。</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在<code>Java</code>中，想要实现真正的<code>深复制</code>，有以下两种方法</p><ol><li>实现<code>Cloneable</code>接口，重写<code>clone()</code>方法。<ul><li>优点：直观，容易理解，贴合人类思维。</li><li>缺点：当有非常多个自定义类，且互相包含的情况下，需要大量复杂地重写方法，对结构改动非常不友好。</li></ul></li><li>实现<code>Serializable</code>接口，利用<code>序列化</code>和<code>反序列化</code>。<ul><li>优点：当多个类结构发生变化时，不需要大量重写复制代码。</li><li>缺点：不直观，不贴合人类思维。</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>一致性Hash算法——分析与模拟</title>
      <link href="/2018/03/17/consistent-hash/"/>
      <url>/2018/03/17/consistent-hash/</url>
      
        <content type="html"><![CDATA[<blockquote><p>如需转载请注明出处。</p></blockquote><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>假设现在有一个存储集群——<code>Cluster_A</code>，<code>Cluster_A</code>包含了<code>N</code>个存储节点，每个存储节点上存储了大量数据。那么在一般情况下，新进入集群的数据会被计算出其对应的Hash值，然后按照一定的规则被分配到某个存储节点上。最常见的分配策略如下：</p><pre><code>// sha256或者md5均为常见的hash函数node_id = hash(data) % N</code></pre><p>在正常情况下，这种数据分配策略的性能取决于hash算法的性能，一般不会出什么大问题。但是往往在一个大集群中会出现<strong>增／删节点</strong>的需求，试想当<code>N</code>变成<code>N+1</code>或<code>N-1</code>时，数据所对应的新<code>node_id</code>大概率会与原<code>node_id</code>不一样。那么用户想从集群读数据时就会找不到该数据，因为算出来的新<code>node_id</code>与数据真正存储的<code>node_id</code>不一样了。</p><pre><code>// 原data_1所属的node_id = 50// data_1被存在50号节点上node_id1 = hash(data_1) % N = 50 // 增加一个存储节点后data_1对应的node_id// 用户读取data_1时，系统算出应该去30号节点取数据，导致读取失败node_id1 = hash(data_1) % (N+1) = 30</code></pre><p>为了保证数据的不丢失，在增／删节点发生后，需要对集群里的所有数据进行<strong>重新Hash</strong>，以进行数据的重新分配。<strong>然而，如果每次增／删节点都对所有数据进行重分配，系统开销将会是一个天文数字，在实际工程中难以承受，总不可能让用户等待几十分钟的重Hash过程才能成功读取数据吧</strong>。 为了解决这个问题，一种特殊的解决方案——<strong><em>一致性Hash算法</em></strong>横空出世。</p><h2 id="算法详解"><a href="#算法详解" class="headerlink" title="算法详解"></a>算法详解</h2><p><strong>一致性Hash算法的核心，是把节点本身也映射到和数据一致的Hash空间。</strong> 这句话听起来似乎有点拗口，<strong>简单而言就是使用一样的Hash方法，不仅对数据进行Hash，同时也对节点进行Hash。</strong>通过这样做，就能将数据和节点放置到一个空间中，假设<code>hash(x)</code>的值域为<code>[0, M]</code>，如下图所示，可以用一个二维的环表示此Hash空间。然后对每一条数据<code>data</code>计算<code>hash(data)</code>，把数据放置到环上顺时针方向最近的节点进行存储。 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/consistent_hash.png" alt=""></p><ul><li><p>正常情况（无节点增删） 假设集群内节点数量<code>N = 100</code>，数据条数<code>DATA_COUNT = 100000</code>。理想情况下，如果Hash算法的结果是均匀的，每个节点应该存储<code>100000 / 100 = 1000</code>条数据。为此我用Python写了一个模拟程序。</p><pre><code># encoding=utf-8from struct import unpack_fromfrom hashlib import md5import matplotlib.pyplot as pltNUM_NODES = 100NUM_DATA = 100000nodes = [0 for i in range(NUM_NODES)]# hash functiondef hash(data):    data_md5 = md5(str(data)).digest()    return unpack_from(&quot;=I&quot;, data_md5)[0]# distribute data to different nodesdef distribute():    for data in range(NUM_DATA):        h = hash(data)        index = h % NUM_NODES        nodes[index] += 1if __name__ == &apos;__main__&apos;:    &apos;&apos;&apos;        Case 1: 正常情况下的数据分布    &apos;&apos;&apos;    distribute()    max_node = max(nodes)    min_node = min(nodes)    print (&quot;Node with max data: {0} piece of data&quot;.format(max_node))    print (&quot;Node with min data: {0} piece of data&quot;.format(min_node))    # plot scatter graph    x = [i for i in range(NUM_NODES)]    y = nodes    plt.scatter(x, y, c=&apos;r&apos;)    plt.yticks(range(0, 2 * NUM_DATA / NUM_NODES, 100))    plt.xlabel(&quot;Node Index&quot;)    plt.ylabel(&quot;Data Count&quot;)    plt.show()</code></pre></li></ul><pre><code>经过模拟，得到如下的数据分布散点图： ![](https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/common.png) 可以看出每个节点存储的数据均在`1000`上下浮动，差距不大，可以看作是均匀分布。</code></pre><ul><li><p>增加／删除一个节点</p><pre><code># encoding=utf-8from struct import unpack_fromfrom hashlib import md5LESS_NUM_NODES = 99         # less nodesORIGINAL_NUM_NODES = 100    # original nodesMORE_NUM_NODES = 101        # more nodesNUM_DATA = 100000# hash functiondef hash(data):    data_md5 = md5(str(data)).digest()    return unpack_from(&quot;=I&quot;, data_md5)[0]# distribute data to more nodesdef distribute_more():    transfer_count = 0    for data in range(NUM_DATA):        h = hash(data)        original_index = h % ORIGINAL_NUM_NODES        more_index = h % MORE_NUM_NODES        if original_index != more_index:            transfer_count += 1    return transfer_count# distribute data to less nodesdef distribute_less():    transfer_count = 0    for data in range(NUM_DATA):        h = hash(data)        original_index = h % ORIGINAL_NUM_NODES        less_index = h % LESS_NUM_NODES        if original_index != less_index:            transfer_count += 1    return transfer_countif __name__ == &apos;__main__&apos;:    &apos;&apos;&apos;        Case 2: 当出现增/删节点时的数据分布情况    &apos;&apos;&apos;    # 新增节点    transfer_count = distribute_more()    print(&quot;##### When one new node is added #####&quot;)    print(&quot;Data that need to be transferred: {}&quot;.format(transfer_count))    print(&quot;Percentage of data that need to be transferred: {}%&quot;.format(transfer_count * 100.0 / NUM_DATA))    # 删除节点    transfer_count = distribute_less()    print(&quot;\n##### When one old node is deleted #####&quot;)    print(&quot;Data that need to be transferred: {}&quot;.format(transfer_count))    print(&quot;Percentage of data that need to be transferred: {}%&quot;.format(transfer_count * 100.0 / NUM_DATA))</code></pre></li></ul><p>数据迁移情况如下图所示 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/case2.png" alt=""> 可以看到，将节点个数从<code>N</code>增加到<code>N+1</code>后或从<code>N</code>减少到<code>N-1</code>后，有差不多<code>99%</code>的数据经过重Hash后都要进行数据迁移，这样的数据迁移压力绝对是无法承受的。</p><ul><li><p>应用一致性Hash算法后数据迁移情况</p><h1 id="encoding-utf-8"><a href="#encoding-utf-8" class="headerlink" title="encoding=utf-8"></a>encoding=utf-8</h1><p>from struct import unpack_from<br>from hashlib import md5<br>from bisect import bisect_left</p><p>ORIGINAL_NUM_NODES = 100    # original node count<br>NEW_NUM_NODES = 101         # new node count<br>NUM_DATA = 100000</p><h1 id="hash-function"><a href="#hash-function" class="headerlink" title="hash function"></a>hash function</h1><p>def hash(data):</p><pre><code>data_md5 = md5(str(data)).digest()return unpack_from(&quot;=I&quot;, data_md5)[0]</code></pre><h1 id="distribute-data-to-different-nodes"><a href="#distribute-data-to-different-nodes" class="headerlink" title="distribute data to different nodes"></a>distribute data to different nodes</h1><p>def distribute(original_nodes, new_nodes):</p><pre><code>transfer_count = 0for data in range(NUM_DATA):    h = hash(data)    original_index = bisect_left(original_nodes, h) % ORIGINAL_NUM_NODES    new_index = bisect_left(new_nodes, h) % NEW_NUM_NODES    if original_index != new_index:        transfer_count += 1return transfer_count</code></pre><p>if <strong>name</strong> == ‘<strong>main</strong>‘:</p><pre><code>&apos;&apos;&apos;    Case 3: 应用一致性Hash后, 需要移动的节点比例&apos;&apos;&apos;original_nodes = sorted([hash(i) for i in range(ORIGINAL_NUM_NODES)])   # 对node本身也取hashnew_nodes = sorted([hash(i) for i in range(NEW_NUM_NODES)])             # 对node本身也取hashtransfer_count = distribute(original_nodes, new_nodes)print(&quot;Percentage of data that need to be transferred: {}%&quot;.format(transfer_count * 100.0 / NUM_DATA))</code></pre></li></ul><p>数据迁移情况如下图所示 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/case3.png" alt=""> 实现了一致性Hash算法后，我们将节点也映射到了同一片Hash空间，成功地将数据迁移的比例从<code>99%</code>降低到<code>37%</code>左右。</p><ul><li><p>应用一致性Hash算法后数据分布情况</p><h1 id="encoding-utf-8-1"><a href="#encoding-utf-8-1" class="headerlink" title="encoding=utf-8"></a>encoding=utf-8</h1><p>from struct import unpack_from<br>from hashlib import md5<br>from bisect import bisect_left<br>import matplotlib.pyplot as plt</p><p>NUM_NODES = 100    # original node count<br>NUM_DATA = 100000</p><h1 id="hash-function-1"><a href="#hash-function-1" class="headerlink" title="hash function"></a>hash function</h1><p>def hash(data):</p><pre><code>data_md5 = md5(str(data)).digest()return unpack_from(&quot;=I&quot;, data_md5)[0]</code></pre><h1 id="distribute-data-to-different-nodes-1"><a href="#distribute-data-to-different-nodes-1" class="headerlink" title="distribute data to different nodes"></a>distribute data to different nodes</h1><p>def distribute(nodes, stat):</p><pre><code>for data in range(NUM_DATA):    h = hash(data)    index = bisect_left(nodes, h) % NUM_NODES    stat[index] += 1</code></pre></li></ul><pre><code>if __name__ == &apos;__main__&apos;:    &apos;&apos;&apos;        Case 4: 应用一致性Hash后, 数据的分布情况    &apos;&apos;&apos;    nodes = sorted([hash(i) for i in range(NUM_NODES)])    stat = [0 for i in range(NUM_NODES)]    distribute(nodes, stat)    max = max(stat)    min = min(stat)    print(&quot;Node with max data: {} piece of data&quot;.format(max))    print(&quot;Node with min data: {} piece of data&quot;.format(min))    # plot scatter graph    x = [i for i in range(NUM_NODES)]    y = stat    plt.scatter(x, y, c=&apos;r&apos;)    plt.yticks(range(0, 10 * NUM_DATA / NUM_NODES, 1000))    plt.xlabel(&quot;Node Index&quot;)    plt.ylabel(&quot;Data Count&quot;)    plt.show()</code></pre><p>应用一致性Hash后，数据分布情况如下图所示 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/case4.png" alt=""> 可以看到，相比期望值<code>1000</code>，出现了偏移的数据，存在很多低于<code>1000</code>条数据的节点。虽然数据迁移率降低了，但是出现了另一个问题——<strong>无法充分利用集群上的所有节点进行数据存储，造成了数据不平衡</strong>。</p><ul><li>一致性Hash算法——解决数据不平衡</li></ul><p>数据不平衡问题，根本原因就在于对节点进行Hash后，它们的Hash值在环上分布不够均匀。那么为了“分布均匀”，自然而然我们可以在环上稀疏的区域多添加一些“假”节点，也就是<strong>虚拟节点（Virtual Node）</strong>，以将稀疏的区域填充得满一点。 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/v_node.png" alt=""> 如图所示，通过在稀疏区域增加<strong>虚拟节点（Virtual Node）</strong>，原本介于<code>Node #3</code>和<code>data #1</code>间的数据可以被“存储”在<code>V-Node #2</code>或者<code>V-Node #1</code>上。再通过查询<code>V-Node</code>到<code>Node</code>的映射关系，即可从实际的存储节点取出数据。 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/v_node_query.png" alt=""> 通过保存<code>V-Node</code>到<code>Node</code>的<code>mapping</code>，可以迅速定位到实际存储节点。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>一致性Hash算法，核心在于解决集群节点增删场景下的数据丢失 &amp; 大量数据迁移问题。实现的关键是将节点本身也通过Hash函数映射到数据所在的Hash空间，从而使数据能够在某一固定空间内作“物理性”（顺时针、逆时针……）的位置分配。</p><h2 id="相关代码"><a href="#相关代码" class="headerlink" title="相关代码"></a>相关代码</h2><p>具体代码请查看<a href="https://github.com/AcepcsMa/Consistent_Hash" target="_blank" rel="noopener">我的Github目录</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
            <tag> 算法 </tag>
            
            <tag> 分布式 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>浅谈join与数据倾斜解决方案</title>
      <link href="/2018/03/04/join-di/"/>
      <url>/2018/03/04/join-di/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文100%为本人（Haoxiang Ma）原创内容，如需转载请注明出处。</p></blockquote><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p><code>join</code>操作在传统的关系型数据库中特别常见，往往用来连接两张或多张在某些键上有关联的表。但当我们的数据并没有被结构化存储到数据库中，手上仅有大量的<strong><em>raw data</em></strong> —— 成千上万个磁盘数据文件（如.dat或.txt）时，又或者当大量的数据文件存放在分布式的文件系统（HDFS）里，难以将其高效导入MySQL中时，我们很自然地就会思考：<strong>有没有不需要依靠关系型数据库而实现的join呢？对于海量文件能不能用Map-Reduce实现join操作呢？</strong> 假设现在系统中有两类数据文件：</p><ol><li><strong><em>用户数据文件（User）</em></strong></li><li><p><strong><em>订单数据文件（Order）</em></strong></p><p>// user文件结构<br>user_id, user_name, phone</p><pre><code>1,      aa,     100862,      bb,     10010......</code></pre><p>// order文件结构<br>order_id, user_id,  price,  date</p><pre><code>1,      1,      15.5    2018-01-012,      1,      3.9     2018-01-013,      1,      26.0    2018-01-034,      2,      2.2     2018-01-04......</code></pre></li></ol><p>需求是将<strong><em>用户数据文件（User）</em></strong>和<strong><em>订单数据文件（Order）</em></strong>通过<code>user_id</code>键相关联，得到总的用户信息+用户订单信息的总“表”（一个数据文件的内容可看作表的一部分）。</p><pre><code>// 期望得到的总表结构user_id, user_name, phone, order_id, price, date    1,      aa,     10086,      1,    15.5, 2018-01-01    ......  </code></pre><p>本文将基于以上需求，对Map-Reduce实现两表关联（join）进行探讨。</p><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><p>要利用Map-Reduce实现两表关联，关键点有以下几个：</p><ol><li><p>如何辨别当前处理的数据来自于<strong><em>User表</em></strong>还是<strong><em>Order表</em></strong> 因为两种文件有不同的数据格式，我们在Mapper里对数据逐行进行处理时，必须要知道当前这行数据来自于<strong><em>User表</em></strong>还是<strong><em>Order表</em></strong>， 不然根本没有办法编写进一步的处理逻辑。其实看过我之前文章的同学应该见过在Mapper里获取当前数据所属文件名的方法。简单来说，就是<strong><em>map方法</em></strong>的<code>Context</code>参数其实带有了很多本次Job的运行信息，其中就包括了当前数据来自于哪个<code>FileSplit</code>，进一步可以获得该<code>FileSplit</code>所属的文件名。</p><pre><code>FileSplit fileSplit = (FileSplit)context.getInputSplit();String fileName = fileSplit.getPath().getName();</code></pre></li></ol><ol start="2"><li><p>有两种不同的数据文件，可是一个MR任务里只有一种K-V对定义 两种文件格式，一种K-V对定义，那很自然我们就要想想如何将两种“合并”成一种，同时还能随时将这个合并的产物分开成两种。其实之前的文章已经多次提及一种在Map-Reduce里常用的概念——<strong>自定义JavaBean</strong>，作为一个自定义类，它的复合性质可以帮助我们“集成”很多原生数据类型。在当前问题里，我们可以定义一个<code>JavaBean</code>，<strong>集成User表与Order表的所有字段</strong>，并额外添加一个<code>flag</code>字段用以表明实例对象是<code>User</code>数据还是<code>Order</code>数据。通过这个<code>JavaBean</code>就可以实现“合并且随时可分离”的需求了。</p><pre><code>JavaBean = {user_id, order_id, user_name, phone, price, date}</code></pre></li></ol><ol start="3"><li><p><strong><em>“join”</em></strong>的逻辑具体怎么写 基于第2点，实现了一个集成的JavaBean后，我们可以在map中将数据封装成JavaBean实例对象，然后用flag字段指明数据类型。为了实现join功能，我们必须确保同一个<code>User</code>的所有数据都到达同一个Reducer处，那样才能将该用户的所有订单与其用户信息关联起来，避免遗漏。为了让同一个User的数据都到达同一个Reducer，我们要让Map端输出的K-V对为<code>&lt;UserID, JavaBean&gt;</code>，那么在Partition的时候，同一个UserID的数据自然会被分配到同一个Reducer。 当Reducer拿到一批<code>&lt;UserID, JavaBean&gt;</code>数据后，将其整合为<code>&lt;UserID, Iterable&lt;JavaBean&gt;&gt;</code>。我们可以在<code>Iterable&lt;JavaBean&gt;&gt;</code>里通过判断<code>flag</code>的值找出<code>User</code>的JavaBean，称为<strong>_U_</strong>。然后对于每个<code>Order</code>的JavaBean（称为<strong>_O_</strong>），从<strong>_O_</strong>中取出<code>order_id</code>, <code>price</code>, <code>date</code>，从<strong>_U_</strong>中取出<code>user_id</code>, <code>user_name</code>, <code>phone</code>，组成结果数据<strong>_R_</strong>。</p><pre><code>R = (order_id, price, date，user_id, user_name, phone)</code></pre></li></ol><pre><code>Reducer循环将多个`&lt;R, NullWritable&gt;`写到最终Context中，便完成了join逻辑。</code></pre><h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><ol><li><p>自定义JavaBean</p><pre><code>/** * 自定义JavaBean, 实现Writable接口, 因本任务不存在&quot;比较&quot;, 无需实现WritableComparable */public class JoinBean implements Writable{    public String userID;    public String uName;    public String phone;    public String orderID;    public String price;    public String date;    public String flag;     // 用于标识User还是Order    public JoinBean() {    }    public JoinBean(String userID, String uName, String phone, String orderID, String price, String date, String flag) {        this.userID = userID;        this.uName = uName;        this.phone = phone;        this.orderID = orderID;        this.price = price;        this.date = date;        this.flag = flag;    }    public void set(String userID, String uName, String phone, String orderID, String price, String date, String flag) {        this.userID = userID;        this.uName = uName;        this.phone = phone;        this.orderID = orderID;        this.price = price;        this.date = date;        this.flag = flag;    }    @Override    public String toString() {        return &quot;uName=&apos;&quot; + uName + &apos;\&apos;&apos; +                &quot;, phone=&apos;&quot; + phone + &apos;\&apos;&apos; +                &quot;, orderID=&apos;&quot; + orderID + &apos;\&apos;&apos; +                &quot;, price=&apos;&quot; + price + &apos;\&apos;&apos; +                &quot;, date=&apos;&quot; + date + &apos;\&apos;&apos;;    }    public String getUserID() {        return userID;    }    public void setUserID(String userID) {        this.userID = userID;    }    public String getFlag() {        return flag;    }    public void setFlag(String flag) {        this.flag = flag;    }    public String getuName() {        return uName;    }    public void setuName(String uName) {        this.uName = uName;    }    public String getPhone() {        return phone;    }    public void setPhone(String phone) {        this.phone = phone;    }    public String getOrderID() {        return orderID;    }    public void setOrderID(String orderID) {        this.orderID = orderID;    }    public String getPrice() {        return price;    }    public void setPrice(String price) {        this.price = price;    }    public String getDate() {        return date;    }    public void setDate(String date) {        this.date = date;    }    @Override    public void write(DataOutput dataOutput) throws IOException {        dataOutput.writeUTF(userID);        dataOutput.writeUTF(uName);        dataOutput.writeUTF(phone);        dataOutput.writeUTF(orderID);        dataOutput.writeUTF(price);        dataOutput.writeUTF(date);        dataOutput.writeUTF(flag);    }    @Override    public void readFields(DataInput dataInput) throws IOException      {        userID = dataInput.readUTF();        uName = dataInput.readUTF();        phone = dataInput.readUTF();        orderID = dataInput.readUTF();        price = dataInput.readUTF();        date = dataInput.readUTF();        flag = dataInput.readUTF();    }}</code></pre></li></ol><ol start="2"><li><p>Mapper</p><pre><code>/** * Mapper类 */public class JoinMapper extends Mapper&lt;LongWritable, Text, Text, JoinBean&gt; {    public static final String USER = &quot;user&quot;;    public static final String ORDER = &quot;order&quot;;    public JoinBean info = new JoinBean();    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        FileSplit fileSplit = (FileSplit)context.getInputSplit();        String fileName = fileSplit.getPath().getName();    // 获取数据所属文件名        String line = value.toString();        String[] data = line.split(&quot;,&quot;);        String userID = data[0];        // 通过flag标识实例对象是User还是Order        if(fileName.startsWith(&quot;user&quot;)) {            info.set(&quot;&quot;, data[1], data[2], &quot;&quot;, &quot;&quot;, &quot;&quot;, USER);        } else {            info.set(&quot;&quot;, &quot;&quot;, &quot;&quot;, data[1], data[2], data[3], ORDER);        }        context.write(new Text(userID), info);    }}</code></pre></li></ol><ol start="3"><li><p>Reducer</p><pre><code>/** * Reducer类 */public class JoinReducer extends Reducer&lt;Text, JoinBean, JoinBean, NullWritable&gt; {    @Override    protected void reduce(Text key, Iterable&lt;JoinBean&gt; values, Context context) throws IOException, InterruptedException {        JoinBean userInfo = new JoinBean();        List&lt;JoinBean&gt; orders = new ArrayList&lt;&gt;();        for(JoinBean info : values) {            // 同一个UserID，有且仅有1个User对象，其他均为该User的Order对象            if(info.getFlag().equals(&quot;user&quot;)) {                try {                    BeanUtils.copyProperties(userInfo, info);                } catch (Exception e) {                    System.out.println(e.toString());                }            } else if(info.getFlag().equals(&quot;order&quot;)){                JoinBean order = new JoinBean();                try {                    BeanUtils.copyProperties(order, info);                } catch (Exception e) {                    System.out.println(e.toString());                }                orders.add(order);            }        }        // 提取所需属性, 合并, 输出        for(JoinBean order : orders) {            JoinBean record = new JoinBean();            record.set(key.toString(), userInfo.getuName(), userInfo.getPhone(), order.getOrderID(), order.getPrice(), order.getDate(), order.getFlag());            context.write(record, NullWritable.get());        }    }}</code></pre></li></ol><ol start="4"><li><p>Driver（程序入口）</p><pre><code>public class JoinDriver {    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);        Job joinJob = Job.getInstance(conf);        joinJob.setMapperClass(JoinMapper.class);        joinJob.setReducerClass(JoinReducer.class);        joinJob.setJarByClass(JoinDriver.class);        joinJob.setMapOutputKeyClass(Text.class);        joinJob.setMapOutputValueClass(JoinBean.class);        joinJob.setOutputKeyClass(JoinBean.class);        joinJob.setOutputValueClass(NullWritable.class);        FileInputFormat.setInputPaths(joinJob, new Path(&quot;/join/input&quot;));        FileOutputFormat.setOutputPath(joinJob, new Path(&quot;/join/output&quot;));        System.exit(joinJob.waitForCompletion(true) ? 0 : 1);    }}</code></pre></li></ol><h2 id="极端情况——“数据倾斜”"><a href="#极端情况——“数据倾斜”" class="headerlink" title="极端情况——“数据倾斜”"></a>极端情况——“数据倾斜”</h2><p>在这个问题里，我们的数据分为<code>User</code>和<code>Order</code>两类，其中明显<code>User</code>的数据会比<code>Order</code>少得多。一位用户只可能有一行<code>User</code>记录（在系统不出错的情况下），而一位用户却可以有N行<code>Order</code>记录，因为他可以“疯狂购物”产生了成千上万条的订单数据。 极端情况下，有一位<code>User</code>：<code>user_1</code>对应了<code>1000000</code>条<code>Order</code>数据，另一位<code>User</code>：<code>user_2</code>对应了<code>2</code>条<code>Order</code>数据。那么必然有一个负责处理<code>user_1</code>的Reducer<code>reducer_1</code>的负载过高，因为要处理<code>1000000</code>条数据，其他负载极少的Reducer干完活之后就得白白浪费时间等着<code>reducer_1</code>完成任务（<strong><em>往往表现为Job进度卡在99%</em></strong>），才能汇报整个Job已完成。 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/dataIncline.png" alt=""> 如图所示，因为大量的数据涌向少数的节点，像是一个倾斜的天平，一端重一端轻，所以以上的情况便称为<code>数据倾斜</code>。 为了解决当前问题引起的数据倾斜，我们可以采用一种<strong>”map端join“</strong>的方式进行关联操作，代替之前在Reducer端收集数据进行join的逻辑。相比<code>Order</code>表，<code>User</code>表无疑小得多，甚至可能是几百倍的量级差距，所以完全可能把这个小表分发给各个Mapper，让每个Mapper都拥有一份<strong><em>完整的User表</em></strong>，进而将其加载入内存构造一个<code>&lt;user_id, user_info&gt;</code>的哈希表。这么一来，在Mapper内只需要处理<code>Order</code>的数据，然后根据<code>Order</code>里的<code>user_id</code>查到哈希表里该id对应的<code>user_info</code>，连接起来，即可完成join操作。</p><ul><li><p>MapJoinMapper</p><p>/**</p><ul><li><p>实现Map端join的Mapper<br>*/<br>public class MapJoinMapper extends Mapper&lt;LongWritable, Text, MapJoinBean, NullWritable&gt; {</p><p> private Map&lt;String, List<string>&gt; userTable = new HashMap&lt;&gt;();<br> private MapJoinBean outputKey = new MapJoinBean();</string></p><p> public static final String ORDER_FILE = “order.txt”;</p><p> /**</p><ul><li>Mapper会自行调用的一个初始化方法</li><li>@param context Job信息</li><li>@throws IOException</li><li><p>@throws InterruptedException<br>*/<br>@Override<br>protected void setup(Context context) throws IOException, InterruptedException {</p><p> // 获取系统”投放”到本地的cache文件, 本问题中只有1个，即user.txt<br> URI[] uris = context.getCacheFiles();<br> FileSystem fs = FileSystem.get(context.getConfiguration());<br> FSDataInputStream inputStream = fs.open(new Path(uris[0]));<br> BufferedReader reader = new BufferedReader(new InputStreamReader(inputStream));</p><p> // 逐行读取user.txt中的数据, 构建userTable<br> String line = null;<br> while((line = reader.readLine()) != null) {</p><pre><code>String[] terms = line.split(&quot;,&quot;);if(!userTable.containsKey(terms[0])) {    userTable.put(terms[0], new ArrayList&lt;&gt;());}userTable.get(terms[0]).add(terms[1]);userTable.get(terms[0]).add(terms[2]);</code></pre><p> }</p><p> reader.close();<br>}</p><p>@Override<br>protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {</p><p> FileSplit fileSplit = (FileSplit) context.getInputSplit();<br> String fileName = fileSplit.getPath().getName();</p><p> // 不需要再处理User数据文件了, 因为已经通过setup把其数据加载到内存中的userTable<br> if(fileName.equals(ORDER_FILE)) {</p><pre><code>String line = value.toString();String[] terms = line.split(&quot;,&quot;);// 从本地的&quot;小表&quot; —— userTable处获得userID对应的数据String userName = userTable.get(terms[0]).get(0);String phone = userTable.get(terms[0]).get(1);// 不再需要flag字段, 简单置为空串outputKey.set(terms[0], userName, phone, terms[1], terms[2], terms[3], &quot;&quot;);context.write(outputKey, NullWritable.get());</code></pre><p> }<br>}<br>}</p></li></ul></li></ul></li></ul><ul><li><p>MapJoinReducer</p><p>/**</p><ul><li><p>Map端Join的Reducer<br>*/<br>public class MapJoinReducer extends Reducer&lt;MapJoinBean, NullWritable, MapJoinBean, NullWritable&gt; {</p><p> @Override<br> protected void reduce(MapJoinBean key, Iterable<nullwritable> values, Context context) throws IOException, InterruptedException {</nullwritable></p><pre><code>context.write(key, NullWritable.get());</code></pre><p> }<br>}</p></li></ul></li></ul><ul><li><p>MapJoinBean</p><p>/**</p><ul><li><p>自定义JavaBean, 实现WritableComparable接口<br>*/<br>public class MapJoinBean implements WritableComparable<mapjoinbean> {<br> public String userID;<br> public String uName;<br> public String phone;<br> public String orderID;<br> public String price;<br> public String date;<br> public String flag;</mapjoinbean></p><p> public MapJoinBean() {</p><p> }</p><p> public MapJoinBean(String userID, String uName, String phone, String orderID, String price, String date, String flag) {</p><pre><code>this.userID = userID;this.uName = uName;this.phone = phone;this.orderID = orderID;this.price = price;this.date = date;this.flag = flag;</code></pre><p> }</p><p> public void set(String userID, String uName, String phone, String orderID, String price, String date, String flag) {</p><pre><code>this.userID = userID;this.uName = uName;this.phone = phone;this.orderID = orderID;this.price = price;this.date = date;this.flag = flag;</code></pre><p> }</p><p> @Override<br> public String toString() {</p><pre><code>return &quot;uName=&apos;&quot; + uName + &apos;\&apos;&apos; +        &quot;, phone=&apos;&quot; + phone + &apos;\&apos;&apos; +        &quot;, orderID=&apos;&quot; + orderID + &apos;\&apos;&apos; +        &quot;, price=&apos;&quot; + price + &apos;\&apos;&apos; +        &quot;, date=&apos;&quot; + date + &apos;\&apos;&apos;;</code></pre><p> }</p><p> public String getUserID() {</p><pre><code>return userID;</code></pre><p> }</p><p> public void setUserID(String userID) {</p><pre><code>this.userID = userID;</code></pre><p> }</p><p> public String getFlag() {</p><pre><code>return flag;</code></pre><p> }</p><p> public void setFlag(String flag) {</p><pre><code>this.flag = flag;</code></pre><p> }</p><p> public String getuName() {</p><pre><code>return uName;</code></pre><p> }</p><p> public void setuName(String uName) {</p><pre><code>this.uName = uName;</code></pre><p> }</p><p> public String getPhone() {</p><pre><code>return phone;</code></pre><p> }</p><p> public void setPhone(String phone) {</p><pre><code>this.phone = phone;</code></pre><p> }</p><p> public String getOrderID() {</p><pre><code>return orderID;</code></pre><p> }</p><p> public void setOrderID(String orderID) {</p><pre><code>this.orderID = orderID;</code></pre><p> }</p><p> public String getPrice() {</p><pre><code>return price;</code></pre><p> }</p><p> public void setPrice(String price) {</p><pre><code>this.price = price;</code></pre><p> }</p><p> public String getDate() {</p><pre><code>return date;</code></pre><p> }</p><p> public void setDate(String date) {</p><pre><code>this.date = date;</code></pre><p> }</p><p> @Override<br> public void write(DataOutput dataOutput) throws IOException {</p><pre><code>dataOutput.writeUTF(userID);dataOutput.writeUTF(uName);dataOutput.writeUTF(phone);dataOutput.writeUTF(orderID);dataOutput.writeUTF(price);dataOutput.writeUTF(date);dataOutput.writeUTF(flag);</code></pre><p> }</p><p> @Override<br> public void readFields(DataInput dataInput) throws IOException {</p><pre><code>userID = dataInput.readUTF();uName = dataInput.readUTF();phone = dataInput.readUTF();orderID = dataInput.readUTF();price = dataInput.readUTF();date = dataInput.readUTF();flag = dataInput.readUTF();</code></pre><p> }</p><p> @Override<br> public int compareTo(MapJoinBean o) {</p><pre><code>if(o == null) {    throw new RuntimeException();}if(userID.compareTo(o.getUserID()) == 0) {    return orderID.compareTo(o.getOrderID());}return userID.compareTo(o.getUserID());</code></pre><p> }<br>}</p></li></ul></li></ul><ul><li><p>MapJoinDriver</p><p>public class MapJoinDriver {</p><pre><code>public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException, URISyntaxException {    Configuration conf = new Configuration();    conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);    Job joinJob = Job.getInstance(conf);    // 使用Map端join的Mapper和Reducer    joinJob.setMapperClass(MapJoinMapper.class);    joinJob.setReducerClass(MapJoinReducer.class);    joinJob.setJarByClass(MapJoinDriver.class);    joinJob.setMapOutputKeyClass(MapJoinBean.class);    joinJob.setMapOutputValueClass(NullWritable.class);    joinJob.setOutputKeyClass(MapJoinBean.class);    joinJob.setOutputValueClass(NullWritable.class);    // 向各节点&quot;投放&quot;User数据文件    joinJob.addCacheFile(new URI(&quot;/join/input/user.txt&quot;));    FileInputFormat.setInputPaths(joinJob, new Path(&quot;/join/input&quot;));    FileOutputFormat.setOutputPath(joinJob, new Path(&quot;/join/output&quot;));    System.exit(joinJob.waitForCompletion(true) ? 0 : 1);}</code></pre><p>}</p></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>利用Map-Reduce可以完成数据文件的join操作。不需要先将数据结构化，导入MySQL。</li><li>自定义<code>JavaBean</code>类，用于集成多个属性的思路仍然很有用。</li><li>当join的双方是一张<strong>大表</strong>和一张<strong>小表</strong>时，可考虑将小表分发到所有map端，在本地将小表加载到内存，从而实现<strong>map端join</strong>。</li></ol><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>具体代码请点击 <a href="https://github.com/AcepcsMa/hadoop_examples/tree/master/src/join" target="_blank" rel="noopener">Join操作与Map端join操作代码</a></p>]]></content>
      
      
      <categories>
          
          <category> BigData </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> mapreduce </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>谈谈倒排索引，升级版“WordCount”</title>
      <link href="/2018/02/25/ii-wc/"/>
      <url>/2018/02/25/ii-wc/</url>
      
        <content type="html"><![CDATA[<blockquote><p>全文为本人（Haoxiang Ma）原创内容，转载请标明出处。</p></blockquote><h2 id="问题背景：All-About-Search"><a href="#问题背景：All-About-Search" class="headerlink" title="问题背景：All About Search"></a>问题背景：All About Search</h2><p>在数据库领域和搜索引擎领域，<strong><em>倒排索引</em></strong>是一种很重要的数据结构。在大部分的应用场景中，文本型数据（Text）是主流，依靠<strong><em>倒排索引</em></strong>这种数据结构，可以显著提高<strong><em>文本数据项（Term）</em></strong>的搜索速度（无论是理论还是实际应用中）。 假设现在我们拥有以下数据。</p><pre><code>// Document AMy name is superman.// Document BI love my cat whose name is Kitty!// Document CPlease name my car.</code></pre><p>如果不对以上原始数据作额外处理，径直将3个文件保存至磁盘的同一目录下，分别命名为<code>Document A</code>、<code>Document B</code>、<code>Document C</code>。那么当需要查找<code>car</code>这个数据项时，我们需要遍历文件目录下的所有文件，才能输出搜索结果：<code>Found in Document C!</code>。若将此场景扩展至<strong>_N_</strong>个数据文件，平均每个数据文件的Term Count为<strong>_K_</strong>，那么此搜索算法的时间复杂度将为<code>O(N*K)</code>，随着N和K的增长，这个时间复杂度无疑是灾难性的。 也许有人会说，既然每个文件是独立的，那么我们可以将目录下的数据文件进行<strong><em>“Partition”</em></strong>，分成<code>M</code>块，每一块包含<code>N/M</code>个文件，同时启动<code>M</code>个线程独立负责搜索自己块内的文件，不就能够大幅提高速度，解决搜索的性能问题了吗？先撇开硬盘的I/O性能不谈，<strong>启动、协同<code>M</code>个线程，合并多个线程的计算结果所需要的系统开销将会是一个天文数字</strong>，很有可能就因为这个搜索任务导致整个节点崩溃。再者，当系统的用户数快速增长，同时执行多个用户的搜索请求时根本无法保证搜索的及时性，难以并发。 所以这个时候我们就要转变思路，<strong>不要傻傻地把真正的“搜索”放在用户发送搜索请求时执行</strong>，而应该早早地为数据文件里的每个<strong><em>数据项（Term）</em></strong>建立起<strong><em>索引（Term Index）</em></strong>，到用户需要搜索时就可以通过已建立好的索引快速定位并返回结果，不需要一次又一次地扫描磁盘文件。 <strong><em>倒排索引（Inverted Index)</em></strong>这种数据结构就是基于以上需求而来到了这个世界上的。</p><pre><code>// 正常情况下，我们第一反应下的索引应该是以下结构DocumentID          Terms    A         [My, name, is, superman]    B         [I, love, my, cat, whose, name, is, Kitty]    C         [Please, name, my, car]</code></pre><p>如果我们按照以上的结构建立索引，仍需要逐个ID扫描其对应的Terms，搜索的时间复杂度仍然是<code>O(N*K)</code>，压根没有提升任何性能。</p><pre><code>// 所以我们需要通过“倒排”的方式改变索引结构Term        DocumentIDsMy          [A, B, C]name        [A, B, C]is          [A, B]superman    [A]I           [B]love        [B]cat         [B]whose       [B]Kitty       [B]Please      [C]car         [C]</code></pre><p>所谓“倒排”，无非就是将原来的<strong><em>Key</em></strong>（<code>DocuementID</code>）和<strong><em>Value</em></strong>（<code>Terms</code>）颠倒过来，用<code>Term</code>作为<strong><em>Key</em></strong>，<code>DocumentIDs</code>作为<strong><em>Value</em></strong>。通过构建这样的索引结构，当我们需要搜索<code>car</code>这一数据项时，我们只需从头开始线性扫描<strong>一遍</strong>索引表，定位到<code>car</code>那一行并直接取出其对应的<code>DocumentIDs</code>，单次搜索的时间复杂度降低到了<code>O(N)</code>。 当然在数据量特别大时，<code>O(N)</code>仍然不是一个理想的指标，仍然有进步的空间。我们可以对每个<code>Term</code>进行Hash得到<code>h = Hash(Term)</code>，然后记录<code>h</code>与行号的映射表<code>H_TABLE</code>。那么每次搜索时根据搜索项的Hash可以查<code>H_TABLE</code>快速定位到具体的某一行，直接就可取出其对应的<code>DocumentIDs</code>，总的时间复杂度理论上是<code>O(1)</code>。<strong>（关于Hash冲突与优化的问题本文暂时不予探讨）</strong></p><h2 id="实现方案：WordCount-Again？"><a href="#实现方案：WordCount-Again？" class="headerlink" title="实现方案：WordCount Again？"></a>实现方案：WordCount Again？</h2><p>当系统中有海量的数据文件时，第一反应肯定就是使用Hadoop以及Hadoop生态中的工具帮助我们处理数据。那么我们是不是可以用Map-Reduce来构建倒排索引表呢？ <strong>答案是肯定的，确实可以使用Map-Reduce。而且仔细一想，这不就是Hadoop中的“HelloWorld”——WordCount的翻版吗？？？</strong>确实也可以这么说，处理逻辑跟WordCount非常类似，只是我们需要在Reducer中稍微多做一点点工作，所以我称之为<strong><em>升级版WordCount</em></strong>哈哈😄😄😄。</p><pre><code>// 如果100%照搬WordCount的逻辑，那么最终产出的结果文件会是My      AMy      BMy      Cname    Aname    Bname    C......</code></pre><p>显然我们想要的倒排表格式是需要把同一个<code>Term</code>下的所有<code>DocumentID</code>合并到一行里。所以为了数据格式的正确性，我们需要对输出做点小处理。于是我们要设置一个自定义的<code>DocumentBean</code>类，逻辑上可以简单看作<code>DocumentBean = List&lt;DocumentID&gt;</code>。<strong>最终我们需要通过Reducer输出<code>&lt;Term, DocumentBean&gt;</code></strong>，那么结果里的每一行自然就是我们想要的格式了。</p><pre><code>/** * 集成多个DocumentID的Bean */public class DocumentBean implements Writable {    private int documentCount;    private List&lt;String&gt; documentIDs;    public DocumentBean() {        documentCount = 0;        documentIDs = new ArrayList&lt;&gt;();    }    public void set(Iterable&lt;Text&gt; documentIDs) {        this.documentIDs.clear();        for(Text documentID : documentIDs) {            this.documentIDs.add(documentID.toString());        }        documentCount = this.documentIDs.size();    }    @Override    public void write(DataOutput dataOutput) throws IOException {        dataOutput.writeInt(documentCount);        for(String documentID : documentIDs) {            dataOutput.writeUTF(documentID);        }    }    @Override    public void readFields(DataInput dataInput) throws IOException {        documentCount = dataInput.readInt();        for(int i = 0;i &lt; documentCount;i++) {            documentIDs.add(dataInput.readUTF());        }    }    @Override    public String toString() {        StringBuilder sb = new StringBuilder();        for(String documentID : documentIDs) {            sb.append(documentID).append(&quot;, &quot;);        }        return sb.substring(0, sb.length() - 2);    }}/** * 倒排索引的Mapper */public class InvertedMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; {    private Text outputKey = new Text();    private Text outputValue = new Text();    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        FileSplit split = (FileSplit) context.getInputSplit();        String fileName = split.getPath().getName();        outputValue.set(fileName);        String line = value.toString();        String[] terms = line.split(&quot; &quot;);        for(String term : terms) {            outputKey.set(term);            context.write(outputKey, outputValue);        }    }}/** * 倒排索引的Reducer */public class InvertedReducer extends Reducer&lt;Text, Text, Text, DocumentBean&gt; {    private DocumentBean outputValue = new DocumentBean();    @Override    protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException {        outputValue.set(values);        context.write(key, outputValue);    }}/** * 程序入口 */public class InvertedDriver {    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);        Job invertedJob = Job.getInstance(conf);        invertedJob.setJarByClass(InvertedDriver.class);        invertedJob.setMapperClass(InvertedMapper.class);        invertedJob.setReducerClass(InvertedReducer.class);        invertedJob.setMapOutputKeyClass(Text.class);        invertedJob.setMapOutputValueClass(Text.class);        invertedJob.setOutputKeyClass(Text.class);        invertedJob.setOutputValueClass(DocumentBean.class);        FileInputFormat.setInputPaths(invertedJob, new Path(&quot;/inverted_index/data/&quot;));        FileOutputFormat.setOutputPath(invertedJob, new Path(&quot;/inverted_index/output/&quot;));        System.exit(invertedJob.waitForCompletion(true) ? 1 : 0);    }}</code></pre><h2 id="总结：What-Should-Be-Remembered"><a href="#总结：What-Should-Be-Remembered" class="headerlink" title="总结：What Should Be Remembered"></a>总结：What Should Be Remembered</h2><ol><li>利用自定义的<strong><code>Bean</code></strong>类来辅佐Map-Reduce，实现各种复杂功能的思路已经很普遍了，例如自定义输出输出格式／实现两表join／二次排序等等等等。</li></ol><p><img src="http://p0u4yewt0.bkt.clouddn.com/invertedIndex.png" alt=""> 2. <strong>倒排索引表的设计与优化其实很复杂，本文谈到的内容只是管中窥豹</strong>。如上图所示，在倒排索引表中甚至可以存储每个Term在不同文件中的出现次数／文件更新（插入）时间／Term的TF-IDF值等等等等。通过存储这些文本数据可以帮助搭建高效的推荐系统，或者对搜索排序进行优化。</p>]]></content>
      
      
      <categories>
          
          <category> BigData </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
            <tag> bigdata </tag>
            
            <tag> hadoop </tag>
            
            <tag> mapreduce </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>海量小文件优化之自定义InputFormat</title>
      <link href="/2018/02/17/customizeinputformat/"/>
      <url>/2018/02/17/customizeinputformat/</url>
      
        <content type="html"><![CDATA[<blockquote><p>全文为本人（Haoxiang Ma）原创内容，转载请标明出处。</p></blockquote><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>HDFS作为分布式文件存储系统，极其适用于海量大文件的存储场景。每个大文件在系统底层会被切分成多个<strong><em>Block</em></strong>（Block Size默认为128MB），且每个Block会自动被冗余处理（默认备份数为3份）以保证一定程度上的数据安全。 HDFS对大文件友好，但是世事往往不尽如人意。“存储”对于整个业务系统而言只是其中一环，在存储之前必须先有数据收集的流程。假如前端进行数据收集时raw data为海量的小数据文件（从1KB到10MB不等），且没有经过合并就直接通过HDFS的上传API写到HDFS中，那么NameNode上就会保存大量的Block元数据记录（<strong>即使单个小文件的大小远远不到一个Block的容量，但在逻辑上也会被切分为一个Block，虽然物理上并没有占用一个真正的Block的物理容量</strong>）。 相比NameNode中存储大量Block元数据带来的影响，更值得关注的是<strong>海量小文件输入对Map-Reduce任务的影响</strong>。 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/normalMap.png" alt="理想情况"> 理想情况下，假设Map-Reduce任务的文件输入是/xx/xx/xx.dat，为了方便，我们假设xx.dat文件的大小刚好是3个Block的大小（3 * 128MB），暂且忽略备份情况。如图所示，在以该文件作为输入启动Map-Reduce任务时，系统getSplit()自然而然地将该文件分成3个Split，定位到各Split所在的DataNode并在上面启动Map任务。3个Map任务各自处理Local Data（Hadoop提倡的Data Locality），然后Map端输出会被相应的Reducer拉取进行Reduce操作。在以上理想情况中，各个Map任务负载基本均衡，每个Map处理的都是本地128MB的数据，系统运行状态良好。 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/abnormalMap.png" alt="海量小文件"> 然而，如果Map端输入为海量的小文件，那么默认每个小文件本身就会被当作一个Split（小到不可再划分了）。如上图所示，假如系统中小文件的分布比较均匀，在每个存放有小文件的DataNode上都会启动一个Map任务。但是假如小文件的数目远远大于系统中DataNode的个数（100万个小文件，100个DataNode），也就是说一个DataNode上可能存放有N个小文件时，那么DataNode就要不断启动一个又一个的Map任务直到N个小文件都被处理完。更极端的情况是<strong>小文件分布不均匀，在某个特定的DataNode上存放了80%的小文件，剩余DataNode上只存放了20%的小文件</strong>，那么其他DataNode在结束自己本地的计算后，还得“默默等待”那个存放了80%小文件的DataNode完成它的所有map工作，严重影响了整个Map-Reduce任务的效率。 所以对于海量小文件数据，我们在启动Map-Reduce任务前必须对其进行优化，常见的优化思路有</p><ol><li>在系统前沿就把多个小文件合并，将合并后的大文件写入HDFS</li><li>小文件已大量分布在HDFS中，通过一定手段在HDFS中合并它们</li><li>在启动正式的Map-Reduce任务前，先预处理合并大量小文件</li></ol><p>其中<strong><em>思路1</em></strong>的逻辑无关HDFS，不在本文讨论范围。<strong><em>思路2与思路3本质上一样</em></strong>，是通过一定的手段让小文件合并为大文件，以大文件作为正式的Map-Reduce任务的数据输入。</p><h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><p><img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/TextInputFormat.png" alt="Hadoop源码"> 默认情况下，Map-Reduce使用的默认输入格式（InputFormat）是<strong><em>TextInputFormat</em></strong>，通过查看源码可知<strong><em>TextInputFormat</em></strong>实现的泛型是<strong><em>&lt;LongWritable, Text&gt;</em></strong>，这也就解释了为什么默认情况下我们Map任务的输入是<strong><em>&lt;LongWritable, Text&gt;</em></strong>。 调用getRecordReader方法会返回一个真正的<strong><em>RecordReader&lt;LongWritable, Text&gt;</em></strong>对象（实现了具体读文件的逻辑）供外部使用。每次我们把Map-Reduce的作业提交之后，系统会根据设定的InputFormat（默认/自定义）建立一个RecordReader对象来读取Split，并按照一定格式对Split进行预处理。如图所示，默认的RecordReader为<strong><em>LineRecordReader</em></strong>，也就是按行读取Split中的数据内容，每读取一行都会生成一个<strong><em>&lt;LongWritable, Text&gt;</em></strong>（行起始偏移量, 文本内容)的K-V对。利用RecordReader不断地读取，就可以完成map端的输入。 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/mergeProcess.png" alt="合并过程示意图"> 基于以上原理，实现小文件合并为大文件的逻辑其实很简单。我们完全可以自定义InputFormat，对每个小文件不再逐行读取，而是将整个小文件的内容全部读取以生成<strong><em>&lt;小文件名, 文件全部内容&gt;</em></strong>的K-V对，然后将N个小文件的N个K-V对通过<strong><em>1个Reducer</em></strong>最终合并成一个大文件(<strong>本例使用SequenceFile</strong>)。 <strong>所以，关键就在于实现自定义的InputFormat类与自定义的RecordReader类。</strong></p><h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><h4 id="简要原理"><a href="#简要原理" class="headerlink" title="简要原理"></a>简要原理</h4><p><img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/relations.png" alt="合并过程示意图"> 网上的各种自定义InputFormat教程中总是先贴一大段代码，让人很难一下子搞清楚InputFormat、RecordReader和Map-Reduce任务的关系。简单来说，InputFormat和RecordReader的功能如下</p><ul><li><strong>自定义的InputFormat为本次Map-Reduce任务提供了一个自定义的RecordReader</strong></li><li><strong>自定义的RecordReader实现具体的如何从Split中读取数据的逻辑</strong></li></ul><h4 id="Java代码"><a href="#Java代码" class="headerlink" title="Java代码"></a>Java代码</h4><p>首先实现自定义的RecordReader类。</p><pre><code>/** * 自定义的RecordReader, 用于将Split中的内容转换为自定义的K-V对形式 */public class MyRecordReader extends RecordReader&lt;Text, BytesWritable&gt; {    private FileSplit split;        // 读入的文件split    private boolean isFinished;     // 转换是否完成的标志位    private Text key;               // 输出Key    private BytesWritable value;    // 输出Value    private JobContext context;     // 作业内容    /**     * 初始化RecordReader方法     * @param inputSplit 读入的文件split     * @param taskAttemptContext 作业内容     * @throws IOException     * @throws InterruptedException     */    @Override    public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {        this.isFinished = false;        this.key = new Text();        this.value = new BytesWritable();        this.split = (FileSplit) inputSplit;        this.context = taskAttemptContext;    }    /**     * 告诉调用者是否还有未读的下一个K-V对     * @return true / false     * @throws IOException     * @throws InterruptedException     */    @Override    public boolean nextKeyValue() throws IOException, InterruptedException {        if(!isFinished) {            String fileName = this.split.getPath().getName();            this.key.set(fileName);            int contentLength = (int)split.getLength();            byte[] content = new byte[contentLength];            FileSystem fs = FileSystem.get(context.getConfiguration());            FSDataInputStream inputStream = null;            try {                inputStream = fs.open(this.split.getPath());                IOUtils.readFully(inputStream, content, 0, contentLength);                value.set(content, 0, contentLength);            } catch (Exception e) {                System.out.println(e.toString());            } finally {                if(inputStream != null) {                    try {                        IOUtils.closeStream(inputStream);                    } catch (Exception e) {                        System.out.println(e.toString());                    }                }            }            isFinished = true;            return true;        }        return false;    }    /**     * 返回当前读到的Key     * @return key     * @throws IOException     * @throws InterruptedException     */    @Override    public Text getCurrentKey() throws IOException, InterruptedException {        return this.key;    }    /**     * 返回当前读到的Value     * @return     * @throws IOException     * @throws InterruptedException     */    @Override    public BytesWritable getCurrentValue() throws IOException, InterruptedException {        return this.value;    }    /**     * 返回转换过程的状态 (是否转换完成)     * @return true / false     * @throws IOException     * @throws InterruptedException     */    @Override    public float getProgress() throws IOException, InterruptedException {        return this.isFinished ? 1.0f : 0.0f;    }    @Override    public void close() throws IOException {    }}</code></pre><p>然后实现自定义的InputFormat类。</p><pre><code>/** * 自定义InputFormat, 创建一个自定义的RecordReader(以实现自定义的读取输入文件逻辑) */public class MyInputFormat extends FileInputFormat&lt;Text, BytesWritable&gt;{    @Override    public RecordReader&lt;Text, BytesWritable&gt; createRecordReader(InputSplit inputSplit,                                                                TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {        // 初始化一个自定义的RecordReader并返回        RecordReader&lt;Text, BytesWritable&gt; recordReader = new MyRecordReader();        recordReader.initialize(inputSplit, taskAttemptContext);        return recordReader;    }}</code></pre><p>实现了自定义的<strong><em>MyInputFormat</em></strong>类和<strong><em>MyRecordReader</em></strong>类后，Mapper和Reducer并没有什么可做的，map端只是简单地将数据输入直接输出，reduce端收集到K-V对后将其直接输出。</p><pre><code>/** * Mapper, 负责处理从Split中读取的K-V型数据 */public class MyMapper extends Mapper&lt;Text, BytesWritable, Text, BytesWritable&gt; {    @Override    protected void map(Text key, BytesWritable value, Context context) throws IOException, InterruptedException {        context.write(key, value);    }}/** * Reducer, 负责接收K-V对后输出为结果文件 */public class MyReducer extends Reducer&lt;Text, BytesWritable, Text, BytesWritable&gt; {    @Override    protected void reduce(Text key, Iterable&lt;BytesWritable&gt; values, Context context) throws IOException, InterruptedException {        for(BytesWritable value : values) {            context.write(key, value);        }    }}</code></pre><p>最后把Job信息设置好后就可以提交到集群上运行。</p><pre><code>/** * 程序入口 */public class Driver {    private static final String HDFS_HOST = &quot;hdfs://localhost:9000&quot;;    private static final int NUM_REDUCE_TASKS = 1;    private static final String FILE_INPUT_PATH = &quot;/small_files/data&quot;;    private static final String RESULT_OUTPUT_PATH = &quot;/small_files/output&quot;;    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;, HDFS_HOST);        Job myJob = Job.getInstance(conf);        // 通过类名设置运行的jar包        myJob.setJarByClass(Driver.class);        // 设置Mapper Class与Reducer Class        myJob.setMapperClass(MyMapper.class);        myJob.setReducerClass(MyReducer.class);        // 设置Mapper与Reducer的输出数据类型        myJob.setMapOutputKeyClass(Text.class);        myJob.setMapOutputValueClass(BytesWritable.class);        myJob.setOutputKeyClass(Text.class);        myJob.setOutputValueClass(BytesWritable.class);        // 设置InputFormat与OutputFormat        // 使用自定义的InputFormat作为Input格式, Sequence文件作为Output格式        myJob.setInputFormatClass(MyInputFormat.class);        myJob.setOutputFormatClass(SequenceFileOutputFormat.class);        // 自定义Reduce Task数量, 决定最终输出多少个结果文件        myJob.setNumReduceTasks(NUM_REDUCE_TASKS);        // 设置数据所在目录, 结果输出目录        FileInputFormat.setInputPaths(myJob, new Path(FILE_INPUT_PATH));        FileOutputFormat.setOutputPath(myJob, new Path(RESULT_OUTPUT_PATH));        System.exit(myJob.waitForCompletion(true) ? 1 : 0);    }}</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>当HDFS中存储了海量小文件时，利用自定义的InputFormat和RecordReader启动Map-Reduce任务可将小文件合并为大文件。这个合并的Job可以放在正式的Map-Reduce任务前，利用ControlledJob对合并Job与正式Job进行控制，合并结束后才启动正式Job；也可以定时地对HDFS上的文件进行合并，避免需要用时才进行合并。 <strong>当然，最优方案还是在系统前沿就把数据合并成尽量大的文件，再把大文件写入HDFS。</strong></p><h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><p>完整代码请查看： <a href="https://github.com/AcepcsMa/hadoop_examples/tree/master/src/inputformat" target="_blank" rel="noopener">我的Github目录</a></p>]]></content>
      
      
      <categories>
          
          <category> BigData </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bigdata </tag>
            
            <tag> hadoop </tag>
            
            <tag> mapreduce </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>OS X下MapReduce程序运行的几种模式</title>
      <link href="/2018/02/13/osxmr/"/>
      <url>/2018/02/13/osxmr/</url>
      
        <content type="html"><![CDATA[<blockquote><p>全文为本人（Haoxiang Ma）原创内容，转载请标明出处。</p></blockquote><h1 id="1-MapReduce程序运行的模式简介"><a href="#1-MapReduce程序运行的模式简介" class="headerlink" title="1.MapReduce程序运行的模式简介"></a>1.MapReduce程序运行的模式简介</h1><ol><li>程序运行模式<ul><li>本地模式<ul><li>利用本地的JVM运行，使用本地的IDE进行debug</li></ul></li><li>远程模式<ul><li>提交至远程的集群上运行，使用本地的IDE进行debug</li><li>提交至远程的集群上运行，不使用本地IDE进行debug</li></ul></li></ul></li><li>数据存放路径<ul><li>远程文件系统（hdfs)</li><li>本地文件系统（local file system)</li></ul></li></ol><h1 id="2-开发环境简介"><a href="#2-开发环境简介" class="headerlink" title="2.开发环境简介"></a>2.开发环境简介</h1><ul><li>操作系统：macOS Sierra 10.12.6</li><li>Java版本：1.8.0_131-b11</li><li>Hadoop版本：hadoop-2.7.4</li><li>IDE：IntelliJ IDEA</li></ul><h1 id="3-MapReduce程序运行例子"><a href="#3-MapReduce程序运行例子" class="headerlink" title="3.MapReduce程序运行例子"></a>3.MapReduce程序运行例子</h1><h2 id="3-1-程序需求"><a href="#3-1-程序需求" class="headerlink" title="3.1 程序需求"></a>3.1 程序需求</h2><blockquote><p>学校里开设了多门课程，有语文（chinese）、数学（math）、英语（english）等。经过了一次年级统考后，每个学生的成绩都被记录在多个文本文件中，文本文件格式如下。</p></blockquote><ul><li><p>math.txt</p><p>Ben 75<br>Jack 60<br>May 85<br>Tom 91</p></li></ul><ul><li><p>english.txt</p><p>Jack 72<br>May 60<br>Tom 62<br>Ben 90</p></li></ul><ul><li><p>chinese.txt</p><p>Ben 79<br>May 88<br>Tom 68<br>Jack 70</p></li></ul><blockquote><p>现需要根据以上的文本文件，算出每个学生在本次统考中的平均分，并将结果用一个总的文件averageScore.txt进行存储。averageScore.txt的格式如下。</p></blockquote><ul><li><p>averageScore.txt</p><p>#name #score<br>Ben 0.0<br>May 0.0<br>Tom 0.0<br>Jack 0.0</p></li></ul><h2 id="3-2-程序设计思路"><a href="#3-2-程序设计思路" class="headerlink" title="3.2 程序设计思路"></a>3.2 程序设计思路</h2><h3 id="3-2-1-Mapper的处理逻辑"><a href="#3-2-1-Mapper的处理逻辑" class="headerlink" title="3.2.1 Mapper的处理逻辑"></a>3.2.1 Mapper的处理逻辑</h3><p>Mapper每次从文本文件中读取<strong>1行内容</strong>，即调用1次map方法。Mapper需要把原始数据中一行的内容拆分成学生姓名（student name）和该门课程的分数（score）。按照需求，本程序最终要算出每一个学生的平均分，所以学生姓名应作为一个key，对应的value即为该生的平均分<strong><em>（实际上是不严谨的，因为在实际环境中会出现多个学生重名的现象，若不作特殊处理，key是不允许重复的。最根本的解决方案是采用学号作为key，但为了演示直观，仅采用学生姓名作为key）</em></strong>。 Mapper读完一行的数据后，把<code>{student name，score}</code>这个<code>key-value</code>写入中间结果，准备传送给Reducer作下一步的运算。</p><h3 id="3-2-2-Reducer的处理逻辑"><a href="#3-2-2-Reducer的处理逻辑" class="headerlink" title="3.2.2 Reducer的处理逻辑"></a>3.2.2 Reducer的处理逻辑</h3><p>Reducer接收到的数据，实际上是一个key与该key对应的value的一个<strong>集合</strong>（并不仅仅是一个value）。在本需求中，传入reduce方法的参数是学生姓名，以及该生多门课程分数的集合，类似于<code>Ben,[60,70,80,...]</code>。所以Reducer需要将集合中的分数求和，然后求出平均值，最终得到一个<code>{student name, average score}</code>的<code>key-value</code>对。</p><h3 id="3-2-3-具体代码设计"><a href="#3-2-3-具体代码设计" class="headerlink" title="3.2.3 具体代码设计"></a>3.2.3 具体代码设计</h3><ul><li><p>AVGMapper类<br>用于实现map方法</p><p>package mr;</p><p>import org.apache.hadoop.io.DoubleWritable;<br>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;<br>import java.io.IOException;</p><p>/**</p><ul><li>Created by marco on 2017/8/17.<br>*/<br>public class AVGMapper extends Mapper&lt;LongWritable, Text, Text, DoubleWritable&gt;<br>{<br> @Override<br> protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException<br> {<pre><code>String line = value.toString();if(line.length() == 0)  // 文件格式错误，出现空行    return;String[] split = line.split(&quot; &quot;);String stuName = split[0];String stuScore = split[1];double score = Double.parseDouble(stuScore);    // 转成double类型，方便后续求均值计算context.write(new Text(stuName), new DoubleWritable(score));</code></pre> }<br>}</li></ul></li></ul><ul><li><p>AVGReducer类<br>用于实现reduce方法</p><p>package mr;</p><p>import org.apache.hadoop.io.DoubleWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Reducer;<br>import java.io.IOException;</p><p>/**</p><ul><li><p>Created by marco on 2017/8/17.<br>*/<br>public class AVGReducer extends Reducer&lt;Text, DoubleWritable, Text, DoubleWritable&gt;<br>{<br> @Override<br> protected void reduce(Text key, Iterable<doublewritable> values, Context context) throws IOException, InterruptedException<br> {</doublewritable></p><pre><code>double sum = 0;int length = 0;for(DoubleWritable value : values){    sum += value.get();    length++;}double avgScore = sum / (double)length;context.write(key, new DoubleWritable(avgScore));</code></pre><p> }<br>}</p></li></ul></li></ul><ul><li><p>AVGRunner类<br>用于关联Mapper与Reducer，并创建MapReduce任务（Job）提交运行。基本代码如下所示。</p><p>package mr;</p><p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.FileSystem;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.DoubleWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p><p>/**</p><ul><li><p>Created by marco on 2017/8/17.<br>*/<br>public class AVGRunner<br>{<br> static public void main(String[] args) throws Exception<br> {</p><pre><code>// 设置hdfs的handlerConfiguration fsConf = new Configuration();fsConf.set(&quot;fs.default.name&quot;,&quot;hdfs://localhost:9000/&quot;);FileSystem fs = FileSystem.get(fsConf);// MapReduce的配置参数Configuration mrConf = new Configuration();// 新建一个求平均值的JobJob avgJob = Job.getInstance(mrConf);avgJob.setJarByClass(AVGRunner.class);// 设置Mapper类与Reducer类avgJob.setMapperClass(AVGMapper.class);avgJob.setReducerClass(AVGReducer.class);// 设置输入输出的数据结构avgJob.setMapOutputKeyClass(Text.class);avgJob.setMapOutputValueClass(DoubleWritable.class);avgJob.setOutputKeyClass(Text.class);avgJob.setOutputValueClass(DoubleWritable.class);// 检查结果输出目录，若已存在则删除输出目录if(fs.exists(new Path(&quot;/avg/output&quot;))){    fs.delete(new Path(&quot;/avg/output&quot;), true);}// 设置数据目录以及结果输出目录FileInputFormat.setInputPaths(avgJob, new Path(&quot;&quot;));FileOutputFormat.setOutputPath(avgJob, new Path(&quot;&quot;));// 提交任务，等待完成System.exit(avgJob.waitForCompletion(true)?0:1);</code></pre><p> }<br>}</p></li></ul></li></ul><h2 id="3-3-MapReduce程序运行"><a href="#3-3-MapReduce程序运行" class="headerlink" title="3.3 MapReduce程序运行"></a>3.3 MapReduce程序运行</h2><blockquote><p>若使用本地文件系统的数据文件，且在本地模式运行，无需配置hdfs相关的参数，数据目录以及结果输出目录填写本地路径即可。<strong>（确保结果输出文件夹未被创建，否则会报异常）</strong></p></blockquote><pre><code>// 均填写本地文件路径即可FileInputFormat.setInputPaths(avgJob, new Path(&quot;&quot;));FileOutputFormat.setOutputPath(avgJob, new Path(&quot;&quot;));</code></pre><blockquote><p>若使用hdfs上的数据文件，且在本地模式运行，应配置hdfs相关参数，数据目录以及结果输出目录均填写hdfs的路径。<strong>（确保结果输出文件夹未被创建，否则会报异常）</strong></p></blockquote><pre><code>// 设置hdfs参数，并用该配置创建一个新的JobConfiguration fsConf = new Configuration();fsConf.set(&quot;fs.default.name&quot;,&quot;hdfs://localhost:9000/&quot;);Job avgJob = Job.getInstance(fsConf);// 均填写hdfs路径即可FileInputFormat.setInputPaths(avgJob, new Path(&quot;&quot;));FileOutputFormat.setOutputPath(avgJob, new Path(&quot;&quot;));</code></pre><h3 id="3-3-1-本地模式运行"><a href="#3-3-1-本地模式运行" class="headerlink" title="3.3.1 本地模式运行"></a>3.3.1 本地模式运行</h3><p>本地模式运行，直接编译执行AVGRunner的main方法即可，程序运行结束后会在自行设置的结果输出目录中生成运行结果。</p><h3 id="3-3-2-远程集群运行"><a href="#3-3-2-远程集群运行" class="headerlink" title="3.3.2 远程集群运行"></a>3.3.2 远程集群运行</h3><p><strong>首先使用IDE将程序打成一个jar包，本例中命名为hadoop.jar</strong> 提交到远程集群上运行分两种情况</p><ul><li><p>使用本地IDE（IntelliJ IDEA）运行，任务被提交到集群运行，<strong>但可使用IDE进行跟踪debug</strong> 新建一个MapReduce的配置对象，将已经打包好的jar包传入配置中</p><pre><code>// MapReduce的配置参数，远程运行，本地debugConfiguration mrConf = new Configuration();mrConf.set(&quot;mapreduce.job.jar&quot;,&quot;hadoop.jar&quot;);mrConf.set(&quot;mapreduce.framework.name&quot;,&quot;yarn&quot;);//利用以上配置新建一个JobJob avgJob = Job.getInstance(mrConf);avgJob.setJarByClass(AVGRunner.class);</code></pre></li></ul><ul><li><p>在终端直接使用hadoop命令将任务提交到集群运行，<strong>无法使用IDE进行跟踪debug</strong> 直接在终端中输入hadoop命令</p><pre><code>hadoop jar $jar包名称 $待执行的类的名称</code></pre></li></ul><pre><code>在本例中应输入    hadoop jar avg.jar mr.AVGRunner</code></pre><blockquote><p><strong>####################### 注意⚠️ #######################</strong> 在OS X中，使用IntelliJ IDEA打包jar包后，若在终端中直接使用<code>hadoop jar $jar包名称 $待执行的类的名称</code>提交MapReduce任务，会报出异常，因为OS X系统的文件系统对大小写不敏感<strong><em>（case-insensitive）</em></strong>。 经过对此异常的搜索，<strong>暂时的解决方案是通过删除jar包中的LICENSE文件</strong>，使任务顺利提交。</p><pre><code># 在终端中执行以下命令  zip -d $jar包名称 META-INF/LICENSE  zip -d $jar包名称 LICENSE</code></pre><p><strong>#####################################################</strong></p></blockquote><p><img src="http://upload-images.jianshu.io/upload_images/7445555-d6ff47959f4616b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">可以看到使用了hadoop命令提交任务后，系统调用了RPC框架和Yarn框架中的一些服务，用于远程运行，而非使用LocalJobSubmitter于本地运行。<br><img src="http://upload-images.jianshu.io/upload_images/7445555-4c5e6823f3ec9ade.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="">并且在MapReduce任务管理页面可看到任务已经完成的历史记录。</p><h1 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a>4.总结</h1><p>MapReduce任务可在本地运行，也可提交到集群上运行。 在开发初期，需要编写Demo程序时，可在本地进行开发与测试，将数据文件放置在本地文件系统，直接使用IDE运行主类的main方法，观察运行结果。 上线前调试，可采用远程模式运行，不直接使用hadoop命令提交，而是使用IDE进行提交与debug，这样既可以保证程序运行在远处集群上（生产环境or开发环境），也可以在本地方便跟踪调试。 可上线时，使用hadoop命令直接提交到远程集群，并通过localhost:50070<strong>（默认配置）</strong>的任务管理页面进行观察。</p>]]></content>
      
      
      <categories>
          
          <category> BigData </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何在Map-Reduce中实现二次排序（对Value排序）</title>
      <link href="/2018/02/13/ssort/"/>
      <url>/2018/02/13/ssort/</url>
      
        <content type="html"><![CDATA[<blockquote><p>全文为本人（Haoxiang Ma）原创内容，转载请标明出处。</p></blockquote><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>众所周知，Map-Reduce任务完成后，输出的结果文件总是按照Key进行升序排列（shuffle阶段完成）。 例如Hadoop里经典的word count程序：</p><pre><code>// File1原始数据helloworldhelloappleappleapplebaby// 输出结果文件，已按Key进行升序排序apple 3baby 1hello 2world 1</code></pre><p>显然，这种默认的排序方式很多时候能帮开发者减轻负担，因为开发者不用去自行实现对Key进行排序的算法，所有的排序操作均由Hadoop帮开发者完成（详情参考Map-Reduce中的shuffle原理，<strong><em>具体参考map端的partition-&gt;spill-&gt;(spill.sort)-&gt;(combine)过程</em></strong>）。 <strong>一切似乎很美好，可是如果我们遇到了要对value排序的需求呢？</strong></p><pre><code>// 假设有如下电影评分数据 movies.dat// 且我们希望对rating进行降序排序, 以便分析每部电影的评分趋势MovieID, rating  001,    75.5  001,    89  001,    60  002,    55  002,    79  003,    92.5  003,    92.8  003,    60 ......// 期望输出值（Key有序的同时，按rating降序排序）  001   89.0  001   75.5  001   60.0  002   79.0  002   55.0  003   92.8  003   92.5  003   60.0</code></pre><p>在“排序”的需求下，我们很自然地会想到：</p><ul><li>利用系统默认的排序。</li><li>预处理数据，把rating当成Key，movieID当成Value。</li></ul><p>虽然按照以上的想法，确实是对作为Key的rating排序了，但我们需要的是<strong>降序</strong>输出而非默认的升序输出，且输出格式不符合要求（第一列应为movieID）。 此时，就需要引入一种<strong><em>二次排序（Secondary Sort）</em></strong>的概念了。所谓<strong><em>二次排序（Secondary Sort）</em></strong>其实就是人工地对所需字段进行排序，在系统的默认排序基础上做第二次的排序。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p><strong><em>二次排序（Secondary Sort）</em></strong>概念上不难理解，无非就是自行多做一次特定的排序。可是该如何实现呢？怎么在map-reduce的流程框框内对Value进行人工排序呢？ 其实关键技巧就是利用map-reduce会对Key排序的特点，让它“顺带”对Value进行排序。为了达到这种“顺带”的效果，<strong>我们可以将原始数据中的Key（MovieID）和Value（rating）合并到一起作为新的Key（MovieBean），同时仍然保持原Value（rating）作为Value。当系统对这个合并的Key（MovieBean）按照某种特性进行排序时，其对应的Value也会被相应地“排序”（因为map端输出时，Key和Value是一个整体数据结构）</strong>，为此我们应设计一个自定义的Bean类。</p><pre><code>/** * 自定义的MovieBean类, 将原始数据中的Key和Value合并到一个类中。 */public class MovieBean implements WritableComparable&lt;MovieBean&gt;{    public Text movieID;    public DoubleWritable score;    public MovieBean() {    }    public MovieBean(Text movieID, DoubleWritable score) {        this.movieID = movieID;        this.score = score;    }    public void set(Text movieID, DoubleWritable score) {        this.movieID = movieID;        this.score = score;    }    public Text getMovieID() {        return movieID;    }    public void setMovieID(Text movieID) {        this.movieID = movieID;    }    public DoubleWritable getScore() {        return score;    }    public void setScore(DoubleWritable score) {        this.score = score;    }    @Override    public String toString() {        return &quot;movieID=&quot; + movieID +                &quot;, score=&quot; + score;    }    /**     * 重点! 利用自定义的compareTo方法实现排序效果!     * @param o object of MovieBean     * @return result of comparison     */    @Override    public int compareTo(MovieBean o) {        if(o == null) {            throw new RuntimeException();        }        // movieID相同时, 按照score进行降序排序        if(this.movieID.compareTo(o.getMovieID()) == 0) {            return -score.compareTo(o.getScore());        }        // movieID不相同时, 直接按照MovieID排序        return this.movieID.compareTo(o.getMovieID());    }    /**     * @param dataOutput 序列化输出     */    @Override    public void write(DataOutput dataOutput) throws IOException {        dataOutput.writeUTF(movieID.toString());        dataOutput.writeDouble(score.get());    }    /**     * @param dataInput 序列化输入     */    @Override    public void readFields(DataInput dataInput) throws IOException {        movieID = new Text(dataInput.readUTF());        score = new DoubleWritable(dataInput.readDouble());    }}</code></pre><p>有了这个自定义的<strong><em>MovieBean</em></strong>类作为新的Key后，Mapper端的输出就从原来的 <strong><em>\&lt;MovieID, Rating&gt;</em></strong>键值对转换成了<strong><em>\&lt;MovieBean, Rating&gt;</em></strong>键值对。</p><pre><code>&lt;MovieID, Rating&gt;  =&gt;  &lt;(MovieID, Rating), Rating&gt;</code></pre><p>那么Reducer端接收到的将是大量的<strong><em>\&lt;MovieBean, Rating&gt;</em></strong>数据。此时问题就来了，当我们的Key是<strong>简单类型</strong>时（如IntWritable，Text，DoubleWritable），很自然就能将多个K-V对中相同的Key提取出来，且将多个Value合并成一个集合，构成Reducer端的输入数据结构<strong><em>\&lt;Key, List></em></strong>。但是当我们的Key是复合类型，例如MovieBean是MovieID和Rating的复合结构时，<strong>即使两个MovieBean对象的MovieID相同，但这两个MovieBean却是不会被认为是同一个对象的。</strong></p><pre><code>// 1号K-V对&lt;(&quot;0001&quot;, 85.0), 85.0&gt;// 2号K-V对&lt;(&quot;0001&quot;, 68.0), 68.0&gt;// Reducer接收到以上两个K-V对后，并不会把它们合并成&lt;MovieBean, List&lt;Value&gt;&gt;的数据结构// 因为两个K-V对的Key（MovieBean）并不相同</code></pre><p>为了解决这个问题，让Reducer把相同MovieID的MovieBean当成是一样的Key，继而把相同MovieID所对应的Ratings合并成<strong><em>\&lt;MovieBean, List></em></strong>结构，<strong>我们需要通过实现自定义的GroupingComparator来 _欺骗_ Reducer。</strong></p><pre><code>/** * 自定义的GroupingComparator */public class MovieGroupingComparator extends WritableComparator {    /**     * 构造函数, 告知自定义Bean类     */    protected MovieGroupingComparator() {        super(MovieBean.class, true);    }    /**     * 重写WritableComparator接口的compare方法(类似于普通Comparator接口)     * @param a movieA     * @param b movieB     * @return result of comparison     */    @Override    public int compare(WritableComparable a, WritableComparable b) {        MovieBean movieA = (MovieBean) a;        MovieBean movieB = (MovieBean) b;        // 只比较两个MovieBean的MovieID, 忽略其他属性        return movieA.getMovieID().compareTo(movieB.getMovieID());    }}</code></pre><p>实现以上的自定义GroupingComparator时，我们在compare方法中只考虑<strong><em>MovieID</em></strong>这一个属性，等同于<strong>_欺骗_</strong>了Reducer。Reducer判断两个Key是否相同时<strong>只考虑MovieID是否相同</strong>，从而将不同的MovieBean对象抽取成一个统一的MovieBean作为Reducer的输入Key，即可顺利合并出<strong><em>\&lt;MovieBean, List></em></strong>这样的数据结构。</p><pre><code>// example// 假设Reducer0接收到了以下K-V对&lt;(&quot;0001&quot;, 89.0), 89.0&gt;&lt;(&quot;0001&quot;, 76.8), 76.8&gt;&lt;(&quot;0001&quot;, 69.5), 69.5&gt;&lt;(&quot;0001&quot;, 69.3), 69.3&gt;// 由于compare方法只比较MovieBean中的MovieID属性, 完全忽略Rating, 所以// 以上4个K-V对中的MovieBean均会被视作一样的Key，最终合并成的数据结构如下// (为什么是有序的, 因为在map端的spill过程中已经依照rating降序排列了,参考MovieBean// 类中重写的compareTo方法)&lt; Key, List&lt;Value&gt;&gt;&lt;(&quot;0001&quot;, 89.0), [89.0, 76.8, 69.5, 69.3]&gt;</code></pre><h2 id="作业提交"><a href="#作业提交" class="headerlink" title="作业提交"></a>作业提交</h2><p>至此，二次排序中所有自定义的工作已经完成。<strong>但是千万不要忘记在提交Job之前，给Job设置以上自定义GroupingComparator</strong>，否则Job会使用内置默认的GroupingComparator，那我们的二次排序就无法生效了。</p><pre><code>movieJob.setGroupingComparatorClass(MovieGroupingComparator.class);</code></pre><p>另外，<strong>如果需要自定义Reducer数量</strong>（例如有时希望输出N个结果文件，则需要N个Reducer），还要自定义Partitioner。Partitioner的作用简单来说就是给Mapper端产生的K-V对打上一个<strong><em>partition id</em></strong>烙印，让系统知道这个K-V对应该被哪个Reducer取走。在本例中，<strong>如有需要（不是必须）</strong>，我们可以按照MovieID进行划分，不同MovieID的K-V对划分到不同的Reducer上进行处理。</p><pre><code>/** * 自定义Partitioner, 用于划分K-V对被哪个Reducer取走 */public class MoviePartitioner extends Partitioner&lt;MovieBean, DoubleWritable&gt; {    @Override    public int getPartition(MovieBean movieBean, DoubleWritable doubleWritable, int numReducers) {        // 相同MovieID的必定会到同一个Reducer上        return movieBean.getMovieID().hashCode() % numReducers;    }}</code></pre><p>最后给Job设定自定义的Reducer数，即可启动N个Reducer进行数据处理。</p><pre><code>final int NUM_REDUCE_TASK = 5;movieJob.setNumReduceTasks(NUM_REDUCE_TASK);</code></pre><p><img src="/Users/marco/Desktop/ss.png" alt=""></p><p>单Reducer运行结果</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>使用SecondarySort可以使我们在Map-Reduce框架内完成自定义排序。依托Map-Reduce会对Key进行排序的特性，可以将需要排序的字段（Value）与原始Key合成为自定义的Bean作为新的Key，原Value保持不动。有了SecondarySort，我们就不必在框架外做额外的工作进行排序，干扰程序的可读性；也不必将原始Key和Value对换，影响输出格式。</p><h2 id="代码地址"><a href="#代码地址" class="headerlink" title="代码地址"></a>代码地址</h2><p><a href="https://github.com/AcepcsMa/hadoop_examples/tree/master/src/sort" target="_blank" rel="noopener">Github-Secondary Sort</a></p>]]></content>
      
      
      <categories>
          
          <category> BigData </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bigdata </tag>
            
            <tag> hadoop </tag>
            
            <tag> mapreduce </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>利用ZooKeeper开发服务器上下线感知程序</title>
      <link href="/2018/02/13/zkserver/"/>
      <url>/2018/02/13/zkserver/</url>
      
        <content type="html"><![CDATA[<blockquote><p>全文为本人（Haoxiang Ma）原创内容，转载请标明出处。</p></blockquote><h2 id="What-is-ZooKeeper"><a href="#What-is-ZooKeeper" class="headerlink" title="What is ZooKeeper"></a>What is ZooKeeper</h2><p>ZooKeeper是一个分布式的分布式应用程序协调服务。简单地来说，就是用于协调管理多个分布式应用程序的一个工具，扮演着一个第三方管理者的角色。</p><h2 id="问题背景分析"><a href="#问题背景分析" class="headerlink" title="问题背景分析"></a>问题背景分析</h2><p>假设现在有<strong>10</strong>个应用程序(App#0 - App#9)，运行在由<strong>10</strong>台服务器(Server#0 - Server#9)组成的集群上（假设平均分配，每台服务器上运行一个程序）。此时由于某个热门线上活动的开始（如抢票or低价秒杀等），突然间有数以百万计的用户访问服务器上的资源，等待服务器处理并应答（如下图所示）。 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/server.png" alt=""> 很不幸<strong>10</strong>台服务器中有<strong>K</strong>台受不住负载压力，导致服务器崩溃。在这种情况下，如果客户端无法感知服务器的状态（在线／离线），部分向已经崩溃的服务器发送请求的客户端将会有长时间无法获得应答，它们只能一直重复地向已经崩溃的服务器地址重发请求，无法切换至另外<strong>（10-K）</strong>台完好的服务器进行交互。 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/breakdown.png" alt=""> 其实在这种场景下，如果客户端能够及时地感知到集群中哪些节点已经崩溃，哪些节点仍然完好，是可以切换至完好的节点并向其发送请求的。理论上只要集群中仍有1个节点是完好的，它即能向客户端提供服务。 <strong><em>所以整个问题的症结就在于，如何让客户端感知到服务器上下线状态，以便切换请求发送的地址。</em></strong> <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/zk.png" alt=""> 重新参考ZooKeeper的功能描述，ZooKeeper可以用来协调管理多个分布式应用程序，那其实可以用于管理我们的分布式机器集群。如上图所示，<strong>在用户和服务器集群中间可设置ZooKeeper层</strong>，让ZooKeeper实时感知每一个节点的状态，然后客户端并不直接向具体节点发起请求，而应先向ZooKeeper询问当前仍然存活的服务器节点，然后再从中挑选一个负载较低的服务器节点进行交互。由于ZooKeeper本身的高可用性（本身也可拓展为分布式架构），所以就能大大地提高整个系统的可用性。</p><h2 id="ZooKeeper数据结构"><a href="#ZooKeeper数据结构" class="headerlink" title="ZooKeeper数据结构"></a>ZooKeeper数据结构</h2><p>ZooKeeper数据结构采用了树状结构（在文件系统中被广泛使用），且不是简单的二叉树，而是多叉树。在ZooKeeper的树结构中，每一个节点被称为znode，可通过控制台命令或者Java的SDK对内部数据进行管理。 znode的类型有<code>2*2=4</code>种，分别是：</p><ul><li>PERSISTENT</li><li>PERSISTENT_SEQUENTIAL</li><li>EPHEMERAL</li><li>EPHEMERAL_SEQUENTIAL</li></ul><p>其中<strong><em>PERSISTENT</em></strong>和<strong><em>EPHEMERAL</em></strong>的区别正如其名，在无外力影响下<strong><em>PERSISTENT</em></strong>节点不会被改变和删除，而<strong><em>EPHEMERAL</em></strong>节点在创建节点的session结束后会自动从树中删除。至于<strong><em>SEQUENTIAL</em></strong>与<strong><em>非SEQUENTIAL</em></strong>则影响了节点id自增，<strong><em>SEQUENTIAL</em></strong>节点的id会自动遵循父节点下的自增规则进行命名。 <img src="https://blog-image-1253224514.cos.ap-guangzhou.myqcloud.com/zkds.png" alt=""> 如图所示，在本问题中我们可以把一台服务器看作树中的一个节点，我们可以利用<strong><em>EPHEMERAL</em></strong>节点的这一特性进行服务器状态的监听。服务器上线时创建与zk之间的session并向zk注册节点，只要服务器不崩溃，session便不会结束，即<strong><em>EPHEMERAL</em></strong>节点会一直存在，可被客户端感知；当服务器崩溃时，其与zk之间保持的session自然也会结束，<strong><em>EPHEMERAL</em></strong>节点会自动被删除，客户端查询服务器列表时绝对无法获得已删除的节点信息。</p><h2 id="Demo程序"><a href="#Demo程序" class="headerlink" title="Demo程序"></a>Demo程序</h2><ul><li><p><strong><em>Server.java (服务器端代码)</em></strong></p><p>package my.bigdata.zk;</p><p>import org.apache.zookeeper.*;</p><p>public class Server {</p><pre><code>private static final String HOST_ADDRESS = &quot;localhost:2181&quot;;private static final int DEFAULT_TIMEOUT = 2000;private static final String DEFAULT_SERVER_PARENT = &quot;/servers&quot;;private ZooKeeper zkConnect = null;/** * 连接至ZooKeeper * @throws Exception */public void connect() throws Exception{    zkConnect = new ZooKeeper(HOST_ADDRESS, DEFAULT_TIMEOUT, new Watcher() {        @Override        public void process(WatchedEvent watchedEvent) {            System.out.println(&quot;Type:&quot; + watchedEvent.getType()                    + &quot; Path:&quot; + watchedEvent.getPath());        }    });}/** * 向ZooKeeper注册本服务器节点 * @param data 服务器信息 * @throws Exception */public void register(String data) throws Exception{    String create = zkConnect.create(DEFAULT_SERVER_PARENT + &quot;/server&quot;,                                        data.getBytes(),                                        ZooDefs.Ids.OPEN_ACL_UNSAFE,                                        CreateMode.EPHEMERAL_SEQUENTIAL);   // 注册成ephemeral节点以便自动在zk上注销    System.out.println(create + &quot; is registered!&quot;);}/** * 通过sleep模拟服务器在线 */public void sleep() {    try {        Thread.sleep(20000);    } catch (Exception e) {        System.out.println(e.toString());    }}</code></pre></li></ul><pre><code>    public static void main(String[] args) throws Exception {        //连接至zk        Server server = new Server();        server.connect();        //向zk注册服务器信息        String data = args[0];        server.register(data);        server.sleep();    }}</code></pre><p>服务器端的重点在于，程序启动时向ZooKeeper的指定节点下注册服务器信息，相当于通知ZooKeeper这个第三方：“服务器已上线”。其次，注册的节点类型必须是<strong>ephemeral</strong>节点，为了实现节点id自增(auto-increment)还可以使用<strong>ephemeral_sequential</strong>节点。</p><ul><li><p><strong><em>Client.java (客户端代码)</em></strong></p><p>package my.bigdata.zk;</p><p>import org.apache.zookeeper.WatchedEvent;<br>import org.apache.zookeeper.Watcher;<br>import org.apache.zookeeper.ZooKeeper;</p><p>import java.util.ArrayList;<br>import java.util.Arrays;<br>import java.util.List;</p><p>public class Client {</p><pre><code>private static final String HOST_ADDRESS = &quot;localhost:2181&quot;;private static final int DEFAULT_TIMEOUT = 2000;private static final String DEFAULT_SERVER_PARENT = &quot;/servers&quot;;private ZooKeeper zkConnect = null;private List&lt;String&gt; availableServers;/** * 连接至ZooKeeper * @throws Exception */public void connect() throws Exception {    zkConnect = new ZooKeeper(HOST_ADDRESS, DEFAULT_TIMEOUT, new Watcher() {        @Override        public void process(WatchedEvent watchedEvent) {            try {                updateServerCondition();    // 重复注册            } catch (Exception e) {                System.out.println(e.toString());            }        }    });}/** * 向zk查询服务器情况, 并update本地服务器列表 * @throws Exception */public void updateServerCondition() throws Exception {    List&lt;String&gt; children = zkConnect.getChildren(DEFAULT_SERVER_PARENT, true);    List&lt;String&gt; servers = new ArrayList&lt;&gt;();    for(String child : children) {        byte[] data = zkConnect.getData(DEFAULT_SERVER_PARENT + &quot;/&quot; + child,                                    false,                                    null);        servers.add(new String(data));    }    availableServers = servers;    System.out.println(Arrays.toString(servers.toArray(new String[0])));}/** * 通过sleep让客户端持续运行，模拟&quot;监听&quot; */public void sleep() throws Exception{    System.out.println(&quot;client is working&quot;);    Thread.sleep(Long.MAX_VALUE);}public static void main(String[] args) throws Exception {    // 连接zk    Client client = new Client();    client.connect();    // 获取servers节点信息（并监听），从中获取服务器信息列表    client.updateServerCondition();    client.sleep();}</code></pre><p>}</p></li></ul><p>客户端的重点在于，它不断地向ZooKeeper某个特定节点（此处是servers节点）注册了一个<strong>Watcher</strong>，那么一旦该节点下的结构发生改变，ZooKeeper会向注册了<strong>Watcher</strong>的客户端发送“状态变化”的消息，那么客户端即可动态地从ZooKeeper中获取最新的服务器节点信息，甚至无需“主动”询问。 当然，ZooKeeper的应用场景还有很多，考虑到它本身也可拓展为一个分布式应用，在这种高可用性保证下它简直就是多个分布式应用的万能管家和协调者😊。</p>]]></content>
      
      
      <categories>
          
          <category> BigData </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bigdata </tag>
            
            <tag> zookeeper </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
